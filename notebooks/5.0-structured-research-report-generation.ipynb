{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Structured Research Report Generation\n",
    "\n",
    "This notebook demonstrates building a sophisticated **multi-agent report generation system** using LangChain 1.0+ and LangGraph 1.0+.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Planning agent workflows with structured outputs\n",
    "- Parallel web research with Tavily Search\n",
    "- Using `Send()` API for concurrent section writing\n",
    "- Compiling final reports from multiple sources\n",
    "\n",
    "This notebook is adapted from [LangChain's Report Maistro](https://github.com/langchain-ai/report-mAIstro).\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "User Input → Report Planner → [Research Agents (parallel)] → [Section Writers (parallel)] → Final Report\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -qU langchain>=1.0.0 langgraph>=1.0.0\n",
    "%pip install -qU langchain-openai langchain-core\n",
    "%pip install -qU langchain-tavily  # New: replaces tavily-python for LangChain integration\n",
    "%pip install -qU tavily-python     # Direct API access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Enable LangSmith tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"structured-research-report\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the LLM\n",
    "\n",
    "Using `init_chat_model` for provider-agnostic model initialization (LangChain 1.0 pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# LangChain 1.0 pattern: provider-agnostic initialization\n",
    "llm = init_chat_model(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Test the connection\n",
    "result = llm.invoke(\"Write a haiku about LangChain.\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Tavily Search\n",
    "\n",
    "We use both the direct Tavily client (for async operations) and the LangChain integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient, AsyncTavilyClient\n",
    "\n",
    "# Direct Tavily clients for maximum control\n",
    "tavily_client = TavilyClient()\n",
    "tavily_async_client = AsyncTavilyClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Models\n",
    "\n",
    "Using Pydantic models for structured outputs - a key LangChain 1.0 pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "\n",
    "\n",
    "class Section(BaseModel):\n",
    "    \"\"\"Represents a section of the report.\"\"\"\n",
    "    name: str = Field(description=\"Name for this section of the report.\")\n",
    "    description: str = Field(description=\"Brief overview of the main topics and concepts to be covered.\")\n",
    "    research: bool = Field(description=\"Whether to perform web research for this section.\")\n",
    "    content: str = Field(default=\"\", description=\"The content of the section.\")\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    \"\"\"Collection of report sections.\"\"\"\n",
    "    sections: List[Section] = Field(description=\"Sections of the report.\")\n",
    "\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    \"\"\"A search query for web research.\"\"\"\n",
    "    search_query: str = Field(description=\"Query for web search.\")\n",
    "\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    \"\"\"Collection of search queries.\"\"\"\n",
    "    queries: List[SearchQuery] = Field(description=\"List of search queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langsmith import traceable\n",
    "\n",
    "\n",
    "def deduplicate_and_format_sources(\n",
    "    search_response, \n",
    "    max_tokens_per_source: int, \n",
    "    include_raw_content: bool = True\n",
    ") -> str:\n",
    "    \"\"\"Format and deduplicate search results from Tavily API.\"\"\"\n",
    "    # Handle different input formats\n",
    "    if isinstance(search_response, dict):\n",
    "        sources_list = search_response.get('results', [])\n",
    "    elif isinstance(search_response, list):\n",
    "        sources_list = []\n",
    "        for response in search_response:\n",
    "            if isinstance(response, dict) and 'results' in response:\n",
    "                sources_list.extend(response['results'])\n",
    "            else:\n",
    "                sources_list.extend(response if isinstance(response, list) else [response])\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a dict or list of search results\")\n",
    "    \n",
    "    # Deduplicate by URL\n",
    "    unique_sources = {}\n",
    "    for source in sources_list:\n",
    "        if isinstance(source, dict) and source.get('url') not in unique_sources:\n",
    "            unique_sources[source['url']] = source\n",
    "    \n",
    "    # Format output\n",
    "    formatted_text = \"Sources:\\n\\n\"\n",
    "    for source in unique_sources.values():\n",
    "        formatted_text += f\"Source {source.get('title', 'Unknown')}:\\n===\\n\"\n",
    "        formatted_text += f\"URL: {source.get('url', '')}\\n===\\n\"\n",
    "        formatted_text += f\"Most relevant content: {source.get('content', '')}\\n===\\n\"\n",
    "        \n",
    "        if include_raw_content:\n",
    "            char_limit = max_tokens_per_source * 4  # ~4 chars per token\n",
    "            raw_content = source.get('raw_content') or ''\n",
    "            if len(raw_content) > char_limit:\n",
    "                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n",
    "            formatted_text += f\"Full content (limited to {max_tokens_per_source} tokens): {raw_content}\\n\\n\"\n",
    "    \n",
    "    return formatted_text.strip()\n",
    "\n",
    "\n",
    "def format_sections(sections: List[Section]) -> str:\n",
    "    \"\"\"Format a list of sections into a readable string.\"\"\"\n",
    "    formatted_str = \"\"\n",
    "    for idx, section in enumerate(sections, 1):\n",
    "        formatted_str += f\"\"\"\n",
    "{'='*60}\n",
    "Section {idx}: {section.name}\n",
    "{'='*60}\n",
    "Description: {section.description}\n",
    "Requires Research: {section.research}\n",
    "Content: {section.content if section.content else '[Not yet written]'}\n",
    "\n",
    "\"\"\"\n",
    "    return formatted_str\n",
    "\n",
    "\n",
    "@traceable\n",
    "def tavily_search(query: str) -> dict:\n",
    "    \"\"\"Perform a synchronous web search.\"\"\"\n",
    "    return tavily_client.search(\n",
    "        query, \n",
    "        max_results=5, \n",
    "        include_raw_content=True\n",
    "    )\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def tavily_search_async(\n",
    "    search_queries: List[str], \n",
    "    tavily_topic: str, \n",
    "    tavily_days: Optional[int] = None\n",
    ") -> List[dict]:\n",
    "    \"\"\"Perform concurrent web searches.\"\"\"\n",
    "    search_tasks = []\n",
    "    for query in search_queries:\n",
    "        if tavily_topic == \"news\":\n",
    "            task = tavily_async_client.search(\n",
    "                query,\n",
    "                max_results=5,\n",
    "                include_raw_content=True,\n",
    "                topic=\"news\",\n",
    "                days=tavily_days\n",
    "            )\n",
    "        else:\n",
    "            task = tavily_async_client.search(\n",
    "                query,\n",
    "                max_results=5,\n",
    "                include_raw_content=True,\n",
    "                topic=\"general\"\n",
    "            )\n",
    "        search_tasks.append(task)\n",
    "    \n",
    "    return await asyncio.gather(*search_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Report State\n",
    "\n",
    "Using `TypedDict` for state schemas - the recommended LangGraph 1.0 pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "\n",
    "class ReportState(TypedDict):\n",
    "    \"\"\"Main state for the report generation workflow.\"\"\"\n",
    "    topic: str                                          # Report topic\n",
    "    tavily_topic: Literal[\"general\", \"news\"]            # Search type\n",
    "    tavily_days: Optional[int]                          # Days for news search\n",
    "    report_structure: str                               # Structure guidelines\n",
    "    number_of_queries: int                              # Queries per section\n",
    "    sections: List[Section]                             # Planned sections\n",
    "    completed_sections: Annotated[list, operator.add]   # Completed sections (reducible)\n",
    "    report_sections_from_research: str                  # Formatted research content\n",
    "    final_report: str                                   # Final compiled report\n",
    "\n",
    "\n",
    "class SectionState(TypedDict):\n",
    "    \"\"\"State for individual section writing.\"\"\"\n",
    "    tavily_topic: Literal[\"general\", \"news\"]\n",
    "    tavily_days: Optional[int]\n",
    "    number_of_queries: int\n",
    "    section: Section\n",
    "    search_queries: List[SearchQuery]\n",
    "    source_str: str\n",
    "    report_sections_from_research: str\n",
    "    completed_sections: List[Section]\n",
    "\n",
    "\n",
    "class SectionOutputState(TypedDict):\n",
    "    \"\"\"Output state for section subgraph.\"\"\"\n",
    "    completed_sections: List[Section]\n",
    "\n",
    "\n",
    "class ReportStateOutput(TypedDict):\n",
    "    \"\"\"Final output state.\"\"\"\n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query generation for planning\n",
    "REPORT_PLANNER_QUERY_INSTRUCTIONS = \"\"\"You are an expert technical writer planning a report.\n",
    "\n",
    "Topic: {topic}\n",
    "Report Structure: {report_organization}\n",
    "\n",
    "Generate {number_of_queries} search queries to gather comprehensive information for planning the report sections.\n",
    "\n",
    "Queries should:\n",
    "1. Be related to the topic\n",
    "2. Help satisfy the report organization requirements\n",
    "3. Be specific enough to find high-quality sources\"\"\"\n",
    "\n",
    "# Section planning\n",
    "REPORT_PLANNER_INSTRUCTIONS = \"\"\"You are an expert technical writer planning a report.\n",
    "\n",
    "Topic: {topic}\n",
    "Report Structure: {report_organization}\n",
    "Research Context: {context}\n",
    "\n",
    "Generate the outline with these fields per section:\n",
    "- name: Section title\n",
    "- description: Brief overview of topics to cover\n",
    "- research: Whether web research is needed (intro/conclusion usually don't need research)\n",
    "- content: Leave blank for now\"\"\"\n",
    "\n",
    "# Query generation for sections\n",
    "QUERY_WRITER_INSTRUCTIONS = \"\"\"Generate targeted web search queries for a technical report section.\n",
    "\n",
    "Section Topic: {section_topic}\n",
    "\n",
    "Generate {number_of_queries} queries that:\n",
    "1. Cover different aspects (features, applications, architecture)\n",
    "2. Include specific technical terms\n",
    "3. Target recent information (include year markers like \"2024\" or \"2025\")\n",
    "4. Look for comparisons and differentiators\n",
    "5. Search official docs and practical examples\"\"\"\n",
    "\n",
    "# Section writing\n",
    "SECTION_WRITER_INSTRUCTIONS = \"\"\"You are an expert technical writer crafting one section.\n",
    "\n",
    "Section Topic: {section_topic}\n",
    "\n",
    "Guidelines:\n",
    "1. Technical Accuracy: Include version numbers, metrics, official docs\n",
    "2. Length: 150-200 words (strict)\n",
    "3. Structure:\n",
    "   - Use ## for section title\n",
    "   - Start with **bold** key insight\n",
    "   - ONE structural element max (table OR list)\n",
    "   - End with ### Sources\n",
    "4. No preamble or marketing language\n",
    "\n",
    "Source Material:\n",
    "{context}\"\"\"\n",
    "\n",
    "# Final sections (intro/conclusion)\n",
    "FINAL_SECTION_INSTRUCTIONS = \"\"\"You are an expert technical writer crafting a synthesis section.\n",
    "\n",
    "Section: {section_topic}\n",
    "\n",
    "Available Report Content:\n",
    "{context}\n",
    "\n",
    "For Introduction:\n",
    "- Use # for title\n",
    "- 50-100 words\n",
    "- Clear narrative arc\n",
    "- NO structural elements\n",
    "\n",
    "For Conclusion:\n",
    "- Use ## for title  \n",
    "- 100-150 words\n",
    "- Include comparison table for comparative reports\n",
    "- End with next steps/implications\n",
    "- NO sources section\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Planning Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "def invoke_with_retry(structured_llm, messages, max_attempts: int = 3):\n",
    "    \"\"\"Retry structured LLM calls to handle occasional failures.\"\"\"\n",
    "    for _ in range(max_attempts):\n",
    "        result = structured_llm.invoke(messages)\n",
    "        if result:\n",
    "            return result\n",
    "    return result\n",
    "\n",
    "\n",
    "async def generate_report_plan(state: ReportState) -> dict:\n",
    "    \"\"\"Generate the report outline with web research.\"\"\"\n",
    "    topic = state[\"topic\"]\n",
    "    report_structure = state[\"report_structure\"]\n",
    "    number_of_queries = state[\"number_of_queries\"]\n",
    "    tavily_topic = state[\"tavily_topic\"]\n",
    "    tavily_days = state.get(\"tavily_days\")\n",
    "    \n",
    "    # Convert structure to string if needed\n",
    "    if isinstance(report_structure, dict):\n",
    "        report_structure = str(report_structure)\n",
    "    \n",
    "    # Step 1: Generate search queries for planning\n",
    "    query_llm = llm.with_structured_output(Queries)\n",
    "    system_msg = REPORT_PLANNER_QUERY_INSTRUCTIONS.format(\n",
    "        topic=topic,\n",
    "        report_organization=report_structure,\n",
    "        number_of_queries=number_of_queries\n",
    "    )\n",
    "    \n",
    "    queries = invoke_with_retry(\n",
    "        query_llm,\n",
    "        [SystemMessage(content=system_msg), HumanMessage(content=\"Generate search queries.\")]\n",
    "    )\n",
    "    \n",
    "    # Step 2: Perform web searches\n",
    "    query_list = [q.search_query for q in queries.queries]\n",
    "    search_results = await tavily_search_async(query_list, tavily_topic, tavily_days)\n",
    "    source_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000)\n",
    "    \n",
    "    # Step 3: Generate section outline\n",
    "    sections_llm = llm.with_structured_output(Sections)\n",
    "    system_msg = REPORT_PLANNER_INSTRUCTIONS.format(\n",
    "        topic=topic,\n",
    "        report_organization=report_structure,\n",
    "        context=source_str\n",
    "    )\n",
    "    \n",
    "    sections = invoke_with_retry(\n",
    "        sections_llm,\n",
    "        [SystemMessage(content=system_msg), HumanMessage(content=\"Generate the report sections.\")]\n",
    "    )\n",
    "    \n",
    "    return {\"sections\": sections.sections}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Writing Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(state: SectionState) -> dict:\n",
    "    \"\"\"Generate search queries for a section.\"\"\"\n",
    "    section = state[\"section\"]\n",
    "    number_of_queries = state[\"number_of_queries\"]\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(Queries)\n",
    "    system_msg = QUERY_WRITER_INSTRUCTIONS.format(\n",
    "        section_topic=section.description,\n",
    "        number_of_queries=number_of_queries\n",
    "    )\n",
    "    \n",
    "    queries = invoke_with_retry(\n",
    "        structured_llm,\n",
    "        [SystemMessage(content=system_msg), HumanMessage(content=\"Generate search queries.\")]\n",
    "    )\n",
    "    \n",
    "    return {\"search_queries\": queries.queries}\n",
    "\n",
    "\n",
    "async def search_web(state: SectionState) -> dict:\n",
    "    \"\"\"Search the web for section content.\"\"\"\n",
    "    search_queries = state[\"search_queries\"]\n",
    "    tavily_topic = state[\"tavily_topic\"]\n",
    "    tavily_days = state.get(\"tavily_days\")\n",
    "    \n",
    "    query_list = [q.search_query for q in search_queries]\n",
    "    search_results = await tavily_search_async(query_list, tavily_topic, tavily_days)\n",
    "    source_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=5000)\n",
    "    \n",
    "    return {\"source_str\": source_str}\n",
    "\n",
    "\n",
    "def write_section(state: SectionState) -> dict:\n",
    "    \"\"\"Write a section based on research.\"\"\"\n",
    "    section = state[\"section\"]\n",
    "    source_str = state[\"source_str\"]\n",
    "    \n",
    "    system_msg = SECTION_WRITER_INSTRUCTIONS.format(\n",
    "        section_topic=section.description,\n",
    "        context=source_str\n",
    "    )\n",
    "    \n",
    "    content = llm.invoke([\n",
    "        SystemMessage(content=system_msg),\n",
    "        HumanMessage(content=\"Write the section based on the sources.\")\n",
    "    ])\n",
    "    \n",
    "    section.content = content.content\n",
    "    return {\"completed_sections\": [section]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Section Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Build section writing subgraph\n",
    "section_builder = StateGraph(SectionState, output=SectionOutputState)\n",
    "\n",
    "section_builder.add_node(\"generate_queries\", generate_queries)\n",
    "section_builder.add_node(\"search_web\", search_web)\n",
    "section_builder.add_node(\"write_section\", write_section)\n",
    "\n",
    "section_builder.add_edge(START, \"generate_queries\")\n",
    "section_builder.add_edge(\"generate_queries\", \"search_web\")\n",
    "section_builder.add_edge(\"search_web\", \"write_section\")\n",
    "section_builder.add_edge(\"write_section\", END)\n",
    "\n",
    "section_graph = section_builder.compile()\n",
    "\n",
    "display(Image(section_graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Report Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORT_STRUCTURE = \"\"\"This report focuses on comparative analysis.\n",
    "\n",
    "Structure:\n",
    "1. Introduction (no research needed)\n",
    "   - Overview of the topic\n",
    "   - Context for comparison\n",
    "\n",
    "2. Main Body Sections:\n",
    "   - One section per item being compared\n",
    "   - Each section covers:\n",
    "     * Core Features (bulleted list)\n",
    "     * Architecture & Implementation (2-3 sentences)\n",
    "     * Example use case (2-3 sentences)\n",
    "\n",
    "3. Conclusion with Comparison Table (no research needed)\n",
    "   - Comparison table across key dimensions\n",
    "   - Strengths and weaknesses\n",
    "   - Final recommendations\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Section Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test topic\n",
    "report_topic = \"Comparison of AI agent frameworks: LangChain, CrewAI, and AutoGen\"\n",
    "\n",
    "# Generate the plan first\n",
    "sections_result = await generate_report_plan({\n",
    "    \"topic\": report_topic,\n",
    "    \"report_structure\": REPORT_STRUCTURE,\n",
    "    \"number_of_queries\": 3,\n",
    "    \"tavily_topic\": \"general\",\n",
    "    \"tavily_days\": None\n",
    "})\n",
    "\n",
    "# Display the planned sections\n",
    "for section in sections_result[\"sections\"]:\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Name: {section.name}\")\n",
    "    print(f\"Description: {section.description}\")\n",
    "    print(f\"Research: {section.research}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with one section that requires research\n",
    "test_section = next(s for s in sections_result[\"sections\"] if s.research)\n",
    "print(f\"Testing with section: {test_section.name}\")\n",
    "\n",
    "result = await section_graph.ainvoke({\n",
    "    \"section\": test_section,\n",
    "    \"number_of_queries\": 2,\n",
    "    \"tavily_topic\": \"general\",\n",
    "    \"tavily_days\": None\n",
    "})\n",
    "\n",
    "# Display the result\n",
    "from IPython.display import Markdown\n",
    "Markdown(result[\"completed_sections\"][0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Full Report Graph\n",
    "\n",
    "Using LangGraph's `Send()` API to parallelize section writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "\n",
    "def initiate_section_writing(state: ReportState):\n",
    "    \"\"\"Kick off parallel research for sections that need it.\"\"\"\n",
    "    return [\n",
    "        Send(\"build_section_with_web_research\", {\n",
    "            \"section\": s,\n",
    "            \"number_of_queries\": state[\"number_of_queries\"],\n",
    "            \"tavily_topic\": state[\"tavily_topic\"],\n",
    "            \"tavily_days\": state.get(\"tavily_days\")\n",
    "        })\n",
    "        for s in state[\"sections\"]\n",
    "        if s.research\n",
    "    ]\n",
    "\n",
    "\n",
    "def gather_completed_sections(state: ReportState) -> dict:\n",
    "    \"\"\"Format completed research sections.\"\"\"\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "    formatted = format_sections(completed_sections)\n",
    "    return {\"report_sections_from_research\": formatted}\n",
    "\n",
    "\n",
    "def write_final_sections(state: SectionState) -> dict:\n",
    "    \"\"\"Write intro/conclusion sections using research context.\"\"\"\n",
    "    section = state[\"section\"]\n",
    "    context = state[\"report_sections_from_research\"]\n",
    "    \n",
    "    system_msg = FINAL_SECTION_INSTRUCTIONS.format(\n",
    "        section_topic=section.description,\n",
    "        context=context\n",
    "    )\n",
    "    \n",
    "    content = llm.invoke([\n",
    "        SystemMessage(content=system_msg),\n",
    "        HumanMessage(content=\"Write the section.\")\n",
    "    ])\n",
    "    \n",
    "    section.content = content.content\n",
    "    return {\"completed_sections\": [section]}\n",
    "\n",
    "\n",
    "def initiate_final_section_writing(state: ReportState):\n",
    "    \"\"\"Kick off parallel writing for non-research sections.\"\"\"\n",
    "    return [\n",
    "        Send(\"write_final_sections\", {\n",
    "            \"section\": s,\n",
    "            \"report_sections_from_research\": state[\"report_sections_from_research\"]\n",
    "        })\n",
    "        for s in state[\"sections\"]\n",
    "        if not s.research\n",
    "    ]\n",
    "\n",
    "\n",
    "def compile_final_report(state: ReportState) -> dict:\n",
    "    \"\"\"Assemble the final report in order.\"\"\"\n",
    "    sections = state[\"sections\"]\n",
    "    completed = {s.name: s.content for s in state[\"completed_sections\"]}\n",
    "    \n",
    "    # Update sections with completed content\n",
    "    for section in sections:\n",
    "        section.content = completed.get(section.name, section.content)\n",
    "    \n",
    "    # Compile in original order\n",
    "    final_report = \"\\n\\n\".join(s.content for s in sections if s.content)\n",
    "    return {\"final_report\": final_report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the full report graph\n",
    "builder = StateGraph(ReportState, output=ReportStateOutput)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"generate_report_plan\", generate_report_plan)\n",
    "builder.add_node(\"build_section_with_web_research\", section_graph)\n",
    "builder.add_node(\"gather_completed_sections\", gather_completed_sections)\n",
    "builder.add_node(\"write_final_sections\", write_final_sections)\n",
    "builder.add_node(\"compile_final_report\", compile_final_report)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"generate_report_plan\")\n",
    "builder.add_conditional_edges(\n",
    "    \"generate_report_plan\",\n",
    "    initiate_section_writing,\n",
    "    [\"build_section_with_web_research\"]\n",
    ")\n",
    "builder.add_edge(\"build_section_with_web_research\", \"gather_completed_sections\")\n",
    "builder.add_conditional_edges(\n",
    "    \"gather_completed_sections\",\n",
    "    initiate_final_section_writing,\n",
    "    [\"write_final_sections\"]\n",
    ")\n",
    "builder.add_edge(\"write_final_sections\", \"compile_final_report\")\n",
    "builder.add_edge(\"compile_final_report\", END)\n",
    "\n",
    "# Compile\n",
    "report_graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "display(Image(report_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Full Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a full report\n",
    "report_topic = \"Comparison of AI coding assistants: GitHub Copilot, Cursor, and Claude Code\"\n",
    "\n",
    "report = await report_graph.ainvoke({\n",
    "    \"topic\": report_topic,\n",
    "    \"report_structure\": REPORT_STRUCTURE,\n",
    "    \"number_of_queries\": 2,\n",
    "    \"tavily_topic\": \"general\",\n",
    "    \"tavily_days\": None\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the final report\n",
    "from IPython.display import Markdown\n",
    "Markdown(report[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print as plain text if preferred\n",
    "print(report[\"final_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Structured Outputs**: Using Pydantic models with `llm.with_structured_output()` for reliable data extraction\n",
    "2. **Subgraphs**: Composing complex workflows from smaller, reusable graph components\n",
    "3. **Parallel Execution**: Using LangGraph's `Send()` API to process sections concurrently\n",
    "4. **State Management**: Using `TypedDict` with reducers (like `operator.add`) for accumulating results\n",
    "5. **Async Search**: Leveraging `asyncio.gather()` for concurrent web searches\n",
    "\n",
    "### Key LangChain 1.0 Patterns Used\n",
    "\n",
    "| Pattern | Example |\n",
    "|---------|--------|\n",
    "| Provider-agnostic init | `init_chat_model(model=\"gpt-4o-mini\")` |\n",
    "| Structured outputs | `llm.with_structured_output(Sections)` |\n",
    "| TypedDict state | `class ReportState(TypedDict)` |\n",
    "| Tavily integration | Direct `TavilyClient` + async support |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
