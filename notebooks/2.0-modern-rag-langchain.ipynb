{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern RAG with LangChain 1.0 & LangGraph\n",
    "\n",
    "This notebook demonstrates Retrieval-Augmented Generation (RAG) using the latest LangChain patterns. We'll cover:\n",
    "\n",
    "1. **Basic RAG Pipeline** - Document loading, splitting, embedding, retrieval, generation\n",
    "2. **Conversational RAG** - Multi-turn conversations with chat history\n",
    "3. **Agentic RAG with LangGraph** - Dynamic, stateful RAG workflows\n",
    "4. **Structured Output RAG** - Extracting typed data from RAG responses\n",
    "\n",
    "---\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) enhances LLM responses by:\n",
    "1. **Retrieving** relevant documents from a knowledge base\n",
    "2. **Augmenting** the LLM's context with those documents\n",
    "3. **Generating** a response grounded in the retrieved information\n",
    "\n",
    "This reduces hallucinations and enables LLMs to answer questions about your own data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-community langchain-chroma langgraph pypdf chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Basic RAG Pipeline\n",
    "\n",
    "The RAG pipeline consists of:\n",
    "\n",
    "1. **Load** - Read documents from various sources\n",
    "2. **Split** - Break documents into smaller chunks\n",
    "3. **Embed** - Convert text to numerical vectors\n",
    "4. **Store** - Save vectors in a vector database\n",
    "5. **Retrieve** - Find relevant chunks for a query\n",
    "6. **Generate** - Use retrieved context to answer\n",
    "\n",
    "![RAG Pipeline](https://python.langchain.com/v0.1/assets/images/rag_pipeline-8f9d36e0a21e6aaa36d42b48a21f5bd2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Document Loading\n",
    "\n",
    "LangChain provides loaders for many file types. Let's load a PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the \"Attention Is All You Need\" paper (Transformer architecture)\n",
    "pdf_path = \"./assets-resources/attention-paper.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "# load_and_split() loads and splits by page\n",
    "documents = loader.load_and_split()\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages\")\n",
    "print(f\"\\nFirst page preview (first 500 chars):\")\n",
    "print(documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the metadata\n",
    "print(\"Document metadata:\")\n",
    "print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Text Splitting\n",
    "\n",
    "Large documents need to be split into smaller chunks for effective retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# RecursiveCharacterTextSplitter tries to keep related text together\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,      # Target chunk size in characters\n",
    "    chunk_overlap=200,    # Overlap between chunks for context continuity\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]  # Try these in order\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split {len(documents)} pages into {len(splits)} chunks\")\n",
    "print(f\"\\nExample chunk (first 300 chars):\")\n",
    "print(splits[5].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Embeddings & Vector Store\n",
    "\n",
    "Embeddings convert text to numerical vectors that capture semantic meaning. Similar texts have similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Create embeddings model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create vector store from documents\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    # persist_directory=\"./chroma_db\"  # Uncomment to persist to disk\n",
    ")\n",
    "\n",
    "print(f\"Created vector store with {vectorstore._collection.count()} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Retrieval\n",
    "\n",
    "A retriever finds the most relevant chunks for a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 4}  # Return top 4 most relevant chunks\n",
    ")\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What is the self-attention mechanism?\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Found {len(relevant_docs)} relevant chunks for: '{query}'\")\n",
    "print(\"\\nTop result:\")\n",
    "print(relevant_docs[0].page_content[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Generation with RAG Chain\n",
    "\n",
    "Now we combine retrieval with generation using LangChain's LCEL (LangChain Expression Language):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Create the RAG prompt\n",
    "system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Create the RAG chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question!\n",
    "question = \"What is the key innovation of the Transformer architecture?\"\n",
    "\n",
    "result = rag_chain.invoke({\"input\": question})\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nAnswer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the retrieved context\n",
    "print(\"\\nRetrieved context came from these sources:\")\n",
    "for i, doc in enumerate(result['context'], 1):\n",
    "    print(f\"  {i}. Page {doc.metadata.get('page', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More questions\n",
    "questions = [\n",
    "    \"What are positional encodings and why are they needed?\",\n",
    "    \"How does multi-head attention work?\",\n",
    "    \"What BLEU scores did the Transformer achieve?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    result = rag_chain.invoke({\"input\": q})\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Conversational RAG with Chat History\n",
    "\n",
    "For multi-turn conversations, we need to:\n",
    "1. Reformulate queries based on chat history\n",
    "2. Maintain context across turns\n",
    "\n",
    "The `create_history_aware_retriever` handles query reformulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Prompt for reformulating queries based on chat history\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"\"\"Given a chat history and the latest user question \n",
    "     which might reference context in the chat history, \n",
    "     formulate a standalone question which can be understood \n",
    "     without the chat history. Do NOT answer the question, \n",
    "     just reformulate it if needed and otherwise return it as is.\"\"\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Create history-aware retriever\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA prompt that includes chat history\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Create the conversational RAG chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "conversational_rag = create_retrieval_chain(\n",
    "    history_aware_retriever, \n",
    "    question_answer_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation\n",
    "chat_history = []\n",
    "\n",
    "# Turn 1\n",
    "question1 = \"What is self-attention?\"\n",
    "response1 = conversational_rag.invoke({\n",
    "    \"input\": question1,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "print(f\"User: {question1}\")\n",
    "print(f\"Assistant: {response1['answer']}\")\n",
    "\n",
    "# Update history\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question1),\n",
    "    AIMessage(content=response1['answer']),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 2 - references previous context (\"it\")\n",
    "question2 = \"How does it differ from traditional attention?\"\n",
    "response2 = conversational_rag.invoke({\n",
    "    \"input\": question2,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "print(f\"\\nUser: {question2}\")\n",
    "print(f\"Assistant: {response2['answer']}\")\n",
    "\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question2),\n",
    "    AIMessage(content=response2['answer']),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 3 - more follow-up\n",
    "question3 = \"What are the computational advantages?\"\n",
    "response3 = conversational_rag.invoke({\n",
    "    \"input\": question3,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "print(f\"\\nUser: {question3}\")\n",
    "print(f\"Assistant: {response3['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Agentic RAG with LangGraph\n",
    "\n",
    "Agentic RAG gives the LLM control over when to retrieve. The agent can:\n",
    "- Decide whether retrieval is needed\n",
    "- Reformulate queries if results are poor\n",
    "- Handle complex multi-step reasoning\n",
    "\n",
    "This is the most flexible RAG pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Create a retrieval tool\n",
    "@tool\n",
    "def retrieve_transformer_docs(query: str) -> str:\n",
    "    \"\"\"Retrieve documents about the Transformer architecture.\n",
    "    Use this when you need information from the 'Attention Is All You Need' paper.\n",
    "    \"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "tools = [retrieve_transformer_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLM with tools bound\n",
    "llm_with_tools = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).bind_tools(tools)\n",
    "\n",
    "# Define the agent node\n",
    "def call_model(state: MessagesState):\n",
    "    \"\"\"Call the model with current messages.\"\"\"\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are a helpful assistant that answers questions about the Transformer architecture.\n",
    "        Use the retrieve_transformer_docs tool when you need specific information from the paper.\n",
    "        If you can answer from general knowledge, you may do so, but cite the paper when relevant.\"\"\"\n",
    "    }\n",
    "    messages = [system_message] + state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,  # Routes to 'tools' if tool calls exist, else END\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "agentic_rag = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(agentic_rag.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agentic RAG\n",
    "response = agentic_rag.invoke({\n",
    "    \"messages\": [(\"user\", \"What is the scaled dot-product attention formula?\")]\n",
    "})\n",
    "\n",
    "# Show the conversation\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent might NOT use retrieval for general questions\n",
    "response = agentic_rag.invoke({\n",
    "    \"messages\": [(\"user\", \"What does BLEU stand for?\")]\n",
    "})\n",
    "\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex question requiring retrieval\n",
    "response = agentic_rag.invoke({\n",
    "    \"messages\": [(\"user\", \n",
    "        \"Compare the Transformer's training time to previous models mentioned in the paper.\"\n",
    "    )]\n",
    "})\n",
    "\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Structured Output RAG\n",
    "\n",
    "Combine RAG with structured output to extract specific data types from documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "class PaperSummary(BaseModel):\n",
    "    \"\"\"Structured summary of a research paper.\"\"\"\n",
    "    title: str = Field(description=\"Title of the paper\")\n",
    "    main_contribution: str = Field(description=\"The key innovation or contribution\")\n",
    "    key_results: List[str] = Field(description=\"List of important results/findings\")\n",
    "    limitations: Optional[List[str]] = Field(description=\"Any limitations mentioned\")\n",
    "    future_work: Optional[str] = Field(description=\"Suggested future directions\")\n",
    "\n",
    "# Create structured output LLM\n",
    "structured_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).with_structured_output(PaperSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Structured RAG chain\n",
    "structured_rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Extract a structured summary from the following research paper excerpts.\n",
    "    Focus on the main contributions, key results, and any limitations.\n",
    "    \n",
    "    Context:\n",
    "    {context}\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "structured_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | structured_rag_prompt\n",
    "    | structured_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get structured summary\n",
    "summary = structured_rag_chain.invoke(\"Summarize this paper's contributions and results\")\n",
    "\n",
    "print(f\"Title: {summary.title}\")\n",
    "print(f\"\\nMain Contribution: {summary.main_contribution}\")\n",
    "print(f\"\\nKey Results:\")\n",
    "for result in summary.key_results:\n",
    "    print(f\"  - {result}\")\n",
    "if summary.limitations:\n",
    "    print(f\"\\nLimitations:\")\n",
    "    for limit in summary.limitations:\n",
    "        print(f\"  - {limit}\")\n",
    "if summary.future_work:\n",
    "    print(f\"\\nFuture Work: {summary.future_work}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Quiz Generation from Documents\n",
    "\n",
    "A practical use case: generate quiz questions from the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuizQuestion(BaseModel):\n",
    "    question: str = Field(description=\"The quiz question\")\n",
    "    options: List[str] = Field(description=\"4 multiple choice options\")\n",
    "    correct_answer: str = Field(description=\"The correct option\")\n",
    "    explanation: str = Field(description=\"Brief explanation of the answer\")\n",
    "\n",
    "class Quiz(BaseModel):\n",
    "    topic: str = Field(description=\"Topic of the quiz\")\n",
    "    questions: List[QuizQuestion] = Field(description=\"List of quiz questions\")\n",
    "\n",
    "quiz_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3).with_structured_output(Quiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Generate a quiz based on the following research paper content.\n",
    "    Create challenging but fair questions that test understanding of key concepts.\n",
    "    \n",
    "    Context:\n",
    "    {context}\"\"\"),\n",
    "    (\"human\", \"Create {num_questions} quiz questions about {topic}\")\n",
    "])\n",
    "\n",
    "quiz_chain = (\n",
    "    {\"context\": (lambda x: x[\"topic\"]) | retriever | format_docs, \n",
    "     \"topic\": lambda x: x[\"topic\"],\n",
    "     \"num_questions\": lambda x: x[\"num_questions\"]}\n",
    "    | quiz_prompt\n",
    "    | quiz_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a quiz\n",
    "quiz = quiz_chain.invoke({\n",
    "    \"topic\": \"self-attention mechanism\",\n",
    "    \"num_questions\": 3\n",
    "})\n",
    "\n",
    "print(f\"Quiz: {quiz.topic}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, q in enumerate(quiz.questions, 1):\n",
    "    print(f\"\\nQ{i}: {q.question}\")\n",
    "    for j, opt in enumerate(q.options):\n",
    "        print(f\"   {chr(65+j)}) {opt}\")\n",
    "    print(f\"\\n   Correct: {q.correct_answer}\")\n",
    "    print(f\"   Explanation: {q.explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we covered modern RAG patterns:\n",
    "\n",
    "1. **Basic RAG Pipeline**:\n",
    "   - Document loading with `PyPDFLoader`\n",
    "   - Text splitting with `RecursiveCharacterTextSplitter`\n",
    "   - Embeddings with `OpenAIEmbeddings`\n",
    "   - Vector storage with `Chroma`\n",
    "   - Retrieval chain with `create_retrieval_chain`\n",
    "\n",
    "2. **Conversational RAG**:\n",
    "   - `create_history_aware_retriever` for query reformulation\n",
    "   - `MessagesPlaceholder` for chat history\n",
    "   - Multi-turn conversations with context\n",
    "\n",
    "3. **Agentic RAG with LangGraph**:\n",
    "   - `StateGraph` for workflow definition\n",
    "   - Tools for retrieval (`@tool` decorator)\n",
    "   - Dynamic decision-making (retrieve or not)\n",
    "\n",
    "4. **Structured Output RAG**:\n",
    "   - Pydantic models for typed responses\n",
    "   - Quiz generation from documents\n",
    "   - Paper summarization with structured output\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try loading your own documents (CSV, HTML, etc.)\n",
    "- Experiment with different embedding models\n",
    "- Add evaluation with RAGAS\n",
    "- Deploy with LangServe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
