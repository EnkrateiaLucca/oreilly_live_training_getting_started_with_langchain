{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lucas is trying a live Demo right now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain                                0.3.3\n",
      "langchain-chroma                         0.1.4\n",
      "langchain-cli                            0.0.24\n",
      "langchain-community                      0.3.2\n",
      "langchain-core                           0.3.10\n",
      "langchain-experimental                   0.3.2\n",
      "langchain-ollama                         0.2.0\n",
      "langchain-openai                         0.2.2\n",
      "langchain-text-splitters                 0.3.0\n",
      "langchainhub                             0.1.20\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list | grep langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# # Set OPENAI API Key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "# OR (load from .env file)\n",
    "# make sure you have python-dotenv installed\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Components\n",
    "\n",
    "1. Model\n",
    "2. Prompt Template\n",
    "3. Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Introducing people to the LangChain framework can be done effectively through a structured approach that combines theoretical understanding with practical application. Here’s a step-by-step guide to help you introduce LangChain to newcomers:\\n\\n### 1. **Overview of LangChain**\\n   - **What is LangChain?**: Start with a brief introduction to LangChain, explaining that it is a framework designed for building applications with language models. Highlight its capabilities in chaining together different components to create complex workflows.\\n   - **Use Cases**: Discuss various use cases such as chatbots, document analysis, data extraction, and more. This helps to contextualize the framework's utility.\\n\\n### 2. **Core Concepts**\\n   - **Components**: Explain the key components of LangChain, such as:\\n     - **Chains**: Sequences of calls to language models or other tools.\\n     - **Agents**: Components that can make decisions based on user input and context.\\n     - **Memory**: Mechanisms for maintaining state across interactions.\\n   - **Integration**: Discuss how LangChain integrates with various language models (like OpenAI's GPT, Hugging Face models, etc.) and other tools (APIs, databases).\\n\\n### 3. **Installation and Setup**\\n   - Provide a step-by-step guide on how to install LangChain. This can include:\\n     - Prerequisites (Python version, libraries).\\n     - Installation commands (e.g., using pip).\\n     - Setting up an IDE or environment (like Jupyter Notebook or VSCode).\\n\\n### 4. **Hands-On Examples**\\n   - **Basic Example**: Start with a simple example, such as creating a basic chatbot or a text summarizer. Walk through the code, explaining each part.\\n   - **Interactive Session**: Encourage participants to modify the example, such as changing prompts or adding new features, to see how it affects the output.\\n\\n### 5. **Advanced Features**\\n   - Once the basics are covered, introduce more advanced features:\\n     - **Custom Chains**: How to create and customize chains for specific tasks.\\n     - **Using Memory**: Demonstrate how to implement memory in applications for a more interactive experience.\\n     - **Integrating with APIs**: Show how to connect LangChain with external APIs for enhanced functionality.\\n\\n### 6. **Resources and Community**\\n   - Share resources for further learning:\\n     - Official documentation and tutorials.\\n     - Community forums, GitHub repositories, and social media groups.\\n     - Recommended courses or workshops.\\n   - Encourage joining the LangChain community for support and collaboration.\\n\\n### 7. **Project Ideas**\\n   - Suggest project ideas that participants can work on after the introduction, such as:\\n     - Building a personal assistant.\\n     - Creating a content generation tool.\\n     - Developing a question-answering system.\\n\\n### 8. **Q&A Session**\\n   - Allow time for questions and discussions. This helps clarify doubts and encourages deeper engagement with the material.\\n\\n### 9. **Follow-Up**\\n   - Provide a way for participants to follow up with you or each other after the session, whether through a mailing list, chat group, or forum.\\n\\nBy following this structured approach, you can effectively introduce newcomers to the LangChain framework, ensuring they have a solid understanding and practical experience to build upon.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 670, 'prompt_tokens': 21, 'total_tokens': 691, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'stop', 'logprobs': None}, id='run-8c7430d3-48c4-45ec-9f27-03d180cf3d59-0', usage_metadata={'input_tokens': 21, 'output_tokens': 670, 'total_tokens': 691, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "llm.invoke(\"What is the best way to introduce people to the LangChain framework?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing someone to the LangChain framework can be a bit challenging, but here are some tips to help you get started:\n",
      "\n",
      "1. **Start with the basics**: Explain what LangChain is and how it's different from other frameworks like Langit or Langs. Emphasize its focus on decentralized data management and its role in enabling blockchain-based applications.\n",
      "2. **Use analogies**: Help your audience understand complex concepts by using relatable analogies. For example, you can compare LangChain to a hub-and-spoke model, where the hub is the central database and the spokes are the individual nodes that interact with it.\n",
      "3. **Highlight key features**: Explain the main benefits of LangChain, such as its ability to manage data at scale, provide real-time updates, and enable secure access control. Emphasize how these features can improve the efficiency and effectiveness of blockchain-based applications.\n",
      "4. **Provide examples**: Share concrete examples of projects that use LangChain, such as decentralized social media platforms or supply chain management systems. This will help your audience visualize how the framework can be applied in real-world scenarios.\n",
      "5. **Use code snippets**: Provide code snippets to illustrate key concepts and demonstrate how LangChain can be used to build blockchain-based applications. This will give your audience a hands-on understanding of the framework's capabilities.\n",
      "6. **Leverage community resources**: Point your audience to existing resources, such as documentation, tutorials, or online forums, where they can learn more about LangChain and get support from the community.\n",
      "7. **Be prepared for questions**: Anticipate common questions and concerns, such as \"How does it compare to other frameworks?\" or \"What are the security implications of using LangChain?\" Be prepared to address these queries confidently and thoroughly.\n",
      "\n",
      "Here's an example introduction script:\n",
      "\n",
      "\"Welcome! Today, I want to introduce you to LangChain, a powerful framework for building blockchain-based applications. At its core, LangChain is designed to manage data at scale, providing real-time updates and secure access control. This makes it an ideal choice for decentralized applications that require high performance and reliability.\n",
      "\n",
      "\"To help you understand the framework better, let me provide some examples of how LangChain has been used in real-world projects. We can take a look at some code snippets to demonstrate its capabilities, and I'll also point you to some community resources where you can learn more.\"\n",
      "\n",
      "Remember to be enthusiastic, patient, and approachable when introducing someone to the LangChain framework. Good luck!\n"
     ]
    }
   ],
   "source": [
    "output = llm.invoke(\"What is the best way to introduce people to the LangChain framework?\")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nYou are a translation engine. You translate any text into French\\nUser input:\\nHello, how are you?\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are a translation engine. You translate any text into {language}\n",
    "User input:\n",
    "{text}\n",
    "\"\"\"\n",
    "prompt  = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "prompt.format(language=\"French\", text=\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bonjour, comment allez-vous?', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2024-10-14T16:36:24.754288Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 233090709, 'load_duration': 33704417, 'prompt_eval_count': 47, 'prompt_eval_duration': 126408000, 'eval_count': 8, 'eval_duration': 72005000}, id='run-0a9f3459-e690-4ee0-9ad3-fe53ed9e2d79-0', usage_metadata={'input_tokens': 47, 'output_tokens': 8, 'total_tokens': 55})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_translation_chain = prompt | llm\n",
    "\n",
    "simple_translation_chain.invoke({\"language\": \"French\", \"text\": \"Hello, how are you?\"}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bonjour, je m\\'appelle Lucas et je suis le maître du crêpe.\\n\\n(Note: I\\'ve kept the original meaning and tone of the sentence, but adapted it to fit common French usage. The term \"maître\" is often used in France to convey expertise or mastery, especially in culinary contexts.)', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2024-10-14T16:36:39.071252Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 804066875, 'load_duration': 29132875, 'prompt_eval_count': 55, 'prompt_eval_duration': 126092000, 'eval_count': 64, 'eval_duration': 647741000}, id='run-206425ac-9409-4d09-a2e5-4462c1a0a70e-0', usage_metadata={'input_tokens': 55, 'output_tokens': 64, 'total_tokens': 119})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_translation_chain.invoke({\"language\": \"French\", \"text\": \"Hello, my name is Lucas and I am the pancake master.\"}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Les crêpes sont la seule nourriture de petit déjeuner maîtrisée et belle.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"language\": \"French\", \"text\": \"Pancakes are the only masterful and beautiful breakfast food.\"}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MP's question: Does the prompts consume tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes they do, but when used with the model of course, and with a paid model, which is using an API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part2: How do we calculate token limits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UI\n",
    "Can we use multiple models in langchain like if we use one model to analyze the prompt and then forward it to a model that will better handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olá, como você está?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm_to_portuguese = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "llm_to_english = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "prompt_multi_language_translation = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a translation engine. You translate any text into {language}\n",
    "    User input:\n",
    "    {text}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain1 = prompt_multi_language_translation | llm_to_portuguese | output_parser\n",
    "\n",
    "chain1.invoke({\"language\": \"Portuguese\", \"text\": \"Hello, how are you?\"}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = prompt_multi_language_translation | llm_to_english | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.invoke({\"language\": \"English\", \"text\": \"Salut!\"}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the AI instructor bring a ladder to class? \\n\\nBecause they wanted to teach their students how to reach new heights in engineering!'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a translation engine. You translate any text into {language}\n",
    "    User input:\n",
    "    {text}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You take any piece of text and translate that into a joke.\n",
    "    User input:\n",
    "    {text}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain1 = prompt1 | llm_to_portuguese | output_parser\n",
    "chain2 = prompt2 | llm_to_portuguese | output_parser\n",
    "\n",
    "@chain\n",
    "def translate_and_joke(text):\n",
    "    language = \"English\"\n",
    "    output1 = chain1.invoke({\"language\": language, \"text\": text}, return_only_outputs=True)\n",
    "    output2 = chain2.invoke({\"text\": output1}, return_only_outputs=True)\n",
    "    \n",
    "    return output2\n",
    "\n",
    "translate_and_joke.invoke(\"Eu sou um instrutor de IA e engenheiro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
