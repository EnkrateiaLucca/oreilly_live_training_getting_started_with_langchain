{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tiktoken in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: faiss-cpu in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from faiss-cpu) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: duckduckgo-search in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (6.1.6)\n",
      "Requirement already satisfied: click>=8.1.7 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from duckduckgo-search) (8.1.7)\n",
      "Requirement already satisfied: pyreqwest-impersonate>=0.4.7 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from duckduckgo-search) (0.4.7)\n",
      "Requirement already satisfied: orjson>=3.10.4 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from duckduckgo-search) (3.10.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pydantic in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (2.7.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from pydantic) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from pydantic) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# uncomment and run below:\n",
    "%pip install -qU langchain\n",
    "%pip install -qU langchain-openai\n",
    "%pip install tiktoken\n",
    "%pip install faiss-cpu\n",
    "%pip install beautifulsoup4\n",
    "%pip install google-search-results\n",
    "%pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set OPENAI API Key\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"var: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain \n",
    "\n",
    "Working with LLMs involves in one way or another working with a specific type of abstraction: \"Prompts\".\n",
    "\n",
    "However, in the practical context of day-to-day tasks we expect LLMs to perform, these prompts won't be some static and dead type of abstraction. Instead we'll work with dynamic prompts re-usable prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lanchain\n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a framework that allows you to connect LLMs together by allowing you to work with modular components like prompt templates and chains giving you immense flexibility in creating tailored solutions powered by the capabilities of large language models.\n",
    "\n",
    "\n",
    "Its main features are:\n",
    "- **Components**: abstractions for working with LMs\n",
    "- **Off-the-shelf chains**: assembly of components for accomplishing certain higher-level tasks\n",
    "\n",
    "LangChain facilitates the creation of complex pipelines that leverage the connection of components like chains, prompt templates, output parsers and others to compose intricate pipelines that give you everything you need to solve a wide variety of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the core of LangChain, we have the following elements:\n",
    "\n",
    "- Models\n",
    "- Prompts\n",
    "- Output parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models**\n",
    "\n",
    "Models are nothing more than abstractions over the LLM APIs like the ChatGPT API.â€‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL='gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(model=MODEL, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That sounds exciting! Teaching about large language models (LLMs) can be a great opportunity to share knowledge about cutting-edge technology. Here are some key topics and tips you might consider including in your live training:\\n\\n### Key Topics to Cover\\n\\n1. **Introduction to LLMs**:\\n   - What are LLMs?\\n   - Brief history and evolution of language models (from n-grams to transformers).\\n\\n2. **How LLMs Work**:\\n   - Overview of the transformer architecture.\\n   - Explanation of concepts like attention mechanisms, embeddings, and tokenization.\\n\\n3. **Training LLMs**:\\n   - Data collection and preprocessing.\\n   - Training processes (supervised vs. unsupervised learning).\\n   - Fine-tuning and transfer learning.\\n\\n4. **Applications of LLMs**:\\n   - Natural language processing tasks (text generation, summarization, translation, etc.).\\n   - Use cases in various industries (healthcare, finance, customer service).\\n\\n5. **Ethical Considerations**:\\n   - Bias in language models and its implications.\\n   - Responsible AI usage and the importance of transparency.\\n\\n6. **Hands-On Demonstration**:\\n   - Show how to use an LLM (e.g., OpenAI's GPT, Hugging Face Transformers).\\n   - Simple coding examples or interactive tools.\\n\\n7. **Future of LLMs**:\\n   - Trends in research and development.\\n   - Potential advancements and challenges.\\n\\n### Tips for Effective Training\\n\\n- **Engage Your Audience**: Encourage questions and discussions throughout the session. Use polls or quizzes to keep participants engaged.\\n  \\n- **Use Visual Aids**: Incorporate slides, diagrams, and videos to illustrate complex concepts.\\n\\n- **Provide Resources**: Share links to papers, articles, and tools for further learning.\\n\\n- **Encourage Hands-On Practice**: If possible, provide a coding environment where participants can experiment with LLMs.\\n\\n- **Be Prepared for Questions**: Familiarize yourself with common questions and concerns about LLMs, including technical, ethical, and practical aspects.\\n\\n- **Follow Up**: After the training, consider sending a summary or additional resources to participants to reinforce their learning.\\n\\nIf you have specific areas you want to focus on or any questions about the content, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 467, 'prompt_tokens': 18, 'total_tokens': 485, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bba3c8e70b', 'finish_reason': 'stop', 'logprobs': None}, id='run-92129cbc-c7e9-4bc8-b77b-90cce6777fa2-0', usage_metadata={'input_tokens': 18, 'output_tokens': 467, 'total_tokens': 485, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = chat_model.invoke(\"I am teaching a live-training about LLMs!\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That sounds exciting! Teaching about large language models (LLMs) can be a great opportunity to share knowledge about cutting-edge technology. Here are some key topics and tips you might consider including in your live training:\n",
      "\n",
      "### Key Topics to Cover\n",
      "\n",
      "1. **Introduction to LLMs**:\n",
      "   - What are LLMs?\n",
      "   - Brief history and evolution of language models (from n-grams to transformers).\n",
      "\n",
      "2. **How LLMs Work**:\n",
      "   - Overview of the transformer architecture.\n",
      "   - Explanation of concepts like attention mechanisms, embeddings, and tokenization.\n",
      "\n",
      "3. **Training LLMs**:\n",
      "   - Data collection and preprocessing.\n",
      "   - Training processes (supervised vs. unsupervised learning).\n",
      "   - Fine-tuning and transfer learning.\n",
      "\n",
      "4. **Applications of LLMs**:\n",
      "   - Natural language processing tasks (text generation, summarization, translation, etc.).\n",
      "   - Use cases in various industries (healthcare, finance, customer service).\n",
      "\n",
      "5. **Ethical Considerations**:\n",
      "   - Bias in language models and its implications.\n",
      "   - Responsible AI usage and the importance of transparency.\n",
      "\n",
      "6. **Hands-On Demonstration**:\n",
      "   - Show how to use an LLM (e.g., OpenAI's GPT, Hugging Face Transformers).\n",
      "   - Simple coding examples or interactive tools.\n",
      "\n",
      "7. **Future of LLMs**:\n",
      "   - Trends in research and development.\n",
      "   - Potential advancements and challenges.\n",
      "\n",
      "### Tips for Effective Training\n",
      "\n",
      "- **Engage Your Audience**: Encourage questions and discussions throughout the session. Use polls or quizzes to keep participants engaged.\n",
      "  \n",
      "- **Use Visual Aids**: Incorporate slides, diagrams, and videos to illustrate complex concepts.\n",
      "\n",
      "- **Provide Resources**: Share links to papers, articles, and tools for further learning.\n",
      "\n",
      "- **Encourage Hands-On Practice**: If possible, provide a coding environment where participants can experiment with LLMs.\n",
      "\n",
      "- **Be Prepared for Questions**: Familiarize yourself with common questions and concerns about LLMs, including technical, ethical, and practical aspects.\n",
      "\n",
      "- **Follow Up**: After the training, consider sending a summary or additional resources to participants to reinforce their learning.\n",
      "\n",
      "If you have specific areas you want to focus on or any questions about the content, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can predict outputs from both LLMs and ChatModels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic components are:\n",
    "\n",
    "- Models\n",
    "- Prompt templates\n",
    "- Output parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Show me 5 examples of this concept: animal'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"Show me 5 examples of this concept: {concept}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "prompt.format(concept=\"animal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"concept\": \"animal\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! Here are five examples of different animals, each representing a unique category:\\n\\n1. **Mammal**: **Elephant** - The largest land animal, known for its intelligence, social behavior, and strong familial bonds.\\n\\n2. **Bird**: **Bald Eagle** - A bird of prey known for its impressive wingspan and as a symbol of strength and freedom in the United States.\\n\\n3. **Reptile**: **Green Iguana** - A large lizard native to Central and South America, known for its vibrant green color and ability to adapt to various environments.\\n\\n4. **Amphibian**: **Poison Dart Frog** - A small, brightly colored frog found in Central and South America, known for its toxic skin and vibrant colors that serve as a warning to predators.\\n\\n5. **Fish**: **Clownfish** - A small, colorful fish that lives in sea anemones, known for its symbiotic relationship with the anemone and its popularity due to the animated film \"Finding Nemo.\"\\n\\nThese examples illustrate the diversity of the animal kingdom across different classes.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure! Here are five examples of different animals, each representing a unique category:\n",
       "\n",
       "1. **Mammal**: **Elephant** - The largest land animal, known for its intelligence, social behavior, and strong familial bonds.\n",
       "\n",
       "2. **Bird**: **Bald Eagle** - A bird of prey known for its impressive wingspan and as a symbol of strength and freedom in the United States.\n",
       "\n",
       "3. **Reptile**: **Green Iguana** - A large lizard native to Central and South America, known for its vibrant green color and ability to adapt to various environments.\n",
       "\n",
       "4. **Amphibian**: **Poison Dart Frog** - A small, brightly colored frog found in Central and South America, known for its toxic skin and vibrant colors that serve as a warning to predators.\n",
       "\n",
       "5. **Fish**: **Clownfish** - A small, colorful fish that lives in sea anemones, known for its symbiotic relationship with the anemone and its popularity due to the animated film \"Finding Nemo.\"\n",
       "\n",
       "These examples illustrate the diversity of the animal kingdom across different classes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "Markdown(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the predict method over a string input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here are some cute name ideas for a dog that loves to nap:\\n\\n1. Snoozer\\n2. Naptime\\n3. Dreamer\\n4. Dozer\\n5. Snuggles\\n6. Siesta\\n7. Zzz\\n8. Pillow\\n9. Drowse\\n10. Napster\\n\\nChoose one that fits your dog's personality!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 21, 'total_tokens': 95, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_818c284075', 'finish_reason': 'stop', 'logprobs': None}, id='run-d719f930-17c2-44b3-afad-f5ad8c24b924-0', usage_metadata={'input_tokens': 21, 'output_tokens': 74, 'total_tokens': 95, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What would be a good name for a dog that loves to nap??\"\n",
    "chat_model.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompts**\n",
    "\n",
    "The same works for prompts. Now, prompts are pieces of text we feed to LLMs, and LangChain allows you to work with prompt templates.\n",
    "\n",
    "Prompt Templates are useful abstractions for reusing prompts and they are used to provide context for the specific task that the language model needs to complete. \n",
    "\n",
    "A simple example is a `PromptTemplate` that formats a string into a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What is a good dog name for a dog that loves to sleeping?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts  import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"What is a good dog name for a dog that loves to {activity}?\")\n",
    "prompt.format(activity=\"sleeping\")\n",
    "# Output: \"What is a good dog name for a dog that loves to nap?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here are some cute and fitting names for a dog that loves to sleep:\\n\\n1. **Snoozer**\\n2. **Napster**\\n3. **Dozer**\\n4. **Slumber**\\n5. **Dreamer**\\n6. **Pillow**\\n7. **Cuddles**\\n8. **Naptime**\\n9. **Resty**\\n10. **Zzz**\\n\\nChoose one that resonates with your dog's personality!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 21, 'total_tokens': 109, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bba3c8e70b', 'finish_reason': 'stop', 'logprobs': None}, id='run-c9f7ab7a-d302-4706-8e45-e063ba228f76-0', usage_metadata={'input_tokens': 21, 'output_tokens': 88, 'total_tokens': 109, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | chat_model\n",
    "\n",
    "chain.invoke({'activity': 'sleeping'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Parsers**\n",
    "\n",
    "OutputParsers convert the raw output from an LLM into a format that can be used downstream. Here is an example of an OutputParser that converts a comma-separated list into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A great name for a dog that loves landscapes could be \"Scenic.\" Other options might include \"Vista,\" \"Meadow,\" \"Canyon,\" or \"Willow.\" These names evoke the beauty of nature and the outdoors, reflecting your dog\\'s love for landscapes!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 22, 'total_tokens': 75, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bba3c8e70b', 'finish_reason': 'stop', 'logprobs': None}, id='run-397bd2a2-0c2b-4c3e-aec6-42b8f0b4ca23-0', usage_metadata={'input_tokens': 22, 'output_tokens': 53, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"activity\": \"Landscapes\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Certainly! Here are five fundamental concepts that are essential to understand when learning about Artificial Neural Networks (ANNs):\\n\\n1. **Neurons and Activation Functions**:\\n   - At the core of ANNs are artificial neurons, which are inspired by biological neurons. Each neuron receives inputs, processes them, and produces an output. The output is typically passed through an activation function, which introduces non-linearity into the model. Common activation functions include Sigmoid, ReLU (Rectified Linear Unit), and Tanh. Understanding how these functions work and their impact on the network's performance is crucial.\\n\\n2. **Network Architecture**:\\n   - The architecture of an ANN refers to its structure, including the number of layers (input, hidden, and output layers) and the number of neurons in each layer. Different architectures (e.g., feedforward networks, convolutional neural networks, recurrent neural networks) are suited for different types of tasks. Learning how to design and choose the appropriate architecture for a given problem is fundamental.\\n\\n3. **Forward Propagation and Backpropagation**:\\n   - Forward propagation is the process of passing input data through the network to obtain an output. Backpropagation is the algorithm used to update the weights of the network based on the error of the output compared to the expected result. This involves calculating gradients and applying optimization techniques (like gradient descent) to minimize the loss function. Understanding these processes is key to training ANNs effectively.\\n\\n4. **Loss Functions and Optimization**:\\n   - The loss function quantifies how well the ANN's predictions match the actual target values. Common loss functions include Mean Squared Error for regression tasks and Cross-Entropy Loss for classification tasks. Optimization algorithms (such as Stochastic Gradient Descent, Adam, and RMSprop) are used to minimize the loss function during training. Knowing how to select and implement the right loss function and optimization method is critical for model performance.\\n\\n5. **Overfitting and Regularization**:\\n   - Overfitting occurs when a model learns the training data too well, capturing noise and outliers, which leads to poor generalization on unseen data. Regularization techniques, such as L1/L2 regularization, dropout, and early stopping, help mitigate overfitting by adding constraints or modifying the training process. Understanding these concepts is essential for building robust and generalizable neural network models.\\n\\nThese concepts form the foundation for understanding and working with artificial neural networks, enabling learners to build, train, and optimize their own models effectively.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=MODEL, temperature=0.0)\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Write 5 concepts that are fundamental to learn about {topic}.\n",
    "                                          \"\"\")\n",
    "chain = prompt | llm | output_parser\n",
    "chain.invoke({\"topic\": \"Artificial Neural Networks\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to an LLM, and then pass the output through an output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so these are the basics of langchain. But how can we leverage these abstraction capabilities inside our LLM app application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to put everything together LangChain allows you to build something called \"chains\", which are components that connect prompts, llms and output parsers into a building block that allows you to create more interesting and complex functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the example below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what the chain is doing is connecting these basic components (the LLM and the prompt template) into\n",
    "a block that can be run separately. The chain allows you to turn workflows using LLLMs into this modular process of composing components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the newer versions of LangChain have a new representation language to create these chains (and more) known as LCEL or LangChain expression language, which is a declarative way to easily compose chains together. The same example as above expressed in this LCEL format would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Certainly! Here are five fundamental concepts to understand about sleep:\\n\\n1. **Sleep Stages and Cycles**: Sleep is divided into several stages, primarily categorized into Non-Rapid Eye Movement (NREM) and Rapid Eye Movement (REM) sleep. A typical sleep cycle lasts about 90 minutes and includes multiple stages, each serving different functions, such as physical restoration, memory consolidation, and emotional regulation.\\n\\n2. **Circadian Rhythms**: These are the natural, internal processes that regulate the sleep-wake cycle and repeat roughly every 24 hours. Circadian rhythms are influenced by external cues like light and temperature, and they play a crucial role in determining sleep patterns, alertness, and overall health.\\n\\n3. **Sleep Hygiene**: This refers to a set of practices and habits that promote good quality sleep. Key components include maintaining a consistent sleep schedule, creating a comfortable sleep environment, limiting exposure to screens before bedtime, and avoiding stimulants like caffeine and nicotine close to bedtime.\\n\\n4. **Impact of Sleep on Health**: Sleep is essential for physical and mental health. Poor sleep quality or insufficient sleep can lead to a range of health issues, including weakened immune function, increased risk of chronic conditions (like obesity, diabetes, and cardiovascular disease), impaired cognitive function, and mood disorders.\\n\\n5. **Sleep Disorders**: Understanding common sleep disorders, such as insomnia, sleep apnea, restless legs syndrome, and narcolepsy, is crucial. These conditions can significantly affect sleep quality and overall health, and recognizing their symptoms can lead to timely diagnosis and treatment.\\n\\nThese concepts provide a foundational understanding of sleep and its importance in daily life and overall well-being.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 336, 'prompt_tokens': 21, 'total_tokens': 357, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bba3c8e70b', 'finish_reason': 'stop', 'logprobs': None}, id='run-00b6f78b-e7b0-4bc1-a98d-7df53d65b706-0', usage_metadata={'input_tokens': 21, 'output_tokens': 336, 'total_tokens': 357, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"topic\": \"sleep\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama pull llama3 in the terminal\n",
    "llm = ChatOllama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_ollama.chat_models.ChatOllama"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are five fundamental concepts to learn about neuroscience of sleep:\\n\\n1. **Sleep Cycles**: Sleep is composed of multiple stages, including light and deep sleep. Understanding the different stages of sleep is crucial in understanding how the brain functions during rest. A full sleep cycle typically lasts around 90-120 minutes and consists of three stages:\\n\\t* Stage 1: Transition from wakefulness to sleep\\n\\t* Stage 2: Light sleep with slow brain waves\\n\\t* Stage 3: Deep sleep with slow delta brain waves\\n2. **Neurotransmitters and Hormones**: Neurotransmitters such as GABA, glutamate, and serotonin play a crucial role in regulating sleep-wake cycles. Hormones like melatonin, cortisol, and adenosine also influence sleep patterns. Understanding the roles of these chemicals is essential to grasping how the brain controls sleep.\\n3. **Brain Regions Involved in Sleep**: Different brain regions are active during different stages of sleep. For example:\\n\\t* The hippocampus, involved in memory consolidation\\n\\t* The prefrontal cortex, responsible for executive functions and decision-making\\n\\t* The amygdala, involved in emotional processing\\n\\t* The hypothalamus, regulates body temperature, heart rate, and other physiological processes\\n4. **Sleep Stage Transitions**: Understanding how the brain transitions between different stages of sleep is critical to grasping sleep regulation. Sleep stage transitions are influenced by factors such as:\\n\\t* Wakefulness and arousal\\n\\t* REM (rapid eye movement) and NREM (non-rapid eye movement) sleep\\n\\t* Age, lifestyle, and health status\\n5. **Sleep Disorders and Brain Alterations**: Sleep disorders such as insomnia, narcolepsy, and restless leg syndrome can result from alterations in brain structure and function. Understanding the neural mechanisms underlying these conditions is essential for developing effective treatments.\\n\\nMastering these concepts provides a solid foundation for understanding the complexities of sleep neuroscience and its role in maintaining overall health and well-being.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"neuroscience of sleep\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now the output is an `AIMessage()` object, which represents LangChain's way to abstract the output from an LLM model like ChatGPT or others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These building blocks and abstractions that LangChain provides are what makes this library so unique, because it gives you the tools you didn't know you need it to build awesome stuff powered by LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our First LangChain App\n",
    "\n",
    "See `./1.1-langchain-app.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple chain for summarization of content. \n",
    "\n",
    "Your chain should:\n",
    "\n",
    "- A prompt template with one or more variables\n",
    "- A model like ChatGPT or other (you can use local models if you'd like, I recommend `ChatOllama` for that!)\n",
    "- Optional: use output parsing or just fetch the string output at the end!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Answer\n",
    "\n",
    "Let's make use of the `ChatPromptTemplate` to abstract away the following pieces of the prompt: \n",
    "- `content` - the text content to be summarized  \n",
    "- `summary_format` - the format in which we want the summary to be presented (like bullet points and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Summarize this: This is a test.. The output should be in the following format: One word summary.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Summarize this: {content}. The output should be in the following format: {summary_format}.\")\n",
    "\n",
    "# We can look at a simple example to illustrate what that prompt is doing\n",
    "prompt.format(content=\"This is a test.\", summary_format=\"One word summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have our prompt template done, let's load the llm and create a nice chain to put everything together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_chat =  ChatOpenAI()\n",
    "chain = prompt | llm_chat # This is the Pipe symbol! from LCEL that connect model to prompt!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have our chain we can run some tests. The cool thing about working with LLMs is that you can use them to create examples for simple tests like this (avoiding the annoynace of searching online for some piece of text, copying and pasting etc...). So, let's generate a few examples of tests below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='One of the most fascinating aspects of human-machine interaction is the concept of artificial intelligence and its ability to mimic human behavior. From chatbots that can hold conversations with users to voice assistants that can schedule appointments and answer questions, AI has become increasingly integrated into our daily lives. This technology has the potential to revolutionize industries such as healthcare, transportation, and finance, but also raises concerns about privacy, security, and the ethical implications of relying on machines to make decisions for us.\\n\\nAnother important aspect of human-machine interaction is the idea of augmented reality and virtual reality, which allow users to immerse themselves in digital environments and interact with virtual objects. These technologies have the potential to enhance education, entertainment, and communication, but also blur the lines between reality and fiction. As we continue to develop more advanced human-machine interfaces, it will be crucial to consider how these technologies impact our cognitive abilities, social interactions, and overall well-being. Ultimately, the future of human-machine interaction will be shaped by our ability to balance the benefits of technology with the potential risks and challenges it presents.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 214, 'prompt_tokens': 25, 'total_tokens': 239, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a6216b71-ecc3-4def-9f89-426a45a290a3-0', usage_metadata={'input_tokens': 25, 'output_tokens': 214, 'total_tokens': 239, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='As technology continues to advance, the relationship between humans and machines becomes increasingly complex. One of the key areas of focus in human-machine interaction is how to make these interactions more seamless and intuitive for users. This includes designing interfaces that are easy to navigate and understand, as well as ensuring that machines can effectively interpret and respond to human input.\\n\\nAnother important aspect of human-machine interaction is the ethical considerations that come into play when integrating technology into our daily lives. With the rise of artificial intelligence and automation, there are concerns about the potential impact on jobs, privacy, and security. It is essential for designers and developers to consider these ethical implications and work towards creating technology that enhances human capabilities rather than replacing them. By striving for a balance between human needs and machine capabilities, we can ensure that human-machine interaction is beneficial and empowering for all users.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 167, 'prompt_tokens': 25, 'total_tokens': 192, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bedebea2-6f4f-45ef-9372-59df2741c457-0', usage_metadata={'input_tokens': 25, 'output_tokens': 167, 'total_tokens': 192, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='One fascinating aspect of human-machine interaction is the concept of artificial intelligence being able to understand and predict human behavior. With advancements in technology, machines are becoming more adept at analyzing data and interpreting patterns to anticipate how humans will react in certain situations. This ability has the potential to revolutionize various industries, such as marketing, healthcare, and transportation, by allowing machines to tailor their actions to best meet the needs and preferences of individuals. However, this also raises ethical concerns about privacy and autonomy, as machines have the power to manipulate human behavior based on their predictions.\\n\\nAnother intriguing aspect of human-machine interaction is the development of emotional intelligence in machines. Researchers are exploring ways to program robots and AI systems to recognize and respond to human emotions, such as empathy, frustration, and joy. This could lead to more personalized and intuitive interactions between humans and machines, resulting in a more seamless and enjoyable experience for users. However, the challenge lies in ensuring that machines can accurately interpret and respond to complex human emotions, as this could have significant implications for the quality of interactions and the overall well-being of individuals.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 217, 'prompt_tokens': 25, 'total_tokens': 242, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3b1eb7ee-95cb-4322-8d4e-7cd1ebbb7b8e-0', usage_metadata={'input_tokens': 25, 'output_tokens': 217, 'total_tokens': 242, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples = 3\n",
    "examples = []\n",
    "for i in range(num_examples):\n",
    "    examples.append(llm_chat.invoke(\"Create a piece of text with 2 paragraphs about a random topic regarding human-machine interaction.\"))\n",
    "\n",
    "examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now that we have our examples, let's run our chain on them and check out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='- Artificial intelligence is a fascinating aspect of human-machine interaction that mimics human behavior\\n- AI is integrated into daily life through chatbots, voice assistants, and has the potential to revolutionize industries\\n- Concerns arise about privacy, security, and ethical implications of relying on machines for decision-making\\n- Augmented reality and virtual reality enhance education, entertainment, and communication\\n- These technologies blur the lines between reality and fiction\\n- It is crucial to consider the impact of advanced human-machine interfaces on cognitive abilities, social interactions, and overall well-being\\n- The future of human-machine interaction depends on balancing the benefits of technology with potential risks and challenges', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 130, 'prompt_tokens': 460, 'total_tokens': 590, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1a8187f9-3229-48e0-bfb4-1418d468e66e-0', usage_metadata={'input_tokens': 460, 'output_tokens': 130, 'total_tokens': 590, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_format = \"bullet points\"\n",
    "\n",
    "outputs = []\n",
    "for ex in examples:\n",
    "    outputs.append(chain.invoke({\"content\": ex, \"summary_format\": summary_format}))\n",
    "\n",
    "# Let's display one example output\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So it seems our chain worked and we generated some summaries! Let's visualize all the summaries generated in a neat way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Output 0 \n",
       " - Artificial intelligence is a fascinating aspect of human-machine interaction that mimics human behavior\n",
       "- AI is integrated into daily life through chatbots, voice assistants, and has the potential to revolutionize industries\n",
       "- Concerns arise about privacy, security, and ethical implications of relying on machines for decision-making\n",
       "- Augmented reality and virtual reality enhance education, entertainment, and communication\n",
       "- These technologies blur the lines between reality and fiction\n",
       "- It is crucial to consider the impact of advanced human-machine interfaces on cognitive abilities, social interactions, and overall well-being\n",
       "- The future of human-machine interaction depends on balancing the benefits of technology with potential risks and challenges"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Output 1 \n",
       " - The relationship between humans and machines is becoming more complex as technology advances.\n",
       "- The focus in human-machine interaction is on making interactions seamless and intuitive for users.\n",
       "- Designing easy-to-navigate interfaces and ensuring machines can interpret human input are key aspects.\n",
       "- Ethical considerations arise when integrating technology into daily lives, such as concerns about job impact, privacy, and security.\n",
       "- Designers and developers must consider ethical implications and aim to create technology that enhances human capabilities.\n",
       "- Balancing human needs with machine capabilities is crucial to ensuring beneficial and empowering human-machine interaction for all users."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Output 2 \n",
       " - Artificial intelligence can understand and predict human behavior by analyzing data and interpreting patterns\n",
       "- This ability has the potential to revolutionize industries like marketing, healthcare, and transportation\n",
       "- Raises ethical concerns about privacy and autonomy as machines can manipulate human behavior based on predictions\n",
       "- Researchers are exploring emotional intelligence in machines to recognize and respond to human emotions\n",
       "- Developing emotional intelligence in machines could lead to more personalized interactions and a seamless user experience\n",
       "- Ensuring machines can accurately interpret and respond to complex human emotions is a challenge with significant implications for interactions and individual well-being"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "for i in range(num_examples):\n",
    "    display(Markdown(f\"Output {i} \\n {outputs[i].content}\"))\n",
    "# Markdown(f\"**Input**: {examples[0]}\\n\\n**Output**: {outputs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our summaries worked, and we were able to apply a given summary format to all of them.\n",
    "\n",
    "LangChain is an extremely powerful library to work with abstractions like these and throughout this course we hope to give you a gliimpse of the cool stuff you can build with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain Expression Language ([LCEL](https://python.langchain.com/docs/get_started/introduction))\n",
    "\n",
    "LCEL is a declarative way to compose chains of components. \n",
    "\n",
    "What does that mean? Means its an easy way to put useful building blocks together.\n",
    "\n",
    "\n",
    "Here's quick summary of the LangChain Expression Language (LCEL) page:\n",
    "\n",
    "- LCEL Basics: Simplifies building complex chains from basic components using a unified interface and composition primitives.\n",
    "\n",
    "- Unified Interface: Every LCEL object implements the Runnable interface, supporting common invocation methods like invoke, batch, stream, ainvoke, and more.\n",
    "\n",
    "- Composition Primitives: LCEL provides tools for composing chains, parallelizing components, adding fallbacks, and dynamically configuring internal chain elements.\n",
    "\n",
    "- Model Flexibility: LCEL allows for easy switching between different models and providers (like OpenAI or Anthropic), and runtime configurability of chat models or LLMs.\n",
    "\n",
    "- Advanced Features: LCEL features things like logging intermediate results with LangSmith integration and adding fallback logic for enhanced reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, cool but what is a component?\n",
    "\n",
    "A component is something that implements the `Runnable` protocol.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok....and what is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's an object with some nice desirable features like:\n",
    "\n",
    "- input and output schemas (describe what that object takes as input and the structure of its output)\n",
    "\n",
    "Some nice methods are:\n",
    "\n",
    "- `invoke` [ainvoke]\n",
    "- `stream` [astream]\n",
    "- `batch` [abatch]\n",
    "\n",
    "\n",
    "Below is a list of common i/o types for each component:\n",
    "\n",
    "![](./assets-resources/components_input_type_output_type.png)\n",
    "\n",
    "[source for the image](https://python.langchain.com/docs/expression_language/interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "output = model.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'AIMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ai',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessage',\n",
       "   'type': 'object'},\n",
       "  'AIMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Message chunk from an AI.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'AIMessageChunk',\n",
       "     'default': 'AIMessageChunk',\n",
       "     'enum': ['AIMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None},\n",
       "    'tool_call_chunks': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCallChunk'},\n",
       "     'title': 'Tool Call Chunks',\n",
       "     'type': 'array'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ChatMessage': {'additionalProperties': True,\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'chat',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessage',\n",
       "   'type': 'object'},\n",
       "  'ChatMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Chat Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ChatMessageChunk',\n",
       "     'default': 'ChatMessageChunk',\n",
       "     'enum': ['ChatMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ChatPromptValueConcrete': {'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.',\n",
       "   'properties': {'messages': {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'},\n",
       "       {'$ref': '#/$defs/HumanMessage'},\n",
       "       {'$ref': '#/$defs/ChatMessage'},\n",
       "       {'$ref': '#/$defs/SystemMessage'},\n",
       "       {'$ref': '#/$defs/FunctionMessage'},\n",
       "       {'$ref': '#/$defs/ToolMessage'},\n",
       "       {'$ref': '#/$defs/AIMessageChunk'},\n",
       "       {'$ref': '#/$defs/HumanMessageChunk'},\n",
       "       {'$ref': '#/$defs/ChatMessageChunk'},\n",
       "       {'$ref': '#/$defs/SystemMessageChunk'},\n",
       "       {'$ref': '#/$defs/FunctionMessageChunk'},\n",
       "       {'$ref': '#/$defs/ToolMessageChunk'}]},\n",
       "     'title': 'Messages',\n",
       "     'type': 'array'},\n",
       "    'type': {'const': 'ChatPromptValueConcrete',\n",
       "     'default': 'ChatPromptValueConcrete',\n",
       "     'enum': ['ChatPromptValueConcrete'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['messages'],\n",
       "   'title': 'ChatPromptValueConcrete',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'function',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessage',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Function Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'FunctionMessageChunk',\n",
       "     'default': 'FunctionMessageChunk',\n",
       "     'enum': ['FunctionMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'HumanMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'human',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessage',\n",
       "   'type': 'object'},\n",
       "  'HumanMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Human Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'HumanMessageChunk',\n",
       "     'default': 'HumanMessageChunk',\n",
       "     'enum': ['HumanMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'InputTokenDetails': {'description': 'Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"cache_creation\": 200,\\n            \"cache_read\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'cache_creation': {'title': 'Cache Creation', 'type': 'integer'},\n",
       "    'cache_read': {'title': 'Cache Read', 'type': 'integer'}},\n",
       "   'title': 'InputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'InvalidToolCall': {'description': 'Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Error'},\n",
       "    'type': {'const': 'invalid_tool_call',\n",
       "     'enum': ['invalid_tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error'],\n",
       "   'title': 'InvalidToolCall',\n",
       "   'type': 'object'},\n",
       "  'OutputTokenDetails': {'description': 'Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"reasoning\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'reasoning': {'title': 'Reasoning', 'type': 'integer'}},\n",
       "   'title': 'OutputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'StringPromptValue': {'description': 'String prompt value.',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'type': {'const': 'StringPromptValue',\n",
       "     'default': 'StringPromptValue',\n",
       "     'enum': ['StringPromptValue'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['text'],\n",
       "   'title': 'StringPromptValue',\n",
       "   'type': 'object'},\n",
       "  'SystemMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'system',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessage',\n",
       "   'type': 'object'},\n",
       "  'SystemMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'System Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'SystemMessageChunk',\n",
       "     'default': 'SystemMessageChunk',\n",
       "     'enum': ['SystemMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolCall': {'description': 'Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"name\": \"foo\",\\n            \"args\": {\"a\": 1},\\n            \"id\": \"123\"\\n        }\\n\\n    This represents a request to call the tool named \"foo\" with arguments {\"a\": 1}\\n    and an identifier of \"123\".',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'object'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'type': {'const': 'tool_call',\n",
       "     'enum': ['tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id'],\n",
       "   'title': 'ToolCall',\n",
       "   'type': 'object'},\n",
       "  'ToolCallChunk': {'description': 'A chunk of a tool call (e.g., as part of a stream).\\n\\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    left_chunks = [ToolCallChunk(name=\"foo\", args=\\'{\"a\":\\', index=0)]\\n    right_chunks = [ToolCallChunk(name=None, args=\\'1}\\', index=0)]\\n\\n    (\\n        AIMessageChunk(content=\"\", tool_call_chunks=left_chunks)\\n        + AIMessageChunk(content=\"\", tool_call_chunks=right_chunks)\\n    ).tool_call_chunks == [ToolCallChunk(name=\\'foo\\', args=\\'{\"a\":1}\\', index=0)]',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "     'title': 'Index'},\n",
       "    'type': {'const': 'tool_call_chunk',\n",
       "     'enum': ['tool_call_chunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'index'],\n",
       "   'title': 'ToolCallChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A ToolMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content=\\'42\\', tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\')\\n\\n\\nExample: A ToolMessage where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    .. versionadded:: 0.2.17\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        tool_output = {\\n            \"stdout\": \"From the graph we can see that the correlation between x and y is ...\",\\n            \"stderr\": None,\\n            \"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"},\\n        }\\n\\n        ToolMessage(\\n            content=tool_output[\"stdout\"],\\n            artifact=tool_output,\\n            tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\',\\n        )\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'tool',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessage',\n",
       "   'type': 'object'},\n",
       "  'ToolMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Tool Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ToolMessageChunk',\n",
       "     'default': 'ToolMessageChunk',\n",
       "     'enum': ['ToolMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'UsageMetadata': {'description': 'Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"input_tokens\": 350,\\n            \"output_tokens\": 240,\\n            \"total_tokens\": 590,\\n            \"input_token_details\": {\\n                \"audio\": 10,\\n                \"cache_creation\": 200,\\n                \"cache_read\": 100,\\n            },\\n            \"output_token_details\": {\\n                \"audio\": 10,\\n                \"reasoning\": 200,\\n            }\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.',\n",
       "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
       "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
       "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'},\n",
       "    'input_token_details': {'$ref': '#/$defs/InputTokenDetails'},\n",
       "    'output_token_details': {'$ref': '#/$defs/OutputTokenDetails'}},\n",
       "   'required': ['input_tokens', 'output_tokens', 'total_tokens'],\n",
       "   'title': 'UsageMetadata',\n",
       "   'type': 'object'}},\n",
       " 'anyOf': [{'type': 'string'},\n",
       "  {'$ref': '#/$defs/StringPromptValue'},\n",
       "  {'$ref': '#/$defs/ChatPromptValueConcrete'},\n",
       "  {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'},\n",
       "     {'$ref': '#/$defs/HumanMessage'},\n",
       "     {'$ref': '#/$defs/ChatMessage'},\n",
       "     {'$ref': '#/$defs/SystemMessage'},\n",
       "     {'$ref': '#/$defs/FunctionMessage'},\n",
       "     {'$ref': '#/$defs/ToolMessage'},\n",
       "     {'$ref': '#/$defs/AIMessageChunk'},\n",
       "     {'$ref': '#/$defs/HumanMessageChunk'},\n",
       "     {'$ref': '#/$defs/ChatMessageChunk'},\n",
       "     {'$ref': '#/$defs/SystemMessageChunk'},\n",
       "     {'$ref': '#/$defs/FunctionMessageChunk'},\n",
       "     {'$ref': '#/$defs/ToolMessageChunk'}]},\n",
       "   'type': 'array'}],\n",
       " 'title': 'ChatOpenAIInput'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'AIMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ai',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessage',\n",
       "   'type': 'object'},\n",
       "  'AIMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Message chunk from an AI.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'AIMessageChunk',\n",
       "     'default': 'AIMessageChunk',\n",
       "     'enum': ['AIMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None},\n",
       "    'tool_call_chunks': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCallChunk'},\n",
       "     'title': 'Tool Call Chunks',\n",
       "     'type': 'array'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ChatMessage': {'additionalProperties': True,\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'chat',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessage',\n",
       "   'type': 'object'},\n",
       "  'ChatMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Chat Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ChatMessageChunk',\n",
       "     'default': 'ChatMessageChunk',\n",
       "     'enum': ['ChatMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'function',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessage',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Function Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'FunctionMessageChunk',\n",
       "     'default': 'FunctionMessageChunk',\n",
       "     'enum': ['FunctionMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'HumanMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'human',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessage',\n",
       "   'type': 'object'},\n",
       "  'HumanMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Human Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'HumanMessageChunk',\n",
       "     'default': 'HumanMessageChunk',\n",
       "     'enum': ['HumanMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'InputTokenDetails': {'description': 'Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"cache_creation\": 200,\\n            \"cache_read\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'cache_creation': {'title': 'Cache Creation', 'type': 'integer'},\n",
       "    'cache_read': {'title': 'Cache Read', 'type': 'integer'}},\n",
       "   'title': 'InputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'InvalidToolCall': {'description': 'Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Error'},\n",
       "    'type': {'const': 'invalid_tool_call',\n",
       "     'enum': ['invalid_tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error'],\n",
       "   'title': 'InvalidToolCall',\n",
       "   'type': 'object'},\n",
       "  'OutputTokenDetails': {'description': 'Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"reasoning\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'reasoning': {'title': 'Reasoning', 'type': 'integer'}},\n",
       "   'title': 'OutputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'SystemMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'system',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessage',\n",
       "   'type': 'object'},\n",
       "  'SystemMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'System Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'SystemMessageChunk',\n",
       "     'default': 'SystemMessageChunk',\n",
       "     'enum': ['SystemMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolCall': {'description': 'Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"name\": \"foo\",\\n            \"args\": {\"a\": 1},\\n            \"id\": \"123\"\\n        }\\n\\n    This represents a request to call the tool named \"foo\" with arguments {\"a\": 1}\\n    and an identifier of \"123\".',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'object'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'type': {'const': 'tool_call',\n",
       "     'enum': ['tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id'],\n",
       "   'title': 'ToolCall',\n",
       "   'type': 'object'},\n",
       "  'ToolCallChunk': {'description': 'A chunk of a tool call (e.g., as part of a stream).\\n\\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    left_chunks = [ToolCallChunk(name=\"foo\", args=\\'{\"a\":\\', index=0)]\\n    right_chunks = [ToolCallChunk(name=None, args=\\'1}\\', index=0)]\\n\\n    (\\n        AIMessageChunk(content=\"\", tool_call_chunks=left_chunks)\\n        + AIMessageChunk(content=\"\", tool_call_chunks=right_chunks)\\n    ).tool_call_chunks == [ToolCallChunk(name=\\'foo\\', args=\\'{\"a\":1}\\', index=0)]',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "     'title': 'Index'},\n",
       "    'type': {'const': 'tool_call_chunk',\n",
       "     'enum': ['tool_call_chunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'index'],\n",
       "   'title': 'ToolCallChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A ToolMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content=\\'42\\', tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\')\\n\\n\\nExample: A ToolMessage where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    .. versionadded:: 0.2.17\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        tool_output = {\\n            \"stdout\": \"From the graph we can see that the correlation between x and y is ...\",\\n            \"stderr\": None,\\n            \"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"},\\n        }\\n\\n        ToolMessage(\\n            content=tool_output[\"stdout\"],\\n            artifact=tool_output,\\n            tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\',\\n        )\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'tool',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessage',\n",
       "   'type': 'object'},\n",
       "  'ToolMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Tool Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ToolMessageChunk',\n",
       "     'default': 'ToolMessageChunk',\n",
       "     'enum': ['ToolMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'UsageMetadata': {'description': 'Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"input_tokens\": 350,\\n            \"output_tokens\": 240,\\n            \"total_tokens\": 590,\\n            \"input_token_details\": {\\n                \"audio\": 10,\\n                \"cache_creation\": 200,\\n                \"cache_read\": 100,\\n            },\\n            \"output_token_details\": {\\n                \"audio\": 10,\\n                \"reasoning\": 200,\\n            }\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.',\n",
       "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
       "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
       "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'},\n",
       "    'input_token_details': {'$ref': '#/$defs/InputTokenDetails'},\n",
       "    'output_token_details': {'$ref': '#/$defs/OutputTokenDetails'}},\n",
       "   'required': ['input_tokens', 'output_tokens', 'total_tokens'],\n",
       "   'title': 'UsageMetadata',\n",
       "   'type': 'object'}},\n",
       " 'oneOf': [{'$ref': '#/$defs/AIMessage'},\n",
       "  {'$ref': '#/$defs/HumanMessage'},\n",
       "  {'$ref': '#/$defs/ChatMessage'},\n",
       "  {'$ref': '#/$defs/SystemMessage'},\n",
       "  {'$ref': '#/$defs/FunctionMessage'},\n",
       "  {'$ref': '#/$defs/ToolMessage'},\n",
       "  {'$ref': '#/$defs/AIMessageChunk'},\n",
       "  {'$ref': '#/$defs/HumanMessageChunk'},\n",
       "  {'$ref': '#/$defs/ChatMessageChunk'},\n",
       "  {'$ref': '#/$defs/SystemMessageChunk'},\n",
       "  {'$ref': '#/$defs/FunctionMessageChunk'},\n",
       "  {'$ref': '#/$defs/ToolMessageChunk'}],\n",
       " 'title': 'ChatOpenAIOutput'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'questa responsabilitÃ '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm_chat = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template((\"Translate this {word} into {language}\"))\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm_chat | output_parser\n",
    "\n",
    "chain.invoke({\"word\": \"responsibility\", \"language\": \"Italian\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok nice! So we put everything together using this [pipe](https://en.wikipedia.org/wiki/Pipeline_(Unix)) `|` symbol (or [unix pipe operator](https://en.wikipedia.org/wiki/Pipeline_(Unix)) if you want to get fancy) That's the power of the LCEL language, putting different components together through a simple interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[source](https://python.langchain.com/docs/expression_language/get_started#:~:text=its%20cone-fidence!%22-,4.%20entire%20pipeline)\n",
    "\n",
    "So what is happening is:\n",
    "\n",
    "- We pass in user input on the desired concept as {\"concept\": \"machine learning\"}\n",
    "- The prompt component takes the user input, which is then used to construct a `PromptValue` after using the `concept` to construct the `prompt`.\n",
    "- The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a `ChatMessage` object.\n",
    "- Finally, the `output_parser` component takes in a `ChatMessage`, and transforms this into a Python string, which is returned from the invoke method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR;\n",
    "    A[Input concept=machine learning] --dict--> B[Prompt Template]\n",
    "    B --PromptValue--> C[ChatModel]\n",
    "    C --ChatMessage--> D[StrOutputParser]\n",
    "    D --string--> E[Output]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a bullet point summary of the key features and benefits of LangChain Expression Language (LCEL):\n",
    "\n",
    "Declarative Composing: LCEL allows for easy composition of chains, ranging from simple \"prompt + LLM\" chains to complex ones with hundreds of steps.\n",
    "\n",
    "Streaming Support: LCEL offers optimal time-to-first-token, enabling streaming of tokens from an LLM to a streaming output parser for quick, incremental output.\n",
    "\n",
    "Async Support: Chains built with LCEL can be used both synchronously (e.g., in Jupyter notebooks for prototyping) and asynchronously (e.g., in a LangServe server), maintaining consistent code for prototypes and production.\n",
    "\n",
    "Optimized Parallel Execution: LCEL automatically executes parallel steps in a chain (like fetching documents from multiple retrievers) in both sync and async interfaces, reducing latency.\n",
    "\n",
    "Retries and Fallbacks: Users can configure retries and fallbacks for any part of the LCEL chain, enhancing reliability at scale. Streaming support for these features is in development.\n",
    "\n",
    "Access to Intermediate Results: LCEL allows access to intermediate step results, useful for user updates or debugging. This feature includes streaming intermediate results and is available on all LangServe servers.\n",
    "\n",
    "Input and Output Schemas: LCEL chains come with Pydantic and JSONSchema schemas, inferred from the chain's structure, which aid in validating inputs and outputs. This is a core part of LangServe.\n",
    "\n",
    "Seamless LangSmith Tracing Integration: As chains become more complex, LCEL provides automatic logging of all steps to LangSmith for enhanced observability and debuggability.\n",
    "\n",
    "Seamless LangServe Deployment Integration: LCEL chains can be easily deployed using LangServe, facilitating smoother deployment processes.\n",
    "\n",
    "These features highlight LCEL's versatility and efficiency in both development and production environments, making it a powerful tool for creating and managing complex language chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look into the `Runnable` components in more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableLambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "# Create a RunnableLambda and invoke it\n",
    "runnable = RunnableLambda(lambda x: x*23)\n",
    "output = runnable.invoke(5)\n",
    "print(output)  # Output: 115"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`RunnableSequence`](https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable:~:text=runnablesequence%20invokes%20a%20series%20of%20runnables%20sequentially%2C%20with%20one%20runnable%E2%80%99s%20output%20serving%20as%20the%20next%E2%80%99s%20input.%20construct%20using%20the%20%7C%20operator%20or%20by%20passing%20a%20list%20of%20runnables%20to%20runnablesequence.)\n",
    "\n",
    "RunnableSequence invokes a series of runnables sequentially, with one runnableâ€™s output serving as the nextâ€™s input. Construct using the | operator or by passing a list of runnables to RunnableSequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "# Suppose we have a list of numbers\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Create RunnableLambdas\n",
    "preprocess_runnable = RunnableLambda(lambda x: [i+1 for i in x])\n",
    "apply_model_runnable = RunnableLambda(lambda x: x*23)\n",
    "postprocess_runnable = RunnableLambda(lambda x: sum(x))\n",
    "\n",
    "# Create a RunnableSequence and invoke it\n",
    "sequence = preprocess_runnable | apply_model_runnable | postprocess_runnable\n",
    "# runnable_sequence = RunnableSequence(sequence)\n",
    "output = sequence.invoke(data)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1,2,3,4,5]\n",
    "\n",
    "# # Runnable 1\n",
    "# [2,3,4,5,6]\n",
    "\n",
    "# # Runnable 2 \n",
    "# [2,3,4,5,6] * 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(output)\n",
    "# or\n",
    "runnable_sequence = RunnableSequence(first=preprocess_runnable, middle=[apply_model_runnable], last=postprocess_runnable)\n",
    "runnable_sequence.invoke(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Runnableparallel`]([`Runnableparallel`](https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable:~:text=runnableparallel%20invokes%20runnables%20concurrently%2C%20providing%20the%20same%20input%20to%20each.%20construct%20it%20using%20a%20dict%20literal%20within%20a%20sequence%20or%20by%20passing%20a%20dict%20to%20runnableparallel.)https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable:~:text=runnableparallel%20invokes%20runnables%20concurrently%2C%20providing%20the%20same%20input%20to%20each.%20construct%20it%20using%20a%20dict%20literal%20within%20a%20sequence%20or%20by%20passing%20a%20dict%20to%20runnableparallel.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableParallel invokes runnables concurrently, providing the same input to each. Construct it using a dict literal within a sequence or by passing a dict to RunnableParallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.3\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6, 8]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# A RunnableSequence constructed using the `|` operator\n",
    "sequence = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)\n",
    "sequence.invoke(1) # 4\n",
    "sequence.batch([1, 2, 3]) # [4, 6, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mul_2': 4, 'mul_5': 10}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A sequence that contains a RunnableParallel constructed using a dict literal\n",
    "sequence = RunnableLambda(lambda x: x + 1) | {\n",
    "    'mul_2': RunnableLambda(lambda x: x * 2),\n",
    "    'mul_5': RunnableLambda(lambda x: x * 5)\n",
    "}\n",
    "sequence.invoke(1) # {'mul_2': 4, 'mul_5': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multiply': 2, 'add': 3}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "sequence_with_runnable_parallel = RunnableParallel(\n",
    "    multiply=RunnableLambda(lambda x: x * 2),\n",
    "    add=RunnableLambda(lambda x: x + 2))\n",
    "\n",
    "sequence_with_runnable_parallel.invoke(1) # [2, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you build chains, what you're doing is building a type of Runnable!\n",
    "\n",
    "That could be a RunnableSequence or a RunnableParallel  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': AIMessage(content='The Single Responsibility Principle in programming states that a class should have only one reason to change, meaning it should only have one responsibility or job to do. This helps to keep code organized, maintainable, and easier to understand. It promotes modular and flexible code design by separating concerns and preventing classes from becoming too complex or tightly coupled.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 16, 'total_tokens': 82, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f6deea2f-839d-4f70-9af8-ac5ffa4b8c34-0', usage_metadata={'input_tokens': 16, 'output_tokens': 66, 'total_tokens': 82, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " 'explanation': AIMessage(content='The Single Responsibility Principle (SRP) is a design principle in object-oriented programming that states that a class should have only one reason to change, meaning that it should have only one responsibility or job within the system. \\n\\nThis principle helps to improve the maintainability and readability of code by ensuring that each class is focused on a single task and does not have multiple unrelated responsibilities. This makes it easier to understand, test, and modify the code, as changes to one responsibility will not affect other parts of the system.\\n\\nBy adhering to the SRP, classes can be more easily reused in different contexts, and code becomes more modular and flexible. It also helps to reduce the complexity of the codebase, as each class has a clear and defined purpose.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 151, 'prompt_tokens': 15, 'total_tokens': 166, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ee99366c-1a6a-4029-8df9-58eb2f1fa77c-0', usage_metadata={'input_tokens': 15, 'output_tokens': 151, 'total_tokens': 166, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = PromptTemplate.from_template(\"Summarize this {text}\")\n",
    "prompt2 = PromptTemplate.from_template(\"Explain this {text}\")\n",
    "\n",
    "\n",
    "chain1 = prompt | llm\n",
    "\n",
    "chain2 = prompt2 | llm\n",
    "\n",
    "runnable_parallel = RunnableParallel(summary=chain1, explanation=chain2)\n",
    "\n",
    "runnable_parallel.invoke({\"text\": \"Single responsibility principle in programming\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableParallel"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(runnable_parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RunnableParallel can be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence.](https://python.langchain.com/docs/expression_language/how_to/map#:~:text=RunnableParallel%20can%20be,the%0A%E2%80%9Cquestion%E2%80%9D%20key.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableParallel (aka. RunnableMap) makes it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.\n",
    "\n",
    "([see it in docs here](https://python.langchain.com/docs/expression_language/how_to/map#:~:text=runnableparallel%20(aka.%20runnablemap)%20makes%20it%20easy%20to%20execute%20multiple%20runnables%20in%20parallel%2C%20and%20to%20return%20the%20output%20of%20these%20runnables%20as%20a%20map.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': AIMessage(content=\"Why did the bear break up with his girlfriend?\\nBecause he couldn't bear the relationship any longer!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a217e92d-a12c-4d64-bbfd-276da18e4d4a-0', usage_metadata={'input_tokens': 13, 'output_tokens': 20, 'total_tokens': 33, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " 'poem': AIMessage(content='In the dark forest, a bear roams free,\\nWith strength and grace, a wild beauty to see.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 15, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-aeee6dc2-a134-40cb-b75b-4d3575641580-0', usage_metadata={'input_tokens': 15, 'output_tokens': 22, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "model = ChatOpenAI()\n",
    "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
    ")\n",
    "\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "map_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableParallel are also useful for running independent processes in parallel, since each Runnable in the map is executed in parallel. For example, we can see our earlier joke_chain, poem_chain and map_chain all have about the same runtime, even though map_chain executes both of the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572 ms Â± 78 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "joke_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477 ms Â± 26.8 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "poem_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11 s Â± 88.5 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "joke_chain.invoke({\"topic\": \"bear\"})\n",
    "\n",
    "poem_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528 ms Â± 35.3 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "map_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RunnablePassthrough](https://python.langchain.com/docs/expression_language/how_to/passthrough#:~:text=RunnablePassthrough%20allows%20to,pass%20it%20through.)\n",
    "\n",
    "Allows you to pass inputs unchanged or with addition of new keys.\n",
    "\n",
    "Usually you would connect this with `RunnableParallel` to assign data to new key in the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just call it, it will take the input and pass it along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Create a RunnablePassthrough\n",
    "passthrough = RunnablePassthrough()\n",
    "passthrough.invoke(5) # 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you call it with the `.assign(...)` mthod, then it will take the input, and add extra arguments passed to the assign function. \n",
    "\n",
    "The key being added has to be a lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 1, 'extra_key_add_5': 6}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# Passthrough with assignment, adds an extra key-value pair\n",
    "runnable = RunnablePassthrough.assign(extra_key_add_5=lambda x:  x[\"num\"]+5)\n",
    "input_data = {\"num\": 1}\n",
    "result = runnable.invoke(input_data)\n",
    "# Output: {'num': 1, 'extra_value': 42}\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In combination with RunnableParallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'origin': 1, 'modified': 2}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable = RunnableParallel(\n",
    "    origin=RunnablePassthrough(),\n",
    "    modified=lambda x: x+1\n",
    ")\n",
    "\n",
    "runnable.invoke(1) # {'origin': 1, 'modified': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': 'completion', 'parsed': 'noitelpmoc (applied the parsing logic)'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modified example from langchain docs: https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html?highlight=runnablepassthrough#langchain_core.runnables.passthrough.RunnablePassthrough \n",
    "# or for an \"LLM\" example:\n",
    "\n",
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return \"completion\"\n",
    "\n",
    "chain = RunnableLambda(fake_llm) | {\n",
    "    'original': RunnablePassthrough(), # Original LLM output\n",
    "    'parsed': lambda text: text[::-1] + \" (applied the parsing logic)\" # Parsing logic\n",
    "}\n",
    "\n",
    "chain.invoke('hello') # {'original': 'completion', 'parsed': 'noitelpmoc'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so we have these 4 types of main objects:\n",
    "\n",
    "- `RunnableLambda`\n",
    "- `RunnableSequence`\n",
    "- `RunnablePassthrough`\n",
    "- `RunnableParallel`\n",
    "\n",
    "combined they make up kind of the core building block system of langchain allow you to create complex inner logics powered by llms. \n",
    "\n",
    "Let's create some fun examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original-text-input': AIMessage(content='Why did the gorilla bring a ladder to the party? \\nBecause he heard the drinks were on the house!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 13, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8e2ee0ba-130f-40ab-bbca-32ff4ba3cf74-0'),\n",
       " 'make-it-corporate': AIMessage(content='Subject: Hilarious Joke to Lighten Up Your Day! ðŸ¤£ðŸŽ‰\\n\\nDear Team,\\n\\nI hope this email finds you well. I wanted to share a little humor to brighten up your day.\\n\\n\"Why did the gorilla bring a ladder to the party? Because he heard the drinks were on the house!\"\\n\\nI hope this joke brings a smile to your face and adds a touch of laughter to your day. Let\\'s keep the positive vibes going as we work together towards our goals.\\n\\nWishing you all a fantastic day ahead!\\n\\nBest regards,\\n\\n[Your Name]', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 119, 'prompt_tokens': 195, 'total_tokens': 314, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6a5f4295-755d-4813-977b-b3b9c22bce45-0')}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "def fun_llm(prompt: str):\n",
    "    return ChatOpenAI().invoke(f\"Make this funny: {prompt}\")\n",
    "\n",
    "\n",
    "def corporate_llm(prompt: str):\n",
    "    return ChatOpenAI().invoke(f\"Make this topic a subject of a very corporate email: {prompt}\")\n",
    "\n",
    "fun_chain = RunnableLambda(fun_llm) | {\n",
    "    \"original-text-input\": RunnablePassthrough(), \n",
    "    \"make-it-corporate\": lambda x: corporate_llm(x)\n",
    "}\n",
    "\n",
    "\n",
    "fun_chain.invoke(\"gorillas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm1': 'completion', 'llm2': 'completion', 'total_chars': 20}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return \"completion\"\n",
    "\n",
    "runnable = {\n",
    "    'llm1':  fake_llm,\n",
    "    'llm2':  fake_llm,\n",
    "} | RunnablePassthrough.assign(\n",
    "    total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2'])\n",
    "  )\n",
    "\n",
    "runnable.invoke('hello')\n",
    "# {'llm1': 'completion', 'llm2': 'completion', 'total_chars': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking down whats happening:\n",
    "\n",
    "We create a `runnable` object, which is an instance of the `RunableSequence` object. This chain takes in as input a simple string: `hello`, and what it does is it \n",
    "simulates passing that same string to 2 `different` llms (which in this case are the same) and applies the logic of each of these llms as well as a third logic described by the new extra key added by the RunnablePassthrough.assign() method, that takes as input the output of the `llm1` and `llm2` as input to combine it through the lambda function associated with the new extra key created: `total_chars`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this example a bit more interesting by applying some research workflow vibe to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a couple of llms to do different things with some piece of content.\n",
    "# In this case let's use different llms to create summarization levels for some text.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def llm_summarization_level1(prompt: str):\n",
    "    return ChatOpenAI().invoke(f\"Summarize this text: {prompt}\")\n",
    "\n",
    "def llm_summarization_level2(prompt: str):\n",
    "    return ChatOpenAI().invoke(f\"Summarize this text in bullet points: {prompt}\")\n",
    "\n",
    "def llm_summarization_level3(prompt: str):\n",
    "    return ChatOpenAI().invoke(f\"Summarize this text into one short paragraph: {prompt}\")\n",
    "\n",
    "\n",
    "# now let's create a chain that will run all of these llms in parallel and then return the results\n",
    "\n",
    "runnable = {\n",
    "    'level1':  llm_summarization_level1,\n",
    "    'level2':  llm_summarization_level2,\n",
    "    'level3':  llm_summarization_level3,\n",
    "} | RunnablePassthrough.assign(prompt_level4=lambda x: f\"Combine these 3 summarizations\\\n",
    "    into one combining all their \\n \\\n",
    "    useful features. \\\n",
    "    Level1: {x['level1']}.\\n \\\n",
    "    Level2: {x['level2']}.\\n\\\n",
    "    Level3: {x['level3']}\"\n",
    "                       ) | RunnablePassthrough.assign(output=RunnableLambda(lambda x: ChatOpenAI().invoke(x['prompt_level4']))) \n",
    "# For the assign method with RunningPasshthrough, we'll combine the summarizations into a fourth one.\n",
    "\n",
    "output = runnable.invoke(\n",
    "    \"\"\"\n",
    "    The code you provided is from the nest_asyncio library, which is designed to patch Python's asyncio library to allow nested usage of the asyncio event loop. To understand this code, it's crucial first to grasp what an event loop is in the context of asynchronous programming.\n",
    "What is an Event Loop?\n",
    "An event loop in programming, particularly in asynchronous programming, is a central control structure that manages and dispatches events or messages in a program. In the context of Python's asyncio library, the event loop is a core feature that runs asynchronous tasks and callbacks, handles network IO operations, and manages subprocesses. It is essentially the heart of the asyncio module, enabling asynchronous programming by juggling and scheduling the execution of various tasks.\n",
    "Key Concepts in the nest_asyncio Code\n",
    "Patching asyncio: The code modifies (patches) certain parts of the asyncio library. This is done to change the default behavior of asyncio, particularly to support nested event loops, which are not allowed in standard asyncio.\n",
    "Reentrancy in Event Loops: The primary function of nest_asyncio is to make the asyncio event loop reentrant. In computing, reentrancy refers to the ability of a function to be paused in the middle of execution and safely called again (\"re-entered\") before its previous executions are complete. This is not normally supported by the asyncio event loops, as they are designed to prevent re-entry (or nesting) to avoid complex problems and unexpected behavior.\n",
    "Modifying Loop Behavior: The code alters the behavior of the event loop methods like run_forever, run_until_complete, and the internal _run_once. These modifications allow the event loop to pause and resume (re-enter) gracefully, facilitating nesting.\n",
    "Context Managers: The code uses context managers (manage_run and manage_asyncgens) to properly manage the state of the event loop during entry and exit of asynchronous contexts, which is crucial for handling nested loops correctly.\n",
    "Task Patching: It also patches the Task class of asyncio to modify its step function. This is necessary to ensure that the tasks (units of work scheduled by the event loop) behave correctly in a nested loop scenario.\n",
    "Tornado Patching: If the Tornado library (an asynchronous networking library) is used, nest_asyncio makes Tornado aware of the Python asyncio Future, ensuring compatibility.\n",
    "Understanding the Event Loop in Async Programming\n",
    "In asynchronous programming, especially in Python's asyncio, the event loop is pivotal. It allows the execution of multiple tasks seemingly in parallel by switching between them. This switching is non-blocking, meaning the program can handle other tasks while waiting for some IO operation to complete, thereby increasing efficiency and responsiveness.\n",
    "\n",
    "The nest_asyncio library's primary role is to tweak the asyncio's event loop to support nested operation, which is particularly useful in scenarios like running an asyncio event loop inside another asyncio event loop, something that standard asyncio does not support by default.\n",
    "\n",
    "For more details on asynchronous programming and event loops in Python, the Python asyncio documentation is a comprehensive resource.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The text discusses the nest_asyncio library, which patches Python's asyncio library to allow nested usage of the asyncio event loop. It explains the concept of an event loop in asynchronous programming and highlights key concepts in the nest_asyncio code, such as patching asyncio, enabling reentrancy in event loops, modifying loop behavior, using context managers, and task patching. The library's primary role is to support nested operation of the asyncio event loop, which is not supported by standard asyncio. Async programming relies heavily on event loops to manage and execute tasks efficiently."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(output['level1'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- The code provided is from the nest_asyncio library, which patches Python's asyncio library to allow nested usage of the event loop\n",
       "- An event loop in programming manages and dispatches events or messages in a program\n",
       "- Key concepts in the nest_asyncio code include patching asyncio, making the event loop reentrant, modifying loop behavior, using context managers, task patching, and Tornado patching\n",
       "- The event loop in asynchronous programming allows for executing multiple tasks seemingly in parallel by switching between them\n",
       "- The nest_asyncio library tweaks asyncio's event loop to support nested operation, which is useful for running an asyncio event loop inside another asyncio event loop."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(output['level2'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The nest_asyncio library modifies Python's asyncio library to enable nested usage of the asyncio event loop. It alters the behavior of the event loop to support reentrancy, modifies loop methods, uses context managers, and patches the Task class to handle nested loops correctly. This allows for efficient execution of multiple tasks in parallel in asynchronous programming. The library also ensures compatibility with the Tornado library and is crucial for understanding event loops in async programming."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(output['level3'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple research assistant, heavily based on this implementation by Harrison Chase:\n",
    "- https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "import os\n",
    "from langchain.tools import tool\n",
    "from serpapi import GoogleSearch\n",
    "import json\n",
    "\n",
    "RESULTS_PER_QUESTION = 1\n",
    "\n",
    "serpapi_params = {\n",
    "    \"engine\": \"google\",\n",
    "    \"api_key\": os.environ[\"SERPAPI_KEY\"]\n",
    "}\n",
    "\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Finds general knowledge information using Google search. Can also be used\n",
    "    to augment more 'general' knowledge to a previous specialist query.\"\"\"\n",
    "    search = GoogleSearch({**serpapi_params, \"q\":query, \"n\": 3})\n",
    "    results = search.get_dict()[\"organic_results\"]\n",
    "    urls = [r[\"link\"] for r in results]\n",
    "    \n",
    "    return urls\n",
    "\n",
    "\n",
    "def scrape_text(url: str):\n",
    "    # Send a GET request to the webpage\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extract all text from the webpage\n",
    "            page_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Print the extracted text\n",
    "            return page_text\n",
    "        else:\n",
    "            return f\"Failed to retrieve the webpage: Status code {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Failed to retrieve the webpage: {e}\"\n",
    "\n",
    "\n",
    "def collapse_list_of_lists(list_of_lists):\n",
    "    content = []\n",
    "    for l in list_of_lists:\n",
    "        content.append(\"\\n\\n\".join(l))\n",
    "    return \"\\n\\n\".join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_TEMPLATE = \"\"\"{text} \n",
    "-----------\n",
    "Using the above text, answer in short the following question: \n",
    "> {question}\n",
    "-----------\n",
    "if the question cannot be answered using the text, imply summarize the text. Include all factual information, numbers, stats etc if available.\"\"\"  # noqa: E501\n",
    "SUMMARY_PROMPT = ChatPromptTemplate.from_template(SUMMARY_TEMPLATE)\n",
    "\n",
    "\n",
    "url = \"https://python.langchain.com/docs/concepts/\"\n",
    "\n",
    "scrape_and_summarize_chain = RunnablePassthrough.assign(\n",
    "    summary = RunnablePassthrough.assign(\n",
    "    text=lambda x: scrape_text(x[\"url\"])[:10000]\n",
    ") | SUMMARY_PROMPT | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n",
    ") | (lambda x: f\"URL: {x['url']}\\n\\nSUMMARY: {x['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, even though this chain requires two keys in the input dictionary, that's not reflected in the input_schema() of the chain, because in it only goes the variables contained in the ChatPromptTemplate(). \n",
    "\n",
    "Interestingly enough, the `text` variable which is in the ChatPromptTemplate, is not in the schema because its already part of the chain itself (therefore not being necessary in the input_schema() I guess)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'URL: https://python.langchain.com/docs/concepts/\\n\\nSUMMARY: Some important concepts in LangChain include:\\n\\n1. **Chat Models**: LLMs exposed via a chat API that process sequences of messages.\\n2. **Messages**: The unit of communication in chat models, representing model input and output.\\n3. **Chat History**: A sequence of messages that represent a conversation.\\n4. **Tools**: Functions with associated schemas defining their name, description, and accepted arguments.\\n5. **Tool Calling**: A chat model API that accepts tool schemas along with messages as input.\\n6. **Structured Output**: A technique for chat models to respond in a structured format, such as JSON.\\n7. **Memory**: Information about a conversation that is persisted for future use.\\n8. **Multimodality**: The ability to work with various forms of data, including text, audio, images, and video.\\n9. **Runnable Interface**: The base abstraction for many LangChain components.\\n10. **Streaming**: APIs for surfacing results as they are generated.\\n11. **LangChain Expression Language (LCEL)**: A syntax for orchestrating LangChain components.\\n12. **Document Loaders**: Components that load sources as lists of documents.\\n13. **Retrieval**: Systems that retrieve structured or unstructured data from a datasource in response to queries.\\n14. **Text Splitters**: Tools to split long text into smaller chunks for granular retrieval.\\n15. **Embedding Models**: Models that represent data in a vector space.\\n\\nThese concepts are foundational to understanding the LangChain framework and its applications.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# At this\n",
    "# let's test this chain on some url\n",
    "scrape_and_summarize_chain.invoke({\"question\": \"What are some important concepts in langchain?\", \"url\": url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_chain = RunnablePassthrough.assign(\n",
    "    urls = lambda x: web_search(x[\"question\"]) # urls will be the output of the web_search function()\n",
    ") | (lambda x: [{\"question\": x[\"question\"], \"url\": u} for u in x[\"urls\"]]) | scrape_and_summarize_chain.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test this web search chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['URL: https://python.langchain.com/docs/concepts/tool_calling/\\n\\nSUMMARY: Tool calling in LangChain refers to the ability of models to interact with external systems, such as databases or APIs, by using structured input schemas. Key concepts include:\\n\\n1. **Tool Creation**: Tools are created using the `@tool` decorator, which associates a function with its schema.\\n2. **Tool Binding**: Tools must be connected to a model that supports tool calling, which allows the model to recognize the tool and its input requirements.\\n3. **Tool Calling**: The model can decide when to call a tool, ensuring that its response adheres to the tool\\'s input schema.\\n4. **Tool Execution**: The tool is executed using the arguments provided by the model.\\n\\nThe recommended workflow involves creating tools, binding them to a model with the `.bind_tools()` method, and invoking the model with user input. If a tool call is made, the model\\'s response will include the arguments for the tool.\\n\\nExample of tool creation:\\n```python\\nfrom langchain_core.tools import tool\\n\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply a and b.\"\"\"\\n    return a * b\\n```\\n\\nLangChain provides a standardized interface for connecting tools to models, and many model providers support tool calling. For more details, refer to the API reference and how-to guides.',\n",
       " 'URL: https://blog.langchain.dev/tool-calling-with-langchain/\\n\\nSUMMARY: Tool calling in LangChain allows Large Language Models (LLMs) to interact with external data sources through a standardized interface for tool invocations. This functionality has been enhanced with the introduction of the `tool_calls` attribute on `AIMessage`, which provides a consistent way to access tool invocations regardless of the LLM provider. \\n\\nKey components include:\\n\\n1. **ChatModel.bind_tools()**: A method to attach tool definitions to model calls, supporting various formats like Pydantic classes, LangChain tools, and dictionaries based on provider requirements.\\n\\n2. **AIMessage.tool_calls**: A new attribute that standardizes access to tool invocations returned by models, allowing developers to handle outputs uniformly across different models.\\n\\n3. **create_tool_calling_agent()**: A utility to create agents that can work with any tool-calling model, facilitating the integration of different models like OpenAI, Anthropic, and Gemini.\\n\\nRecent developments include support from multiple LLM providers introducing native tool calling capabilities, including OpenAI, Anthropic, Gemini, Mistral, Fireworks, Together, Groq, and Cohere, with varying release dates from November 2023 to April 2024. \\n\\nTo utilize these features, users need to upgrade their `langchain_core` and partner package versions. For practical examples, the text provides code snippets showing how to define tools and create agents using different LLMs, demonstrating the flexibility and power of the tool calling interface in LangChain.',\n",
       " 'URL: https://python.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/\\n\\nSUMMARY: Tool calling in LangChain allows a model to detect when specific tools should be invoked and to respond with structured inputs for those tools. This feature enables more reliable and useful tool calls compared to generic text completion or chat APIs. LangChain supports various models from providers like OpenAI, Anthropic, and Google. The process involves initializing tools, creating a tool-calling agent, and executing it to retrieve results. It facilitates tasks such as web searches and integrates chat history for more conversational responses.',\n",
       " 'URL: https://python.langchain.com/docs/how_to/tool_calling/\\n\\nSUMMARY: Tool calling in LangChain allows chat models to generate structured output by \"calling a tool.\" The model itself does not perform the action but generates the arguments needed for a tool, which the user can then execute. This technique can also be applied for tasks like extracting information from unstructured text. \\n\\nTo use tool calling, you need to define tool schemas that describe the tool\\'s functionality and arguments. These schemas can be provided as Python functions (with type hints and docstrings), Pydantic models, TypedDict classes, or LangChain Tool objects. The chat models that support tool calling features implement a `.bind_tools()` method for passing these schemas. \\n\\nFor example, you can define tools using Python functions, where the function name, type hints, and docstring form part of the tool schema. LangChain also offers a `@tool` decorator for additional control over the tool schema.\\n\\nSupported models for tool calling are available, and it is important to create descriptive schemas for optimal model performance.',\n",
       " 'URL: https://deepinfra.com/blog/langchain-tool-search\\n\\nSUMMARY: The text introduces Tool Calling with LangChain, particularly focusing on a use case involving the Tavily tool to search the web for information. It details the setup process, which requires Python 3.8 or higher and API keys for DeepInfra and Tavily. It provides a step-by-step guide for creating a virtual environment, installing necessary packages, and configuring the environment with API keys. \\n\\nThe main script demonstrates how to create a LangChain agent using the ChatDeepInfra model and the Tavily tool to answer queries. The example question posed is, \"Why is the hype for Shadow of the Erdtree so high?\" The script shows how to invoke the agent and print the result. \\n\\nKey components include:\\n- Installation of `python-dotenv`, `langchain`, and `langchain-community`.\\n- Creation of a `.env` file for API key storage.\\n- Use of the `TavilySearchResults` tool to fetch web search results.\\n- The model used is `meta-llama/Meta-Llama-3-70B-Instruct`.\\n\\nThe conclusion emphasizes the potential for further enhancements to the agent with additional tools and models.',\n",
       " 'URL: https://medium.com/@garysvenson09/how-to-use-function-calling-with-tool-choice-in-langchain-787ba3c56c5e\\n\\nSUMMARY: The text indicates an error message stating \"Failed to retrieve the webpage: Status code 403.\" This suggests that access to the requested webpage was denied, typically due to insufficient permissions or restrictions on the server side. There are no specific details or statistics available in the text regarding tool calling in LangChain.',\n",
       " 'URL: https://www.linkedin.com/pulse/tool-calling-langchain-do-more-your-ai-agents-saurav-prateek-so20c\\n\\nSUMMARY: Tool calling in LangChain refers to the capability of large language models (LLMs) to understand human messages and determine which tools to invoke based on context. Tools can include APIs, functions, or databases, enhancing the AI models\\' functionality. In the article, two specific tools are defined:\\n\\n1. **Tool 1**: Adds two integers. It takes two integers as parameters and returns their sum.\\n2. **Tool 2**: Calculates the length of a given string. It takes a string as input and returns its length.\\n\\nThe integration of these tools with an LLM model allows the model to perform operations based on user queries. For instance, when prompted to \"Add number 5 and 4\" and \"Find the length of the string â€˜Crocodileâ€™,\" the model successfully invokes the appropriate tools and returns the correct results.\\n\\nThe article emphasizes the importance of logging tool calls, which includes details like function names and parameters used, confirming that the model can call the tools correctly.\\n\\nThe author, Saurav Prateek, is an engineer at Google and has a significant following on LinkedIn and a newsletter with over 30,000 subscribers.']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_search_chain.invoke({\"question\": \"Look up tool calling in langchain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Write 3 google search queries to search online that form an \"\n",
    "            \"objective opinion from the following: {question}\\n\"\n",
    "            \"You must respond with a list of strings in the following format: \"\n",
    "            '[\"query 1\", \"query 2\", \"query 3\"].',\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "search_question_chain = SEARCH_PROMPT | ChatOpenAI(temperature=0) | StrOutputParser() | json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best langchain utilities for research',\n",
       " 'top langchain tools for academic research',\n",
       " 'langchain software for academic studies']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_question_chain.invoke({\"question\": \"What are the most useful langchain utilities for research?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_research_chain = search_question_chain | (lambda x: [{\"question\": q} for q in x]) | web_search_chain.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['URL: https://www.langchain.com/\\n\\nSUMMARY: LangChain is a composable framework designed for building applications that utilize large language models (LLMs). It offers a suite of products, including LangSmith for debugging, testing, and monitoring LLM applications, and LangGraph for orchestrating agent-driven workflows. LangChain supports developers throughout the LLM application lifecycle, enabling the creation of context-aware and reasoning applications that leverage company data and APIs. The platform has achieved over 15 million monthly downloads, powers more than 100,000 apps, has over 100,000 GitHub stars, and boasts a community of over 4,000 contributors. It is utilized by teams across various industries to enhance operational efficiency, discovery, personalization, and product quality.',\n",
       "  'URL: https://aws.amazon.com/what-is/langchain/\\n\\nSUMMARY: LangChain is an open-source framework designed for building applications based on large language models (LLMs). It provides tools and abstractions to enhance the customization, accuracy, and relevance of the information generated by LLMs. LangChain enables developers to integrate LLMs with internal data sources and simplify the process of prompt engineering, which is essential for creating domain-specific applications. Key features of LangChain include:\\n\\n- **Chains:** Sequences of automated actions that process user queries and generate responses.\\n- **Links:** Individual actions within chains, allowing complex tasks to be divided into smaller tasks.\\n- **LLM Interface:** APIs for connecting and querying various LLMs.\\n- **Prompt Templates:** Pre-built structures for consistent formatting of queries to AI models.\\n- **Agents:** Specialized chains that determine the best action sequence based on user input.\\n- **Retrieval Modules:** Tools for storing, searching, and refining information to improve LLM responses.\\n- **Memory:** Capabilities for recalling past interactions to refine responses.\\n- **Callbacks:** Mechanisms for logging and monitoring events during LangChain operations.\\n\\nLangChain is supported by an active community and is free for organizations to use. It can be integrated with AWS services like Amazon Bedrock, Amazon Kendra, and Amazon SageMaker to build generative AI applications that utilize enterprise data.',\n",
       "  'URL: https://python.langchain.com/docs/introduction/\\n\\nSUMMARY: LangChain is a framework designed for developing applications powered by large language models (LLMs). It simplifies the entire lifecycle of LLM application development, productionization, and deployment. The framework comprises several open-source libraries, including:\\n\\n- **langchain-core**: Provides base abstractions and the LangChain Expression Language.\\n- **Integration packages**: Lightweight packages for important integrations (e.g., langchain-openai).\\n- **langchain**: Contains chains, agents, and retrieval strategies that form the cognitive architecture of applications.\\n- **langchain-community**: Third-party integrations maintained by the community.\\n- **LangGraph**: A tool for building robust and stateful multi-actor applications with LLMs, modeling steps as edges and nodes in a graph.\\n- **LangGraph Platform**: Enables the deployment of LLM applications to production.\\n- **LangSmith**: A platform for debugging, testing, evaluating, and monitoring LLM applications.\\n\\nThe documentation primarily focuses on the Python LangChain library, with additional resources available for the JavaScript version. Tutorials and how-to guides are provided for hands-on learning and addressing specific queries related to LangChain functionalities.',\n",
       "  'URL: https://www.ibm.com/topics/langchain\\n\\nSUMMARY: LangChain is an open-source orchestration framework designed for developing applications that utilize large language models (LLMs). It provides Python and JavaScript libraries that simplify building LLM-driven applications, such as chatbots and virtual agents. Launched by Harrison Chase in October 2022, LangChain became the fastest-growing open-source project on GitHub by June 2023, coinciding with the popularity surge of generative AI following the launch of OpenAI\\'s ChatGPT. \\n\\nLangChain serves as a generic interface for various LLMs, facilitating integration with external data sources and workflows. Its modular, abstraction-based approach allows developers to easily compare prompts and models, enabling applications to utilize multiple LLMs for different tasks. Key features include:\\n\\n- **Prompt Templates**: Simplify the composition of prompts for LLMs without manual coding.\\n- **Chains**: Execute a sequence of functions combining LLMs with other components.\\n- **Indexes**: Facilitate access to external data sources, referred to as \"indexes.\"\\n- **Document Loaders**: Support importing data from various third-party applications.\\n- **Vector Databases**: Allow efficient searching through vector embeddings.\\n- **Text Splitters**: Split large documents into smaller, meaningful chunks for processing.\\n\\nOverall, LangChain empowers users to create versatile LLM applications while minimizing coding efforts and supporting experimentation with different models.',\n",
       "  \"URL: https://www.techtarget.com/searchenterpriseai/definition/LangChain\\n\\nSUMMARY: LangChain is an open-source framework designed for software developers to integrate large language models (LLMs) with external components, enabling the development of natural language processing (NLP) applications. Launched in 2022 by co-founders Harrison Chase and Ankush Gola, LangChain allows developers to connect LLMs like OpenAI's GPT-3.5 and GPT-4 to various data sources, enhancing the capabilities of AI applications. It supports programming languages such as Python, JavaScript, and TypeScript.\\n\\nLangChain consists of several modules, including model interaction, data connection and retrieval, chains, agents, and memory, which facilitate the building and organization of complex NLP applications. It can integrate with various external data sources and platforms, such as Google Cloud and vector databases like Pinecone, making it versatile for applications like chatbots, coding assistants, and healthcare solutions.\",\n",
       "  'URL: https://github.com/langchain-ai/langchain\\n\\nSUMMARY: LangChain is a framework designed for developing applications powered by large language models (LLMs). It simplifies the entire application lifecycle, offering open-source libraries, productionization tools, and deployment options. Key components include:\\n\\n- **Langchain-core**: Base abstractions for building applications.\\n- **Integration packages**: Lightweight packages for important integrations (e.g., `langchain-openai`).\\n- **LangGraph**: A tool for building stateful applications with LLMs by modeling steps as edges and nodes in a graph.\\n- **LangSmith**: A platform for debugging, testing, and monitoring applications built on any LLM framework.\\n\\nLangChain supports various use cases such as question answering, extracting structured outputs, and creating chatbots. It boasts 96.1k stars and 15.6k forks on GitHub, with extensive documentation and community support for contributions.',\n",
       "  'URL: https://medium.com/around-the-prompt/what-is-langchain-and-why-should-i-care-as-a-developer-b2d952c42b28\\n\\nSUMMARY: The provided text does not contain any information about Langchain. It only mentions a failed attempt to retrieve a webpage with a status code of 403, indicating that access is forbidden.',\n",
       "  'URL: https://www.datastax.com/guides/what-is-langchain\\n\\nSUMMARY: LangChain is a Python framework designed to simplify AI application development, focusing on real-time data processing and integration with large language models (LLMs). It aids in implementing the retrieval-augmented generation (RAG) pattern, enhancing data retrieval from various sources to create contextually aware AI applications. Key components of LangChain include LLMs, prompt templates, indexes, retrievers, output parsers, vector stores, and agents. These components work together to allow developers to build sophisticated AI systems capable of understanding and generating human-like language responses. LangChain also offers libraries for common interactions with models, data retrieval, and chat memory, making it easier for developers to handle complex actions without needing to code all the intricate details associated with APIs, data structures, and security.'],\n",
       " [\"URL: https://www.techtarget.com/searchenterpriseai/definition/LangChain\\n\\nSUMMARY: **LangChain Definition and Uses:**\\n\\nLangChain is an open-source framework designed for software developers to combine large language models (LLMs) with external components to create AI-powered applications. It was launched in 2022 by co-founders Harrison Chase and Ankush Gola. LangChain facilitates the development of natural language processing (NLP) applications by connecting LLMs, such as OpenAI's GPT-3.5 and GPT-4, to various data sources, enabling models to access recent information beyond their training data.\\n\\n**Key Uses:**\\n1. **Customer Service Chatbots:** LangChain is utilized to create advanced chat applications capable of handling complex queries and maintaining context throughout conversations.\\n2. **Coding Assistants:** Developers can build tools to enhance coding skills and productivity using LangChain alongside OpenAI's API.\\n3. **Healthcare Applications:** LangChain applications assist in diagnosing conditions and automating repetitive administrative tasks within the healthcare sector.\\n\\n**Core Features:**\\n- **Model Interaction:** Manages input and output for any language model.\\n- **Data Connection and Retrieval:** Transforms and retrieves data from databases.\\n- **Chains:** Links multiple LLMs or components for complex applications.\\n- **Agents:** Orchestrates actions to solve problems efficiently.\\n- **Memory:** Allows LLMs to remember user interactions contextually.\\n\\n**Integrations:** LangChain typically integrates with LLM providers (like OpenAI and Hugging Face) and external data sources (like Google Search and vector databases) to enhance app functionality.\",\n",
       "  'URL: https://python.langchain.com/docs/introduction/\\n\\nSUMMARY: **LangChain Definition and Uses:**\\n\\nLangChain is a framework designed for developing applications powered by large language models (LLMs). It streamlines the entire lifecycle of LLM applications, encompassing development, productionization, and deployment.\\n\\n**Uses:**\\n- **Development:** Provides open-source building blocks, components, and third-party integrations to build applications. It includes `langchain-core` for base abstractions and the LangChain Expression Language, integration packages like `langchain-openai`, and chains and agents for cognitive architectures.\\n- **Productionization:** Utilizes LangSmith for inspecting, monitoring, and evaluating chains to optimize and deploy applications confidently.\\n- **Deployment:** Converts applications into production-ready APIs and assistants through the LangGraph Platform.\\n\\nLangChain also integrates with LangGraph, which allows for building robust and stateful multi-actor applications. It offers community-maintained integrations via `langchain-community`.\\n\\nFor hands-on learners, LangChain provides tutorials such as building simple LLM applications, chatbots, and agents.',\n",
       "  'URL: https://www.ibm.com/topics/langchain\\n\\nSUMMARY: **LangChain Definition and Uses:**\\n\\nLangChain is an open-source orchestration framework designed for developing applications using large language models (LLMs). It provides libraries in both Python and JavaScript to simplify the process of building LLM-driven applications such as chatbots, virtual agents, intelligent search systems, question-answering services, and summarization tools.\\n\\nLaunched by Harrison Chase in October 2022, LangChain quickly became the fastest-growing open-source project on GitHub by June 2023. Its modular approach allows developers to dynamically compare prompts and different foundation models with minimal code changes. It facilitates integration with external data sources and software workflows, enabling applications that utilize multiple LLMs.\\n\\nLangChain features include:\\n- **Prompt Templates:** Simplifies prompt engineering with reusable templates.\\n- **Chains:** Executes sequences of functions to combine LLMs with other components.\\n- **Indexes:** Access to external data sources, including document loaders and vector databases for efficient data handling.\\n- **Text Splitters:** Breaks down large texts into smaller, meaningful chunks for better processing.\\n\\nOverall, LangChain enhances the accessibility and experimentation of generative AI applications.',\n",
       "  'URL: https://medium.com/around-the-prompt/what-is-langchain-and-why-should-i-care-as-a-developer-b2d952c42b28\\n\\nSUMMARY: The text provided does not contain any information regarding Langchain, its definition, or its uses. It only mentions a failure to retrieve a webpage with a status code of 403.',\n",
       "  'URL: https://www.langchain.com/\\n\\nSUMMARY: LangChain is a composable framework designed for building applications that leverage large language models (LLMs). It supports developers throughout the LLM application lifecycle, enabling the creation of context-aware and reasoning applications tailored to company data and APIs. LangChain can be used independently or in conjunction with other products like LangSmith and LangGraph for enhanced functionality.\\n\\nKey statistics and facts include:\\n- Over 15 million monthly downloads.\\n- More than 100,000 apps powered by LangChain.\\n- 100,000+ GitHub stars.\\n- 4,000+ contributors, making it the largest developer community in generative AI.\\n\\nLangChain products help teams increase operational efficiency, improve personalization, and deliver premium products that generate revenue. The platform also includes LangSmith for managing LLM performance and LangGraph for orchestrating agent-driven workflows.',\n",
       "  'URL: https://lakefs.io/blog/what-is-langchain-ml-architecture/\\n\\nSUMMARY: The provided text does not contain any information about Langchain, its definition, or its uses. It only reports a failed attempt to retrieve a webpage with a status code of 403, which indicates a forbidden access error.',\n",
       "  'URL: https://www.geeksforgeeks.org/introduction-to-langchain/\\n\\nSUMMARY: **LangChain Definition and Uses**\\n\\nLangChain is an open-source framework designed to simplify the creation of applications that utilize large language models (LLMs). It provides a standard interface for chains, numerous integrations with other tools, and end-to-end chains for common applications. LangChain enables developers to build applications that leverage LLMs like GPT-4 in conjunction with external data and computations.\\n\\n**Uses of LangChain:**\\n1. **Document Analysis and Summarization**: Analyzing and summarizing documents effectively.\\n2. **Chatbots**: Creating chatbots that can interact naturally with users for support and assistance.\\n3. **Code Analysis**: Analyzing code for potential bugs or security vulnerabilities.\\n4. **Answering Questions**: Providing answers by retrieving information from various sources, including text and code.\\n5. **Data Augmentation**: Generating new data similar to existing datasets, useful for training machine learning models.\\n6. **Text Classification**: Performing text classifications and sentiment analysis.\\n7. **Text Summarization**: Summarizing text into a specified number of words or sentences.\\n8. **Machine Translation**: Translating input text into different languages. \\n\\nOverall, LangChain is a versatile tool for building diverse LLM-powered applications with a large user community.',\n",
       "  'URL: https://www.datastax.com/guides/what-is-langchain\\n\\nSUMMARY: **LangChain Definition and Uses:**\\n\\nLangChain is a Python framework designed to facilitate the development of AI applications by streamlining real-time data processing and integration with large language models (LLMs). It aids in implementing the retrieval-augmented generation (RAG) pattern, allowing applications to retrieve up-to-date data from various sources to enhance the contextual relevance of AI interactions.\\n\\n**Uses of LangChain:**\\n- Simplifies data retrieval and processing for AI applications.\\n- Provides pre-built libraries for popular LLMs (e.g., OpenAI GPT), allowing developers to focus on application logic without needing to manage API specifics.\\n- Supports creating complex AI systems capable of understanding and generating human-like language responses.\\n- Enhances efficiency and accuracy in various industries, including customer service, content creation, and data analysis.\\n- Facilitates the construction of applications that require deep comprehension of language and context through components like prompt templates, indexes, retrievers, output parsers, and vector stores. \\n\\nOverall, LangChain makes AI development more accessible and effective, enabling developers to create contextually aware applications.'],\n",
       " ['URL: https://medium.com/llm-study-diary-a-beginners-path-through-ai/comprehensive-review-of-langchain-part-1-4734d61a49e1\\n\\nSUMMARY: The provided text does not contain any information regarding Langchain reviews and feedback. It only indicates that there was a failure to retrieve a webpage with a status code of 403, which typically means access is forbidden.',\n",
       "  'URL: https://www.reddit.com/r/LangChain/comments/18eukhc/i_just_had_the_displeasure_of_implementing/\\n\\nSUMMARY: The feedback on LangChain from a user with over a decade of engineering experience is largely negative. The user describes LangChain as \"arguably the worst library\" they have ever worked with, citing issues such as inconsistent abstractions, naming schemas, behavior, confusing error management, and unnecessary abstractions. They express that the framework attempts to cater to beginner developers at the expense of those with more coding experience. The user warns others to avoid using LangChain to preserve their sanity.',\n",
       "  'URL: https://news.ycombinator.com/item?id=40739982\\n\\nSUMMARY: LangChain has received mixed reviews and feedback from developers. Many users express frustration with its complexity and multiple layers of abstraction, which can hinder customization and understanding of the underlying processes. Specific points raised include:\\n\\n1. **Over-Engineering**: Users feel that LangChain adds unnecessary complexity for tasks that could be accomplished with simple string handling, API calls, and loops. It is noted that most applications using LLMs do not require such abstractions.\\n\\n2. **Learning Curve**: Some developers experienced difficulties with its abstractions, making it challenging to debug or improve applications built with LangChain. This sentiment was echoed by an intern who found it cumbersome to implement even standard features.\\n\\n3. **Comparison to Other Libraries**: Users compare LangChain to ORMs and web frameworks, suggesting it might be beneficial for managing multiple LLM providers but is often overkill for more straightforward use cases.\\n\\n4. **Community Consensus**: There is a sense of groupthink among developers pushing for LangChain, even when others recognized its limitations. Some users have successfully built their own alternatives or simpler implementations without it.\\n\\n5. **Practical Recommendations**: For those looking to build RAG (Retrieval-Augmented Generation) applications without using LangChain, the community provides various resources and strategies focusing on basic embedding techniques, prompt construction, and direct API interactions.\\n\\nOverall, while LangChain aims to simplify LLM integration, many developers prefer lighter, more straightforward solutions that allow for greater flexibility and understanding of their applications.',\n",
       "  'URL: https://www.g2.com/products/langchain/reviews\\n\\nSUMMARY: The text does not provide any information regarding Langchain reviews and feedback. It simply indicates a failure to retrieve a webpage with a status code of 403, which typically means access is forbidden.',\n",
       "  'URL: https://slashdot.org/software/p/LangChain/\\n\\nSUMMARY: The provided text does not contain any specific user reviews or feedback for LangChain, as it states \"No User Reviews. Be the first to provide a review.\" However, LangChain is described as an AI development platform that supports various modules, including memory implementations, prompt engineering, and integration with language models. It aims to create differentiated applications beyond simple API calls to language models.\\n\\nLangChain features include:\\n- AI Development Platform\\n- Development Frameworks\\n- Prompt Management Tool\\n- Prompt Engineering Tool\\n\\nIt also offers support and documentation online for users. The text discusses LangChain\\'s alternatives, such as BabyAGI, DSPy, LangGraph, and LlamaIndex, highlighting their unique functionalities.\\n\\nFor further exploration, you can compare LangChain against platforms like Literal AI, PromptLayer, Entry Point AI, and others designed for prompt management and LLM applications. Overall, while there are no reviews, LangChain is positioned as a powerful tool for developers in the AI space.',\n",
       "  'URL: https://news.ycombinator.com/item?id=36645575\\n\\nSUMMARY: The provided text indicates a failure to retrieve a webpage due to a \"Status code 503,\" which typically means that the server is temporarily unavailable. There are no reviews or feedback regarding Langchain included in the text.',\n",
       "  'URL: https://python.langchain.com/docs/contributing/reference/review_process/\\n\\nSUMMARY: LangChain\\'s review process for pull requests (PRs) is structured into several statuses: \\n\\n1. **Triage**: Initial status for newly submitted PRs requiring categorization by maintainers.\\n2. **Needs Support**: PRs needing community feedback; promoted to backlog with 5 upvotes. Stale after 25 days and closed after 30 days if no action is taken.\\n3. **In Review**: Actively reviewed PRs.\\n\\nPRs touching core code (/libs/core) receive top priority and are reviewed quickly, while those related to documentation are also prioritized but have specific guidelines. PRs must be in English and include concise descriptions and unit tests; otherwise, they may be closed without in-depth review. Community PRs typically start in the \"Needs Support\" status.\\n\\nFeedback on the review process is encouraged through GitHub Discussions.',\n",
       "  'URL: https://docs.smith.langchain.com/old/cookbook/feedback-examples\\n\\nSUMMARY: The text provided does not contain any information about Langchain reviews and feedback. It only states that there was a failure to retrieve a webpage with a status code 404, indicating that the page was not found.']]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_research_chain.invoke({\"question\": \"What is langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_SYSTEM_PROMPT = \"You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.\"  # noqa: E501\n",
    "\n",
    "\n",
    "# Report prompts from https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/master/prompts.py\n",
    "RESEARCH_REPORT_TEMPLATE = \"\"\"Information:\n",
    "--------\n",
    "{research_summary}\n",
    "--------\n",
    "Using the above information, answer the following question or topic: \"{question}\" in a detailed report -- \\\n",
    "The report should focus on the answer to the question, should be well structured, informative, \\\n",
    "in depth, with facts and numbers if available and a minimum of 1,200 words.\n",
    "You should strive to write the report as long as you can using all relevant and necessary information provided.\n",
    "You must write the report with markdown syntax.\n",
    "You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "Write all used source urls at the end of the report, and make sure to not add duplicated sources, but only one reference for each.\n",
    "You must write the report in apa format.\n",
    "Please do your best, this is very important to my career.\"\"\"  # noqa: E501\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", WRITER_SYSTEM_PROMPT),\n",
    "        (\"user\", RESEARCH_REPORT_TEMPLATE),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(\n",
    "    research_summary= full_research_chain | collapse_list_of_lists\n",
    ") | prompt | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableMap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's modify this example for something that uses the updated version of the pydantic library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5l/y8s3fc655417629rqwgxkhx80000gn/T/ipykernel_32739/2928158330.py:18: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retriever.get_relevant_documents(\"Where did harrison work?\")\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this example requires pydantic==\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "llm_chat = ChatOpenAI()\n",
    "\n",
    "vectorstore = Chroma.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retriever.get_relevant_documents(\"Where did harrison work?\")\n",
    "\n",
    "template = \"Answer the question based on this context: {context}\\n\\nQuestion: {question}\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"],\n",
    "    \n",
    "}) | prompt | llm_chat | output_parser\n",
    "\n",
    "\n",
    "\n",
    "chain.invoke({\"question\": \"What did harrison do?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect just the `RunnableMap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={}, page_content='harrison worked at kensho'),\n",
       "  Document(metadata={}, page_content='bears like to eat honey')],\n",
       " 'question': 'What did harrison do?'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "inputs_with_lambda = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"],\n",
    "})\n",
    "\n",
    "inputs_with_lambda.invoke({\"question\": \"What did harrison do?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableMap` is an alias for `RunnableParallel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={}, page_content='harrison worked at kensho'),\n",
       "  Document(metadata={}, page_content='bears like to eat honey')],\n",
       " 'question': 'What did harrison do?'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_runnable_passthrough = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"],\n",
    "})\n",
    "\n",
    "inputs_with_runnable_passthrough.invoke({\"question\": \"What did harrison do?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get the same thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use tools with runnables in some neat and interesting ways. Let's see that in the next notebook (2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that what the `RunnableMap` is doing, is create the right inputs for the downstream of this chain which in this case is seeting the `context` key with a value of a list of `Document` objects, and the question key with itself.\n",
    "(which now we can probably change to something simpler with the `RunnablePassthrough` for example)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
