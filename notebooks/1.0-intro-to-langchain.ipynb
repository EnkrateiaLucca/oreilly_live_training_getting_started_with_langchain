{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (0.1.8)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain) (2.0.27)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain) (0.0.21)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.24 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain) (0.1.45)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain) (0.1.3)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.24->langchain) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: langchain-openai in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (0.1.3)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain-openai) (0.1.45)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain-openai) (1.12.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-openai) (0.1.3)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-openai) (2.6.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2024.4.16)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain-openai) (3.6)\n",
      "Requirement already satisfied: certifi in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.3)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.42->langchain-openai) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.42->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.42->langchain-openai) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (2.2.1)\n",
      "Requirement already satisfied: langsmith in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (0.1.3)\n",
      "Collecting langsmith\n",
      "  Using cached langsmith-0.1.49-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langsmith) (3.10.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langsmith) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from langsmith) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from pydantic<3,>=1->langsmith) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from pydantic<3,>=1->langsmith) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from pydantic<3,>=1->langsmith) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from requests<3,>=2->langsmith) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from requests<3,>=2->langsmith) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from requests<3,>=2->langsmith) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/greatmaster/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages (from requests<3,>=2->langsmith) (2024.2.2)\n",
      "Using cached langsmith-0.1.49-py3-none-any.whl (115 kB)\n",
      "Installing collected packages: langsmith\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.3\n",
      "    Uninstalling langsmith-0.1.3:\n",
      "      Successfully uninstalled langsmith-0.1.3\n",
      "Successfully installed langsmith-0.1.49\n"
     ]
    }
   ],
   "source": [
    "# uncomment and run below:\n",
    "!pip install langchain\n",
    "!pip install langchain-openai\n",
    "!pip install -U langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain \n",
    "\n",
    "Working with LLMs involves in one way or another working with a specific type of abstraction: \"Prompts\".\n",
    "\n",
    "However, in the practical context of day-to-day tasks we expect LLMs to perform, these prompts won't be some static and dead type of abstraction. Instead we'll work with dynamic prompts re-usable prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lanchain\n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a framework that allows you to connect LLMs together by allowing you to work with modular components like prompt templates and chains giving you immense flexibility in creating tailored solutions powered by the capabilities of large language models.\n",
    "\n",
    "\n",
    "Its main features are:\n",
    "- **Components**: abstractions for working with LMs\n",
    "- **Off-the-shelf chains**: assembly of components for accomplishing certain higher-level tasks\n",
    "\n",
    "LangChain facilitates the creation of complex pipelines that leverage the connection of components like chains, prompt templates, output parsers and others to compose intricate pipelines that give you everything you need to solve a wide variety of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the core of LangChain, we have the following elements:\n",
    "\n",
    "- Models\n",
    "- Prompts\n",
    "- Output parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models**\n",
    "\n",
    "Models are nothing more than abstractions over the LLM APIs like the ChatGPT API.â€‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and add your api key!!!\n",
    "\n",
    "import os\n",
    "\n",
    "# Set OPENAI API Key\n",
    "\n",
    "# colab\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<put your openai api key here\"\n",
    "\n",
    "# OR (load from .env file)\n",
    "# make sure you have python-dotenv installed\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's great! LLMs, or Master of Laws degrees, are advanced law degrees that provide specialized knowledge in a specific area of law. What topics will you be covering in your training?\", response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 19, 'total_tokens': 58}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-967d773b-039e-4c02-8ebf-9703667eadd1-0', usage_metadata={'input_tokens': 19, 'output_tokens': 39, 'total_tokens': 58})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = chat_model.invoke(\"I am teaching a live-training\\\n",
    "    about LLMs!\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's great! LLMs, or Master of Laws degrees, are advanced law degrees that provide specialized knowledge in a specific area of law. What topics will you be covering in your training?\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can predict outputs from both LLMs and ChatModels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic components are:\n",
    "\n",
    "- Models\n",
    "- Prompt templates\n",
    "- Output parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Show me 5 examples of this concept: animal'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "template = \"Show me 5 examples of this concept: {concept}\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "prompt.format(concept=\"animal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"concept\": \"animal\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. A tiger is a wild animal that is known for its strength and agility.\\n2. A dolphin is a marine animal that is highly intelligent and social.\\n3. A bald eagle is a bird of prey that is a symbol of freedom and power.\\n4. A koala is a marsupial animal that is native to Australia and known for its cuddly appearance.\\n5. A cheetah is a fast-running animal that is the fastest land mammal in the world.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. A tiger is a wild animal that is known for its strength and agility.\n",
       "2. A dolphin is a marine animal that is highly intelligent and social.\n",
       "3. A bald eagle is a bird of prey that is a symbol of freedom and power.\n",
       "4. A koala is a marsupial animal that is native to Australia and known for its cuddly appearance.\n",
       "5. A cheetah is a fast-running animal that is the fastest land mammal in the world."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "Markdown(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the predict method over a string input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Snoozeberry', response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 21, 'total_tokens': 25}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-eca221a9-addf-445d-bd77-bda4cb262f72-0', usage_metadata={'input_tokens': 21, 'output_tokens': 4, 'total_tokens': 25})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What would be a good name for a dog that loves to nap??\"\n",
    "chat_model.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompts**\n",
    "\n",
    "The same works for prompts. Now, prompts are pieces of text we feed to LLMs, and LangChain allows you to work with prompt templates.\n",
    "\n",
    "Prompt Templates are useful abstractions for reusing prompts and they are used to provide context for the specific task that the language model needs to complete. \n",
    "\n",
    "A simple example is a `PromptTemplate` that formats a string into a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What is a good dog name for a dog that loves to sleeping?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts  import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"What is a good dog name for a dog that loves to {activity}?\")\n",
    "prompt.format(activity=\"sleeping\")\n",
    "# Output: \"What is a good dog name for a dog that loves to nap?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Parsers**\n",
    "\n",
    "OutputParsers convert the raw output from an LLM into a format that can be used downstream. Here is an example of an OutputParser that converts a comma-separated list into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. A serene watercolor painting of a peaceful countryside with rolling hills, a tranquil lake, and a vibrant sunset in the background.\\n2. A photograph of a vast desert landscape with towering sand dunes, scattered cacti, and a clear blue sky overhead.\\n3. A detailed oil painting of a lush, green forest with towering trees, a winding river, and sunlight filtering through the canopy.\\n4. A digital artwork of a futuristic cityscape with towering skyscrapers, flying cars, and neon lights illuminating the night sky.\\n5. A mixed media collage of a surreal landscape with floating islands, cascading waterfalls, and fantastical creatures roaming the land.', response_metadata={'token_usage': {'completion_tokens': 136, 'prompt_tokens': 19, 'total_tokens': 155}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a50a761b-5d4b-4316-b73c-1158eca43ece-0', usage_metadata={'input_tokens': 19, 'output_tokens': 136, 'total_tokens': 155})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"concept\": \"Landscapes\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Neurons: Artificial neural networks are inspired by the structure and function of biological neurons in the human brain. Neurons in artificial neural networks receive inputs, process them using an activation function, and produce an output.\\n\\n2. Layers: Artificial neural networks are typically organized into layers, with each layer containing multiple neurons. The input layer receives data, the hidden layers process the data through weighted connections, and the output layer produces the final output.\\n\\n3. Weights and biases: Weights and biases are parameters in artificial neural networks that are adjusted during the training process to minimize the error between the predicted output and the actual output. Weights determine the strength of the connections between neurons, while biases allow neurons to learn different patterns in the data.\\n\\n4. Activation functions: Activation functions introduce nonlinearity into artificial neural networks, allowing them to learn complex patterns in the data. Common activation functions include sigmoid, tanh, ReLU, and softmax.\\n\\n5. Backpropagation: Backpropagation is a key algorithm used to train artificial neural networks. It involves calculating the gradient of the loss function with respect to the weights and biases, and then updating them using an optimization algorithm such as stochastic gradient descent. This process allows the network to learn from its mistakes and improve its predictions over time.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Write 5 concepts that are fundamental to learn about {topic}.\n",
    "                                          \"\"\")\n",
    "chain = prompt | llm | output_parser\n",
    "chain.invoke({\"topic\": \"Artificial Neural Networks\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to an LLM, and then pass the output through an output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so these are the basics of langchain. But how can we leverage these abstraction capabilities inside our LLM app application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to put everything together LangChain allows you to build something called \"chains\", which are components that connect prompts, llms and output parsers into a building block that allows you to create more interesting and complex functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the example below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what the chain is doing is connecting these basic components (the LLM and the prompt template) into\n",
    "a block that can be run separately. The chain allows you to turn workflows using LLLMs into this modular process of composing components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the newer versions of LangChain have a new representation language to create these chains (and more) known as LCEL or LangChain expression language, which is a declarative way to easily compose chains together. The same example as above expressed in this LCEL format would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"1. Circadian Rhythms: The body's internal clock that regulates the sleep-wake cycle and influences when we feel alert or tired.\\n\\n2. Sleep Stages: The different stages of sleep, including REM (rapid eye movement) and non-REM, each serving different functions for rest and restoration.\\n\\n3. Sleep Disorders: Conditions that disrupt the normal sleep patterns, such as insomnia, sleep apnea, and narcolepsy.\\n\\n4. Sleep Hygiene: Healthy habits and practices that promote better sleep, including creating a comfortable sleep environment, maintaining a regular sleep schedule, and avoiding stimulants before bedtime.\\n\\n5. Impact of Sleep on Health: The important role sleep plays in overall health and well-being, including its effects on cognitive function, mood, immune system, and risk for chronic diseases.\", response_metadata={'token_usage': {'completion_tokens': 162, 'prompt_tokens': 21, 'total_tokens': 183}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c5bee4b6-59f6-4785-a5e7-538e96ae00c7-0', usage_metadata={'input_tokens': 21, 'output_tokens': 162, 'total_tokens': 183})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | ChatOpenAI()\n",
    "\n",
    "chain.invoke({\"topic\": \"sleep\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now the output is an `AIMessage()` object, which represents LangChain's way to abstract the output from an LLM model like ChatGPT or others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These building blocks and abstractions that LangChain provides are what makes this library so unique, because it gives you the tools you didn't know you need it to build awesome stuff powered by LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple chain for summarization of content. \n",
    "\n",
    "Your chain should:\n",
    "\n",
    "- A prompt template with one or more variables\n",
    "- A model like ChatGPT or other (you can use local models if you'd like, I recommend `ChatOllama` for that!)\n",
    "- Optional: use output parsing or just fetch the string output at the end!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Answer\n",
    "\n",
    "Let's make use of the `ChatPromptTemplate` to abstract away the following pieces of the prompt: \n",
    "- `content` - the text content to be summarized  \n",
    "- `summary_format` - the format in which we want the summary to be presented (like bullet points and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Summarize this: This is a test.. The output should be in the following format: One word summary.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Summarize this: {content}. The output should be in the following format: {summary_format}.\")\n",
    "\n",
    "# We can look at a simple example to illustrate what that prompt is doing\n",
    "prompt.format(content=\"This is a test.\", summary_format=\"One word summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have our prompt template done, let's load the llm and create a nice chain to put everything together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_chat =  ChatOpenAI()\n",
    "chain = prompt | llm_chat # This is the Pipe symbol! from LCEL that connect model to prompt!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have our chain we can run some tests. The cool thing about working with LLMs is that you can use them to create examples for simple tests like this (avoiding the annoynace of searching online for some piece of text, copying and pasting etc...). So, let's generate a few examples of tests below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='As technology continues to advance, the interaction between humans and machines becomes more integrated into everyday life. From virtual assistants like Siri and Alexa to self-driving cars, the reliance on machines to assist in our daily tasks is only growing. This shift in human-machine interaction raises questions about the future of work and the role of automation in society.\\n\\nWhile machines can greatly improve efficiency and productivity, there are concerns about the potential impact on employment. As more tasks become automated, there is a fear that jobs will be displaced and workers will be left without opportunities. However, proponents of automation argue that it can create new job opportunities and allow humans to focus on more creative and strategic tasks. Finding a balance between human labor and machine automation will be crucial in navigating the future of work and ensuring that both humans and machines can coexist harmoniously.', response_metadata={'token_usage': {'completion_tokens': 164, 'prompt_tokens': 25, 'total_tokens': 189}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0fde7da3-af91-4200-bc46-478e16b1a0a0-0', usage_metadata={'input_tokens': 25, 'output_tokens': 164, 'total_tokens': 189}),\n",
       " AIMessage(content='As technology continues to advance, the relationship between humans and machines becomes increasingly intertwined. From smartphones to smart homes, we rely on machines to assist us in our daily lives. This interaction has led to new ways of communication and problem-solving, as well as concerns about privacy and security.\\n\\nOne area of human-machine interaction that is rapidly growing is artificial intelligence. AI systems are becoming more sophisticated in understanding and responding to human language, leading to advancements in virtual assistants and chatbots. However, the ethical implications of AI have sparked debates about the potential impact on jobs, decision-making processes, and societal values. As we continue to navigate this complex relationship, it is crucial to consider the implications of our reliance on machines in shaping our future interactions.', response_metadata={'token_usage': {'completion_tokens': 146, 'prompt_tokens': 25, 'total_tokens': 171}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7e63aef2-a744-4ea0-b0cd-a88dfc2d7e79-0', usage_metadata={'input_tokens': 25, 'output_tokens': 146, 'total_tokens': 171}),\n",
       " AIMessage(content='As technology continues to advance, the interaction between humans and machines becomes increasingly seamless. From voice-activated virtual assistants to self-driving cars, the ways in which we interact with machines are constantly evolving. This raises questions about the impact of these interactions on our daily lives and the potential for machines to take over tasks traditionally performed by humans.\\n\\nOne area of concern is the potential for job displacement as machines become more capable of performing tasks that were once reserved for humans. However, proponents of human-machine interaction argue that this shift can lead to greater efficiency and productivity, allowing humans to focus on more creative and strategic work. Ultimately, the key to successful human-machine interaction lies in finding a balance between harnessing the capabilities of machines while also preserving the unique skills and abilities of humans.', response_metadata={'token_usage': {'completion_tokens': 153, 'prompt_tokens': 25, 'total_tokens': 178}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-66410f00-e4ca-4833-b6cc-fa34f070525d-0', usage_metadata={'input_tokens': 25, 'output_tokens': 153, 'total_tokens': 178})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples = 3\n",
    "examples = []\n",
    "for i in range(num_examples):\n",
    "    examples.append(llm_chat.invoke(\"Create a piece of text with 2 paragraphs about a random topic regarding human-machine interaction.\"))\n",
    "\n",
    "examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now that we have our examples, let's run our chain on them and check out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='- Technology advances lead to increased integration of humans and machines in daily life\\n- From virtual assistants to self-driving cars, reliance on machines for daily tasks is growing\\n- Shift raises questions about the future of work and role of automation in society\\n- Concerns about potential impact on employment as tasks become automated\\n- Debate between job displacement and creation of new opportunities through automation\\n- Finding a balance between human labor and machine automation is crucial for harmonious coexistence.', response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 310, 'total_tokens': 403}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-67ec51b2-de36-4ead-ba88-af5d76418cf1-0', usage_metadata={'input_tokens': 310, 'output_tokens': 93, 'total_tokens': 403})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_format = \"bullet points\"\n",
    "\n",
    "outputs = []\n",
    "for ex in examples:\n",
    "    outputs.append(chain.invoke({\"content\": ex, \"summary_format\": summary_format}))\n",
    "\n",
    "# Let's display one example output\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So it seems our chain worked and we generated some summaries! Let's visualize all the summaries generated in a neat way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Output 0 \n",
       " - Technology advances lead to increased integration of humans and machines in daily life\n",
       "- From virtual assistants to self-driving cars, reliance on machines for daily tasks is growing\n",
       "- Shift raises questions about the future of work and role of automation in society\n",
       "- Concerns about potential impact on employment as tasks become automated\n",
       "- Debate between job displacement and creation of new opportunities through automation\n",
       "- Finding a balance between human labor and machine automation is crucial for harmonious coexistence."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Output 1 \n",
       " - As technology advances, humans and machines are becoming more intertwined in various aspects of daily life.\n",
       "- The reliance on machines for communication and problem-solving has raised concerns about privacy and security.\n",
       "- Artificial intelligence is rapidly growing, with AI systems becoming more sophisticated in understanding and responding to human language.\n",
       "- Ethical debates surrounding AI focus on its potential impact on jobs, decision-making processes, and societal values.\n",
       "- It is important to consider the implications of our reliance on machines in shaping future interactions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Output 2 \n",
       " - Technology is advancing, making interactions between humans and machines more seamless\n",
       "- Voice-activated virtual assistants and self-driving cars are examples of evolving interactions with machines\n",
       "- Concerns arise about the impact on daily lives and the potential for machines to replace human tasks\n",
       "- Job displacement is a key concern as machines become more capable\n",
       "- Proponents argue that human-machine interaction can lead to greater efficiency and productivity\n",
       "- Finding a balance between harnessing machine capabilities and preserving human skills is crucial for successful interaction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "for i in range(num_examples):\n",
    "    display(Markdown(f\"Output {i} \\n {outputs[i].content}\"))\n",
    "# Markdown(f\"**Input**: {examples[0]}\\n\\n**Output**: {outputs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our summaries worked, and we were able to apply a given summary format to all of them.\n",
    "\n",
    "LangChain is an extremely powerful library to work with abstractions like these and throughout this course we hope to give you a gliimpse of the cool stuff you can build with it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
