{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 LangGraph Quick Introduction\n",
    "\n",
    "This notebook provides an introduction to **LangGraph 1.0+**, the framework for building stateful, multi-actor applications with LLMs.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Core LangGraph concepts: State, Nodes, and Edges\n",
    "- Building a simple agent with tools\n",
    "- Memory and persistence with checkpointers\n",
    "- Building chatbots with conversation history\n",
    "\n",
    "Check out the [LangGraph documentation](https://langchain-ai.github.io/langgraph/concepts/#background-agents-ai-workflows-as-graphs) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain 1.0+ and LangGraph 1.0+ Setup\n",
    "%pip install -qU langchain>=1.0.0 langgraph>=1.0.0\n",
    "%pip install -qU langchain-openai\n",
    "%pip install -qU langchain-tavily  # New package for Tavily search tools\n",
    "%pip install -qU tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Enable LangSmith tracing (optional but recommended)\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langgraph-introduction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Core Concepts\n",
    "\n",
    "LangGraph models agent workflows as **state machines**. You define behavior using three key components:\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **State** | A shared data structure representing the current snapshot of your application. Typically a `TypedDict` or Pydantic `BaseModel`. |\n",
    "| **Nodes** | Python functions that receive the current State, perform computation or side-effects, and return an updated State. |\n",
    "| **Edges** | Control flow rules determining which Node to execute next based on the current State. Can be conditional or fixed. |\n",
    "\n",
    "By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building a Simple Agent with LangGraph\n",
    "\n",
    "Let's build an agent that can use tools. This demonstrates:\n",
    "- Defining custom tools with the `@tool` decorator\n",
    "- Creating a state graph with conditional routing\n",
    "- Using the `ToolNode` prebuilt component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools for the agent to use\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search the web for current information.\"\"\"\n",
    "    # Placeholder implementation - in production, use TavilySearch\n",
    "    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n",
    "        return \"It's 60 degrees and foggy.\"\n",
    "    return \"It's 90 degrees and sunny.\"\n",
    "\n",
    "\n",
    "tools = [search]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Bind tools to the model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define routing logic: continue to tools or end?\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"Determine whether to use tools or finish.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If the LLM makes a tool call, route to the tools node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we're done\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the node that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    \"\"\"Invoke the model with the current messages.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    # Return as list to append to existing messages\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Add conditional edge from agent\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# After tools, always go back to agent\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Add memory for persistence\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent\n",
    "result = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is the weather in San Francisco?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}}\n",
    ")\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the full message history\n",
    "for msg in result[\"messages\"]:\n",
    "    print(f\"{msg.__class__.__name__}: {msg.content[:100] if msg.content else '[tool call]'}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building a Simple Chatbot\n",
    "\n",
    "Now let's build a simpler chatbot without tools - just a model with memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# The model can maintain context when given message history\n",
    "response = llm.invoke([\n",
    "    HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "    AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "    HumanMessage(content=\"What's my name?\"),\n",
    "])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Chatbot with LangGraph Persistence\n",
    "\n",
    "For a true chatbot experience, we use LangGraph's built-in persistence to maintain conversation history across turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define the graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    \"\"\"Call the model with current messages.\"\"\"\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Add single node and edge\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "chatbot = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "display(Image(chatbot.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with memory - same thread_id maintains conversation\n",
    "config = {\"configurable\": {\"thread_id\": \"chat-123\"}}\n",
    "\n",
    "# First message\n",
    "output = chatbot.invoke({\"messages\": [HumanMessage(\"Hi! I'm Bob.\")]}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up - the chatbot remembers!\n",
    "output = chatbot.invoke({\"messages\": \"What is my name?\"}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View full conversation history\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Chatbot with Custom System Prompt\n",
    "\n",
    "Let's add a system prompt to customize the chatbot's personality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Create a prompt with a custom system message\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You talk like a pirate. Answer all questions to the best of your ability.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke(state)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "pirate_chatbot = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"pirate-chat-1\"}}\n",
    "\n",
    "output = pirate_chatbot.invoke(\n",
    "    {\"messages\": [HumanMessage(\"Hi! I'm Jim.\")]}, \n",
    "    config\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory\n",
    "output = pirate_chatbot.invoke(\n",
    "    {\"messages\": [HumanMessage(\"What is my name?\")]}, \n",
    "    config\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Adding Custom State Variables\n",
    "\n",
    "Sometimes you need to track more than just messages. Let's create a chatbot that responds in a user-specified language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# Custom state with language parameter\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "# Prompt that uses the language variable\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke(state)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(\"model\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "multilingual_chatbot = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(multilingual_chatbot.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"french-chat\"}}\n",
    "\n",
    "output = multilingual_chatbot.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(\"Hi! I'm Bob.\")],\n",
    "        \"language\": \"French\"\n",
    "    },\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation\n",
    "output = multilingual_chatbot.invoke(\n",
    "    {\"messages\": [HumanMessage(\"What is my name?\")]},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View full state\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Using the New `create_agent` API (LangChain 1.0+)\n",
    "\n",
    "LangChain 1.0 introduces `create_agent` - a simplified way to create agents without manually building graphs.\n",
    "\n",
    "**Key features:**\n",
    "- Single function call to create an agent\n",
    "- Model can be specified as a string (`\"openai:gpt-4o-mini\"`)\n",
    "- Built-in support for tools, system prompts, and middleware\n",
    "- Returns a `CompiledStateGraph` that you can invoke directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the current weather for a location.\"\"\"\n",
    "    # Placeholder - in production use a real weather API\n",
    "    return f\"The weather in {location} is 72Â°F and sunny.\"\n",
    "\n",
    "\n",
    "# Create agent with the new API\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",  # String format!\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful weather assistant. Always be concise.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the agent - note the messages format\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]\n",
    "})\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream the response\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Paris?\"}]},\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TavilySearch with create_agent\n",
    "\n",
    "Let's create an agent that can search the web using the new `langchain-tavily` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "# Create the Tavily search tool\n",
    "tavily_search = TavilySearch(\n",
    "    max_results=3,\n",
    "    topic=\"general\",\n",
    ")\n",
    "\n",
    "# Create agent with search capability\n",
    "search_agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[tavily_search],\n",
    "    system_prompt=\"You are a helpful research assistant. Use the search tool to find current information. Always cite your sources.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for current information\n",
    "result = search_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What are the latest developments in LangChain 1.0?\"}]\n",
    "})\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **LangGraph Fundamentals**: State, Nodes, and Edges for building stateful AI workflows\n",
    "2. **Building Agents Manually**: Creating agents with tools using `StateGraph`, `ToolNode`, and conditional edges\n",
    "3. **Persistence**: Using `MemorySaver` to maintain conversation history across turns\n",
    "4. **Custom State**: Extending `MessagesState` with additional variables like `language`\n",
    "5. **LangChain 1.0 `create_agent`**: The new simplified API for creating agents\n",
    "\n",
    "### Key LangChain 1.0 Changes\n",
    "\n",
    "| Old Pattern | New Pattern |\n",
    "|------------|-------------|\n",
    "| `AgentExecutor` | `create_agent()` returns a graph |\n",
    "| `create_react_agent()` | `create_agent()` |\n",
    "| `TavilySearchResults` | `TavilySearch` from `langchain-tavily` |\n",
    "| Instantiate model classes | String format: `\"openai:gpt-4o-mini\"` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
