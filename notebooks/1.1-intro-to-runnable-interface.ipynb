{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LangChain 1.0+ Runnable Interface\n# %pip install langchain>=1.0.0\n# %pip install langchain-core>=1.0.0\n# %pip install langchain-openai"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look into the `Runnable` components in more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableLambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "# Create a RunnableLambda and invoke it\n",
    "runnable = RunnableLambda(lambda x: x*23)\n",
    "output = runnable.invoke(5)\n",
    "print(output)  # Output: 115"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`RunnableSequence`](https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable:~:text=runnablesequence%20invokes%20a%20series%20of%20runnables%20sequentially%2C%20with%20one%20runnable%E2%80%99s%20output%20serving%20as%20the%20next%E2%80%99s%20input.%20construct%20using%20the%20%7C%20operator%20or%20by%20passing%20a%20list%20of%20runnables%20to%20runnablesequence.)\n",
    "\n",
    "RunnableSequence invokes a series of runnables sequentially, with one runnable’s output serving as the next’s input. Construct using the | operator or by passing a list of runnables to RunnableSequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "# Suppose we have a list of numbers\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Create RunnableLambdas\n",
    "preprocess_runnable = RunnableLambda(lambda x: [i+1 for i in x])\n",
    "apply_model_runnable = RunnableLambda(lambda x: x*23)\n",
    "postprocess_runnable = RunnableLambda(lambda x: sum(x))\n",
    "\n",
    "# Create a RunnableSequence and invoke it\n",
    "sequence = preprocess_runnable | apply_model_runnable | postprocess_runnable\n",
    "# runnable_sequence = RunnableSequence(sequence)\n",
    "output = sequence.invoke(data)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1,2,3,4,5]\n",
    "\n",
    "# # Runnable 1\n",
    "# [2,3,4,5,6]\n",
    "\n",
    "# # Runnable 2 \n",
    "# [2,3,4,5,6] * 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(output)\n",
    "# or\n",
    "runnable_sequence = RunnableSequence(first=preprocess_runnable, middle=[apply_model_runnable], last=postprocess_runnable)\n",
    "runnable_sequence.invoke(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Runnableparallel`]([`Runnableparallel`](https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable:~:text=runnableparallel%20invokes%20runnables%20concurrently%2C%20providing%20the%20same%20input%20to%20each.%20construct%20it%20using%20a%20dict%20literal%20within%20a%20sequence%20or%20by%20passing%20a%20dict%20to%20runnableparallel.)https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable:~:text=runnableparallel%20invokes%20runnables%20concurrently%2C%20providing%20the%20same%20input%20to%20each.%20construct%20it%20using%20a%20dict%20literal%20within%20a%20sequence%20or%20by%20passing%20a%20dict%20to%20runnableparallel.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableParallel invokes runnables concurrently, providing the same input to each. Construct it using a dict literal within a sequence or by passing a dict to RunnableParallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.3\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6, 8]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# A RunnableSequence constructed using the `|` operator\n",
    "sequence = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)\n",
    "sequence.invoke(1) # 4\n",
    "sequence.batch([1, 2, 3]) # [4, 6, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mul_2': 4, 'mul_5': 10}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A sequence that contains a RunnableParallel constructed using a dict literal\n",
    "sequence = RunnableLambda(lambda x: x + 1) | {\n",
    "    'mul_2': RunnableLambda(lambda x: x * 2),\n",
    "    'mul_5': RunnableLambda(lambda x: x * 5)\n",
    "}\n",
    "sequence.invoke(1) # {'mul_2': 4, 'mul_5': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multiply': 2, 'add': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "sequence_with_runnable_parallel = RunnableParallel(\n",
    "    multiply=RunnableLambda(lambda x: x * 2),\n",
    "    add=RunnableLambda(lambda x: x + 2))\n",
    "\n",
    "sequence_with_runnable_parallel.invoke(1) # [2, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you build chains, what you're doing is building a type of Runnable!\n",
    "\n",
    "That could be a RunnableSequence or a RunnableParallel  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': AIMessage(content='The Single Responsibility Principle states that each class or module in a program should have only one responsibility or reason to change. This helps to keep code clean, maintainable, and easy to understand by separating concerns and preventing classes from becoming too complex.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 16, 'total_tokens': 65, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3037e21d-9c97-4d96-8352-9cb9068bf6dd-0', usage_metadata={'input_tokens': 16, 'output_tokens': 49, 'total_tokens': 65, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " 'explanation': AIMessage(content=\"The Single Responsibility Principle (SRP) is a design principle in object-oriented programming that states that a class should have only one reason to change, meaning it should have only one responsibility. This principle suggests that a class should only have one job or task to perform and should not be responsible for multiple unrelated tasks.\\n\\nBy adhering to the SRP, a class becomes more cohesive and easier to maintain, as each class focuses on a single aspect of the application's functionality. This allows for better code organization, readability, and reusability. It also makes it easier to test and debug the code, as changes to one responsibility do not affect other unrelated responsibilities.\\n\\nOverall, following the Single Responsibility Principle helps to create more modular, flexible, and maintainable code that is easier to understand and work with.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 161, 'prompt_tokens': 15, 'total_tokens': 176, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ae101f7a-e1e1-4958-9fc5-0585458423e0-0', usage_metadata={'input_tokens': 15, 'output_tokens': 161, 'total_tokens': 176, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = PromptTemplate.from_template(\"Summarize this {text}\")\n",
    "prompt2 = PromptTemplate.from_template(\"Explain this {text}\")\n",
    "\n",
    "\n",
    "chain1 = prompt | llm\n",
    "\n",
    "chain2 = prompt2 | llm\n",
    "\n",
    "runnable_parallel = RunnableParallel(summary=chain1, explanation=chain2)\n",
    "\n",
    "runnable_parallel.invoke({\"text\": \"Single responsibility principle in programming\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableParallel"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "type(runnable_parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RunnableParallel can be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence.](https://python.langchain.com/docs/expression_language/how_to/map#:~:text=RunnableParallel%20can%20be,the%0A%E2%80%9Cquestion%E2%80%9D%20key.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableParallel (aka. RunnableMap) makes it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.\n",
    "\n",
    "([see it in docs here](https://python.langchain.com/docs/expression_language/how_to/map#:~:text=runnableparallel%20(aka.%20runnablemap)%20makes%20it%20easy%20to%20execute%20multiple%20runnables%20in%20parallel%2C%20and%20to%20return%20the%20output%20of%20these%20runnables%20as%20a%20map.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': AIMessage(content='Why did the bear break up with his girlfriend? \\n\\nBecause she was unbearable!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 13, 'total_tokens': 30, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-fa18f369-248b-40d2-9135-4bdddecdc8f5-0', usage_metadata={'input_tokens': 13, 'output_tokens': 17, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " 'poem': AIMessage(content=\"In the forest's embrace, the bear roams free\\nMajestic and wild, a sight to see.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 15, 'total_tokens': 39, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e7c3f333-7ae0-4bff-8e60-3116e998d9a0-0', usage_metadata={'input_tokens': 15, 'output_tokens': 24, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "model = ChatOpenAI()\n",
    "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
    ")\n",
    "\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "map_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableParallel are also useful for running independent processes in parallel, since each Runnable in the map is executed in parallel. For example, we can see our earlier joke_chain, poem_chain and map_chain all have about the same runtime, even though map_chain executes both of the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939 ms ± 146 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "joke_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "860 ms ± 102 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "poem_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19 s ± 528 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "joke_chain.invoke({\"topic\": \"bear\"})\n",
    "\n",
    "poem_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11 s ± 195 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "map_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RunnablePassthrough](https://python.langchain.com/docs/expression_language/how_to/passthrough#:~:text=RunnablePassthrough%20allows%20to,pass%20it%20through.)\n",
    "\n",
    "Allows you to pass inputs unchanged or with addition of new keys.\n",
    "\n",
    "Usually you would connect this with `RunnableParallel` to assign data to new key in the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just call it, it will take the input and pass it along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Create a RunnablePassthrough\n",
    "passthrough = RunnablePassthrough()\n",
    "passthrough.invoke(5) # 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you call it with the `.assign(...)` mthod, then it will take the input, and add extra arguments passed to the assign function. \n",
    "\n",
    "The key being added has to be a lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 1, 'extra_key_add_5': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# Passthrough with assignment, adds an extra key-value pair\n",
    "runnable = RunnablePassthrough.assign(extra_key_add_5=lambda x:  x[\"num\"]+5)\n",
    "input_data = {\"num\": 1}\n",
    "result = runnable.invoke(input_data)\n",
    "# Output: {'num': 1, 'extra_value': 42}\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In combination with RunnableParallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'origin': 1, 'modified': 2}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runnable = RunnableParallel(\n",
    "    origin=RunnablePassthrough(),\n",
    "    modified=lambda x: x+1\n",
    ")\n",
    "\n",
    "runnable.invoke(1) # {'origin': 1, 'modified': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': 'completion', 'parsed': 'noitelpmoc (applied the parsing logic)'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# modified example from langchain docs: https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html?highlight=runnablepassthrough#langchain_core.runnables.passthrough.RunnablePassthrough \n",
    "# or for an \"LLM\" example:\n",
    "\n",
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return \"completion\"\n",
    "\n",
    "chain = RunnableLambda(fake_llm) | {\n",
    "    'original': RunnablePassthrough(), # Original LLM output\n",
    "    'parsed': lambda text: text[::-1] + \" (applied the parsing logic)\" # Parsing logic\n",
    "}\n",
    "\n",
    "chain.invoke('hello') # {'original': 'completion', 'parsed': 'noitelpmoc'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so we have these 4 types of main objects:\n",
    "\n",
    "- `RunnableLambda`\n",
    "- `RunnableSequence`\n",
    "- `RunnablePassthrough`\n",
    "- `RunnableParallel`\n",
    "\n",
    "combined they make up kind of the core building block system of langchain allow you to create complex inner logics powered by llms. \n",
    "\n",
    "Let's create some fun examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LangChain 1.0: Use langchain_openai instead of langchain.chat_models\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.runnables import RunnablePassthrough, RunnableLambda\n\ndef fun_llm(prompt: str):\n    return ChatOpenAI().invoke(f\"Make this funny: {prompt}\")\n\n\ndef corporate_llm(prompt: str):\n    return ChatOpenAI().invoke(f\"Make this topic a subject of a very corporate email: {prompt}\")\n\nfun_chain = RunnableLambda(fun_llm) | {\n    \"original-text-input\": RunnablePassthrough(), \n    \"make-it-corporate\": lambda x: corporate_llm(x)\n}\n\n\nfun_chain.invoke(\"gorillas\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm1': 'completion', 'llm2': 'completion', 'total_chars': 20}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return \"completion\"\n",
    "\n",
    "runnable = {\n",
    "    'llm1':  fake_llm,\n",
    "    'llm2':  fake_llm,\n",
    "} | RunnablePassthrough.assign(\n",
    "    total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2'])\n",
    "  )\n",
    "\n",
    "runnable.invoke('hello')\n",
    "# {'llm1': 'completion', 'llm2': 'completion', 'total_chars': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking down whats happening:\n",
    "\n",
    "We create a `runnable` object, which is an instance of the `RunableSequence` object. This chain takes in as input a simple string: `hello`, and what it does is it \n",
    "simulates passing that same string to 2 `different` llms (which in this case are the same) and applies the logic of each of these llms as well as a third logic described by the new extra key added by the RunnablePassthrough.assign() method, that takes as input the output of the `llm1` and `llm2` as input to combine it through the lambda function associated with the new extra key created: `total_chars`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this example a bit more interesting by applying some research workflow vibe to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a couple of llms to do different things with some piece of content.\n",
    "# In this case let's use different llms to create summarization levels for some text.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def llm_summarization_level1(prompt: str):\n",
    "    return ChatOpenAI().invoke(f\"Summarize this text: {prompt}\")\n",
    "\n",
    "def llm_summarization_level2(prompt: str):\n",
    "    return ChatOpenAI().invoke(f\"Summarize this text in bullet points: {prompt}\")\n",
    "\n",
    "def llm_summarization_level3(prompt: str):\n",
    "    return ChatOpenAI().invoke(f\"Summarize this text into one short paragraph: {prompt}\")\n",
    "\n",
    "\n",
    "# now let's create a chain that will run all of these llms in parallel and then return the results\n",
    "\n",
    "runnable = {\n",
    "    'level1':  llm_summarization_level1,\n",
    "    'level2':  llm_summarization_level2,\n",
    "    'level3':  llm_summarization_level3,\n",
    "} | RunnablePassthrough.assign(prompt_level4=lambda x: f\"Combine these 3 summarizations\\\n",
    "    into one combining all their \\n \\\n",
    "    useful features. \\\n",
    "    Level1: {x['level1']}.\\n \\\n",
    "    Level2: {x['level2']}.\\n\\\n",
    "    Level3: {x['level3']}\"\n",
    "                       ) | RunnablePassthrough.assign(output=RunnableLambda(lambda x: ChatOpenAI().invoke(x['prompt_level4']))) \n",
    "# For the assign method with RunningPasshthrough, we'll combine the summarizations into a fourth one.\n",
    "\n",
    "output = runnable.invoke(\n",
    "    \"\"\"\n",
    "    The code you provided is from the nest_asyncio library, which is designed to patch Python's asyncio library to allow nested usage of the asyncio event loop. To understand this code, it's crucial first to grasp what an event loop is in the context of asynchronous programming.\n",
    "What is an Event Loop?\n",
    "An event loop in programming, particularly in asynchronous programming, is a central control structure that manages and dispatches events or messages in a program. In the context of Python's asyncio library, the event loop is a core feature that runs asynchronous tasks and callbacks, handles network IO operations, and manages subprocesses. It is essentially the heart of the asyncio module, enabling asynchronous programming by juggling and scheduling the execution of various tasks.\n",
    "Key Concepts in the nest_asyncio Code\n",
    "Patching asyncio: The code modifies (patches) certain parts of the asyncio library. This is done to change the default behavior of asyncio, particularly to support nested event loops, which are not allowed in standard asyncio.\n",
    "Reentrancy in Event Loops: The primary function of nest_asyncio is to make the asyncio event loop reentrant. In computing, reentrancy refers to the ability of a function to be paused in the middle of execution and safely called again (\"re-entered\") before its previous executions are complete. This is not normally supported by the asyncio event loops, as they are designed to prevent re-entry (or nesting) to avoid complex problems and unexpected behavior.\n",
    "Modifying Loop Behavior: The code alters the behavior of the event loop methods like run_forever, run_until_complete, and the internal _run_once. These modifications allow the event loop to pause and resume (re-enter) gracefully, facilitating nesting.\n",
    "Context Managers: The code uses context managers (manage_run and manage_asyncgens) to properly manage the state of the event loop during entry and exit of asynchronous contexts, which is crucial for handling nested loops correctly.\n",
    "Task Patching: It also patches the Task class of asyncio to modify its step function. This is necessary to ensure that the tasks (units of work scheduled by the event loop) behave correctly in a nested loop scenario.\n",
    "Tornado Patching: If the Tornado library (an asynchronous networking library) is used, nest_asyncio makes Tornado aware of the Python asyncio Future, ensuring compatibility.\n",
    "Understanding the Event Loop in Async Programming\n",
    "In asynchronous programming, especially in Python's asyncio, the event loop is pivotal. It allows the execution of multiple tasks seemingly in parallel by switching between them. This switching is non-blocking, meaning the program can handle other tasks while waiting for some IO operation to complete, thereby increasing efficiency and responsiveness.\n",
    "\n",
    "The nest_asyncio library's primary role is to tweak the asyncio's event loop to support nested operation, which is particularly useful in scenarios like running an asyncio event loop inside another asyncio event loop, something that standard asyncio does not support by default.\n",
    "\n",
    "For more details on asynchronous programming and event loops in Python, the Python asyncio documentation is a comprehensive resource.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The text discusses the nest_asyncio library, which patches Python's asyncio library to allow nested event loops. It explains the concept of an event loop in asynchronous programming and how the code modifies the asyncio library to support nested event loops. The code alters the behavior of event loop methods, uses context managers, patches the Task class, and ensures compatibility with the Tornado library. The primary role of nest_asyncio is to enable nested operation of asyncio event loops, increasing efficiency and responsiveness in asynchronous programming."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(output['level1'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- The code provided is from the nest_asyncio library, which patches Python's asyncio library for nested event loop usage\n",
       "- An event loop in programming manages and dispatches events or messages in a program\n",
       "- Key concepts in the nest_asyncio code include patching asyncio, reentrancy in event loops, modifying loop behavior, using context managers, task patching, and Tornado patching\n",
       "- The event loop in asynchronous programming allows for the execution of multiple tasks seemingly in parallel\n",
       "- Nest_asyncio tweaks asyncio's event loop to support nested operation, which standard asyncio does not support by default"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Markdown(output['level2'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The text discusses the role of the nest_asyncio library in patching Python's asyncio library to enable nested usage of the event loop. It explains the concept of event loops in asynchronous programming and how the nest_asyncio code modifies asyncio to support nested event loops. By making the event loop reentrant and modifying loop behavior, the code allows for graceful nesting and proper management of asynchronous contexts. Additionally, it patches the Task class and ensures compatibility with the Tornado library. Overall, understanding the event loop is crucial in asynchronous programming for efficient and responsive task execution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Markdown(output['level3'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple research assistant, heavily based on this implementation by Harrison Chase:\n",
    "- https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "import os\n",
    "from langchain.tools import tool\n",
    "from serpapi import GoogleSearch\n",
    "import json\n",
    "\n",
    "RESULTS_PER_QUESTION = 1\n",
    "\n",
    "serpapi_params = {\n",
    "    \"engine\": \"google\",\n",
    "    \"api_key\": os.environ[\"SERPAPI_KEY\"]\n",
    "}\n",
    "\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Finds general knowledge information using Google search. Can also be used\n",
    "    to augment more 'general' knowledge to a previous specialist query.\"\"\"\n",
    "    search = GoogleSearch({**serpapi_params, \"q\":query, \"n\": 3})\n",
    "    results = search.get_dict()[\"organic_results\"]\n",
    "    urls = [r[\"link\"] for r in results]\n",
    "    \n",
    "    return urls\n",
    "\n",
    "\n",
    "def scrape_text(url: str):\n",
    "    # Send a GET request to the webpage\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extract all text from the webpage\n",
    "            page_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Print the extracted text\n",
    "            return page_text\n",
    "        else:\n",
    "            return f\"Failed to retrieve the webpage: Status code {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Failed to retrieve the webpage: {e}\"\n",
    "\n",
    "\n",
    "def collapse_list_of_lists(list_of_lists):\n",
    "    content = []\n",
    "    for l in list_of_lists:\n",
    "        content.append(\"\\n\\n\".join(l))\n",
    "    return \"\\n\\n\".join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_TEMPLATE = \"\"\"{text} \n",
    "-----------\n",
    "Using the above text, answer in short the following question: \n",
    "> {question}\n",
    "-----------\n",
    "if the question cannot be answered using the text, imply summarize the text. Include all factual information, numbers, stats etc if available.\"\"\"  # noqa: E501\n",
    "SUMMARY_PROMPT = ChatPromptTemplate.from_template(SUMMARY_TEMPLATE)\n",
    "\n",
    "\n",
    "url = \"https://python.langchain.com/docs/concepts/\"\n",
    "\n",
    "scrape_and_summarize_chain = RunnablePassthrough.assign(\n",
    "    summary = RunnablePassthrough.assign(\n",
    "    text=lambda x: scrape_text(x[\"url\"])[:10000]\n",
    ") | SUMMARY_PROMPT | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n",
    ") | (lambda x: f\"URL: {x['url']}\\n\\nSUMMARY: {x['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, even though this chain requires two keys in the input dictionary, that's not reflected in the input_schema() of the chain, because in it only goes the variables contained in the ChatPromptTemplate(). \n",
    "\n",
    "Interestingly enough, the `text` variable which is in the ChatPromptTemplate, is not in the schema because its already part of the chain itself (therefore not being necessary in the input_schema() I guess)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'URL: https://python.langchain.com/docs/concepts/\\n\\nSUMMARY: Some important concepts in LangChain include:\\n\\n1. **Chat Models**: LLMs that process sequences of messages and output messages.\\n2. **Messages**: Units of communication in chat models representing input and output.\\n3. **Chat History**: A conversation represented as a sequence of alternating user messages and model responses.\\n4. **Tools**: Functions with schemas defining their name, description, and arguments.\\n5. **Tool Calling**: A chat model API that accepts tool schemas and messages, returning invocations of those tools.\\n6. **Structured Output**: A technique for models to respond in structured formats like JSON.\\n7. **Memory**: Persistence of conversation information for future use.\\n8. **Multimodality**: Working with various data forms like text, audio, images, and video.\\n9. **Runnable Interface**: The base abstraction for many LangChain components.\\n10. **Streaming**: APIs that surface results as they are generated.\\n11. **LangChain Expression Language (LCEL)**: A syntax for orchestrating components in simpler applications.\\n12. **Document Loaders**: Tools to load sources as lists of documents.\\n13. **Retrieval**: Systems that retrieve data from a datasource based on queries.\\n14. **Text Splitters**: Tools that break long text into smaller, indexable chunks.\\n15. **Embedding Models**: Models used for creating vector representations of text.\\n\\nThe text also mentions a conference, \"Interrupt: The Agent AI Conference by LangChain,\" occurring on May 13 & 14 in San Francisco.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# At this\n",
    "# let's test this chain on some url\n",
    "scrape_and_summarize_chain.invoke({\"question\": \"What are some important concepts in langchain?\", \"url\": url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_chain = RunnablePassthrough.assign(\n",
    "    urls = lambda x: web_search(x[\"question\"]) # urls will be the output of the web_search function()\n",
    ") | (lambda x: [{\"question\": x[\"question\"], \"url\": u} for u in x[\"urls\"]]) | scrape_and_summarize_chain.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test this web search chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"URL: https://python.langchain.com/docs/concepts/tool_calling/\\n\\nSUMMARY: Tool calling in LangChain allows AI models to interact with external systems, such as databases or APIs, by using a structured input schema. It involves four key concepts:\\n\\n1. **Tool Creation**: Tools are created using the `@tool` decorator, which associates a function with its input schema.\\n2. **Tool Binding**: The created tool must be connected to a model that supports tool calling, allowing the model to understand the tool and its required input.\\n3. **Tool Calling**: The model can decide when to utilize a tool, ensuring its response matches the tool's input schema.\\n4. **Tool Execution**: The tool is executed with the arguments provided by the model.\\n\\nA typical workflow includes creating tools, binding them to a model, and invoking the model with user input, which may lead to tool calls. An example tool is a function that multiplies two integers.\\n\\nFor more details, refer to the model integrations that support tool calling and further conceptual guides provided by LangChain.\",\n",
       " 'URL: https://python.langchain.com/v0.1/docs/modules/model_io/chat/function_calling/\\n\\nSUMMARY: Tool calling, also known as function calling, in LangChain allows a model to generate output that matches a user-defined schema by producing calls to tools or functions. The model creates arguments for these tools, but executing the tools is the user\\'s responsibility. Tool calls consist of a name, an arguments dictionary structured as `{argument_name: argument_value}`, and optionally an identifier.\\n\\nMany LLM providers support tool calling features, enabling models to handle requests that include tool schemas. LangChain provides built-in tools and allows users to define custom tools using decorators or Pydantic classes. For example, tools can be defined to perform operations like addition or multiplication.\\n\\nTo use tool calling, developers must bind tool schemas to a chat model via the `.bind_tools()` method. Models can choose to call tools or not, but some models support a `tool_choice` parameter that can enforce tool calls. For instance, setting `tool_choice=\"multiply\"` ensures the multiply tool is always called.\\n\\nWhen the model generates tool calls, they appear in the `.tool_calls` attribute of the model\\'s response. This can include valid tool calls or malformed calls, which are captured as `InvalidToolCall` instances. Developers can further process outputs using output parsers, such as converting results back to Pydantic classes.\\n\\nLangChain\\'s tool calling feature is valuable for creating structured outputs and building agent chains.',\n",
       " 'URL: https://blog.langchain.dev/tool-calling-with-langchain/\\n\\nSUMMARY: Tool calling in LangChain refers to a new feature allowing large language models (LLMs) to interact with external data sources through standardized tool invocations. This functionality has been introduced via a new `tool_calls` attribute on the `AIMessage` class, enabling a consistent interface for accessing tool calls across various LLM providers.\\n\\nKey points about tool calling in LangChain:\\n\\n1. **Standardized Interface**: The `tool_calls` attribute simplifies the extraction of tool invocations, which were previously handled differently by various model APIs. This standardization facilitates switching between different LLM providers.\\n\\n2. **Providers with Tool Calling Support**: Several LLM providers have introduced native tool calling capabilities, including:\\n   - OpenAI (first released in November, with \"function calling\")\\n   - Gemini (December)\\n   - Mistral (February)\\n   - Fireworks (March)\\n   - Together (March)\\n   - Groq (April)\\n   - Cohere (April)\\n   - Anthropic (April)\\n\\n3. **Key Components**:\\n   - `ChatModel.bind_tools()`: This method allows developers to specify available tools for a model by passing tool definitions, which can include Pydantic classes, LangChain tools, and raw functions.\\n   - `AIMessage.tool_calls`: A standardized way to access tool invocations returned by the model, encapsulated as `ToolCall` objects.\\n   - `create_tool_calling_agent()`: A function to build agents that work with any tool-calling model, making it easy to create sophisticated applications.\\n\\n4. **Usage Example**: Developers can invoke models with bound tools, and the output will include the `tool_calls` attribute, which contains details about the invoked tools.\\n\\n5. **Agent Creation Across Models**: The ability to create agents using different LLMs (like Anthropic, OpenAI, and VertexAI) remains consistent, making the framework versatile.\\n\\n6. **Integration with LangGraph**: The new interface simplifies the construction of agents and flows in LangGraph, a LangChain extension for building complex interactions.\\n\\nFor full documentation and further details, developers are encouraged to refer to the tool-calling documentation provided by LangChain.',\n",
       " 'URL: https://python.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/\\n\\nSUMMARY: Tool calling in LangChain allows a model to intelligently detect when one or more tools should be called and to respond with the appropriate inputs for those tools. This is achieved through structured output, such as JSON, which enhances the reliability of tool calls compared to generic text completions. The tool calling mechanism supports various providers such as OpenAI, Anthropic, Google Gemini, and Mistral, enabling a more generalized approach than the specific OpenAI tools agent.\\n\\nTo set up tool calling, developers need to initialize models that support this feature, set the necessary API keys, and create tools (e.g., web search tools). An agent is then created to handle the tool calling, which can invoke multiple tools until a query is resolved. \\n\\nFor example, in a demonstration, a tool for searching the web was created using the Tavily API. The agent was initialized with a prompt and executed to answer a question about LangChain, successfully retrieving relevant information.\\n\\nKey features of LangChain include:\\n- **Retrieval augmented generation**: Allows LLMs to access external data sources.\\n- **Analyzing structured data**: Tools for working with databases, APIs, and PDFs.\\n- **Building chatbots and agents**: Frameworks for conversational AI applications.\\n- **Composability**: Chaining different LLM capabilities in a modular way.\\n\\nLangChain is open-source, with an active community contributing to its development.',\n",
       " 'URL: https://github.com/hwchase17/langchain/issues/2407\\n\\nSUMMARY: The text does not provide information about a \"look up tool calling in langchain.\" Instead, it indicates that a webpage retrieval failed with a status code of 404, which typically means that the requested page was not found.',\n",
       " 'URL: https://stackoverflow.com/questions/79350455/langchain-tool-calling-not-returning-any-content\\n\\nSUMMARY: The text discusses an issue with tool calling in Langchain, specifically when using the `ChatOpenAI` model. The user experienced unexpected behavior where the tool calls did not return any content in the response. They followed a tutorial to implement tools for addition and multiplication but found that the AI response was empty despite the tool calls being present. \\n\\nThe user printed the response details, which showed that the AI message had an empty content field, indicating that it did not generate a response to the question. After troubleshooting, the user discovered that the problem was related to the model they were using (`mistralai/Mixtral-8x7B-Instruct-v0.1`). They switched to a different model (`4o-mini`), which resolved the issue, and everything worked as expected.\\n\\nIn summary, the key points are:\\n- The user implemented tool calling in Langchain but received an empty response.\\n- The model used was causing the issue.\\n- Switching to a different model resolved the problem, allowing the expected behavior to occur.',\n",
       " \"URL: https://www.reddit.com/r/LangChain/comments/1huol2r/questions_on_tool_calling/\\n\\nSUMMARY: The text discusses a user seeking advice on using tool calling with LangGraph in LangChain for LLM development. The user is working with models including Llama, Qwen2.5-72B-Instruct, Qwen2.5-Coder-32B-Instruct, and QwQ-32B-Preview, noting that Qwen2.5-72B-Instruct performs best on the Berkeley Function-Calling Leaderboard but lacks native support for function/tool calling. The user is inquiring about leveraging prompting to enable tool calling and whether including tool information in the system prompt would enhance model performance, or if this information is automatically provided when using `.bind_tools()`. \\n\\nFor more details on tool calling in LangChain, it's recommended to refer to the official LangChain documentation at [langchain.com](https://www.langchain.com/).\",\n",
       " 'URL: https://python.langchain.com/docs/how_to/function_calling/\\n\\nSUMMARY: Tool calling in LangChain refers to the ability of language models to generate output that matches a user-defined schema, allowing for structured interactions with tools or functions. The tool call consists of a name, an arguments dictionary (formatted as {argument_name: argument_value}), and an optional identifier. This feature enables models to issue multiple tool calls in response to a prompt, making it possible to extract structured data from unstructured text.\\n\\nVarious LLM providers, including OpenAI, Anthropic, and others, support tool calling, albeit with different conventions for formatting tool schemas and calls. For example, OpenAI separates tool calls into a distinct parameter, while Anthropic returns them as parsed structures within a larger content block.\\n\\nLangChain offers built-in tools and supports creating custom tools, making tool calling valuable for building agents and chains that utilize tools, as well as for obtaining structured outputs from models.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "web_search_chain.invoke({\"question\": \"Look up tool calling in langchain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Write 3 google search queries to search online that form an \"\n",
    "            \"objective opinion from the following: {question}\\n\"\n",
    "            \"You must respond with a list of strings in the following format: \"\n",
    "            '[\"query 1\", \"query 2\", \"query 3\"].',\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "search_question_chain = SEARCH_PROMPT | ChatOpenAI(temperature=0) | StrOutputParser() | json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best langchain utilities for research',\n",
       " 'top langchain tools for academic research',\n",
       " 'recommended langchain resources for scholarly work']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_question_chain.invoke({\"question\": \"What are the most useful langchain utilities for research?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_research_chain = search_question_chain | (lambda x: [{\"question\": q} for q in x]) | web_search_chain.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['URL: https://www.langchain.com/\\n\\nSUMMARY: LangChain is a composable framework designed to build applications powered by large language models (LLMs). It supports developers throughout the LLM application lifecycle, allowing them to create context-aware and reasoning applications that leverage company data and APIs. LangChain products include LangGraph, which orchestrates agent-driven workflows, and LangSmith, an enterprise platform for debugging, testing, deploying, and monitoring LLM applications. LangChain products have seen over 20 million monthly downloads, with more than 100,000 apps powered and 100,000 GitHub stars. The platform is used by a large developer community to enhance operational efficiency, increase personalization, and deliver revenue-generating products.',\n",
       "  'URL: https://aws.amazon.com/what-is/langchain/\\n\\nSUMMARY: LangChain is an open-source framework designed for building applications based on large language models (LLMs). It provides tools and abstractions to enhance the customization, accuracy, and relevance of responses generated by LLMs. Key features include the ability to create prompt chains, integrate LLMs with internal data sources, and streamline the process of developing data-responsive applications like chatbots, question-answering systems, and content generators. LangChain simplifies AI development by abstracting complexities and enabling developers to connect language models with external data sources efficiently. \\n\\nCore components of LangChain include:\\n\\n1. **Chains**: Sequences of automated actions from user queries to model output.\\n2. **Links**: Individual actions within chains, such as formatting input or retrieving data.\\n3. **LLM Interface**: APIs for connecting and querying various LLMs.\\n4. **Prompt Templates**: Pre-built structures for formatting queries consistently.\\n5. **Agents**: Special chains that determine the best sequence of actions in response to a query.\\n6. **Retrieval Modules**: Tools for transforming and retrieving information to refine model responses.\\n7. **Memory**: Capabilities to recall past interactions for improved responses.\\n8. **Callbacks**: Codes for logging and monitoring specific events during operations.\\n\\nLangChain supports efficient AI application development and is backed by an active open-source community.',\n",
       "  'URL: https://python.langchain.com/docs/introduction/\\n\\nSUMMARY: LangChain is a framework designed for developing applications powered by large language models (LLMs). It simplifies the entire LLM application lifecycle, including development, productionization, and deployment. LangChain offers open-source components and integrates with numerous providers, implementing a standard interface for LLMs, embedding models, and vector stores. Key features include the ability to build stateful agents using LangGraph, monitor and evaluate applications with LangSmith, and turn applications into production-ready APIs. The framework consists of several libraries, including langchain-core, integration packages, and community-maintained components.',\n",
       "  \"URL: https://www.techtarget.com/searchenterpriseai/definition/LangChain\\n\\nSUMMARY: LangChain is an open source framework designed for software developers to combine large language models (LLMs) with external components to create applications powered by artificial intelligence (AI) and natural language processing (NLP). Released in 2022 by co-founders Harrison Chase and Ankush Gola, LangChain allows developers to link powerful LLMs like OpenAI's GPT-3.5 and GPT-4 to various data sources, facilitating the development of applications that produce humanlike responses and answer questions.\\n\\nKey benefits of LangChain include:\\n- Open source access and community support.\\n- A modular design that allows customization and ease of integration with different LLMs.\\n- Simplified development processes for creating generative AI applications.\\n- The ability to repurpose LLMs for domain-specific applications without retraining.\\n- Real-time interaction capabilities for applications like chatbots.\\n\\nLangChain comprises various modules, including model interaction, prompt templates, data connection, chains, agents, memory, and retrieval modules, all of which help streamline the creation of effective NLP applications. Additionally, it supports integration with cloud storage platforms and vector databases for enhanced data handling and retrieval.\",\n",
       "  'URL: https://www.ibm.com/think/topics/langchain\\n\\nSUMMARY: LangChain is an open-source orchestration framework designed for developing applications that utilize large language models (LLMs). It offers libraries in both Python and JavaScript, streamlining the building of LLM-driven applications such as chatbots and virtual agents. Launched by Harrison Chase in October 2022, LangChain quickly became the fastest-growing open-source project on GitHub by June 2023.\\n\\nLangChain serves as a generic interface for various LLMs, providing a centralized environment to integrate applications with external data sources and software workflows. Its modular approach allows developers to easily compare different prompts and models without extensive code rewriting. LangChain supports various use cases, including intelligent search, question-answering, summarization, and robotic process automation.\\n\\nTo work with LLMs, LangChain abstracts complex processes into manageable components, enabling users to quickly prototype and experiment with applications. It allows for easy importing of LLMs through API keys and supports both proprietary and open-source models. LangChain also features tools for prompt engineering, chaining functions, and integrating external data using document loaders and vector databases.\\n\\nOverall, LangChain simplifies the development process for LLM applications, making generative AI more accessible to developers and data scientists.',\n",
       "  'URL: https://medium.com/around-the-prompt/what-is-langchain-and-why-should-i-care-as-a-developer-b2d952c42b28\\n\\nSUMMARY: The text provided does not contain any information about Langchain. It only mentions a failed attempt to retrieve a webpage due to a status code 403, indicating forbidden access.',\n",
       "  \"URL: https://en.wikipedia.org/wiki/LangChain\\n\\nSUMMARY: LangChain is a software framework designed to facilitate the integration of large language models (LLMs) into applications. It was launched in October 2022 by Harrison Chase as an open-source project and has quickly gained popularity, leading to the establishment of a startup that raised over $20 million in funding from Sequoia Capital and Benchmark. LangChain's capabilities include use cases such as document analysis, chatbots, retrieval-augmented generation, and synthetic data generation. As of April 2023, it supports over 50 document types and integrates with various systems, including cloud storage providers and APIs for news, weather, and language models. Notable features introduced include the LangChain Expression Language (LCEL) for defining action chains and LangServe for deploying code as production-ready APIs. The latest stable release is version 0.1.16, as of April 11, 2024.\",\n",
       "  'URL: https://github.com/langchain-ai/langchain\\n\\nSUMMARY: LangChain is a framework designed for developing applications powered by large language models (LLMs). It simplifies the entire application lifecycle, offering open-source libraries and components for building applications, integrating third-party tools, and facilitating productionization and deployment. Key features include:\\n\\n- **Open-source Libraries**: Components like `langchain-core`, integration packages (e.g., `langchain-openai`), and community-maintained packages.\\n- **LangGraph**: A tool for building stateful, multi-actor applications with LLMs, allowing for robust application architecture.\\n- **LangSmith**: A developer platform for debugging, testing, and monitoring LLM applications.\\n- **Components**: Modular building blocks for tasks such as model I/O, retrieval, and agent-based decision-making.\\n- **Ecosystem**: Includes tools for tracing and evaluating language model applications.\\n\\nAs of now, LangChain has garnered 102k stars and 16.5k forks on GitHub, indicating a strong community interest and usage. The repository contains over 12,754 commits and includes a variety of documentation resources to assist developers.'],\n",
       " ['URL: https://lakefs.io/blog/what-is-langchain-ml-architecture/\\n\\nSUMMARY: The provided text does not contain any information about Langchain features and benefits. It only mentions a failed attempt to retrieve a webpage due to a status code 403, indicating a forbidden access error.',\n",
       "  'URL: https://www.trantorinc.com/blog/what-is-langchain\\n\\nSUMMARY: **LangChain Features:**\\n\\n1. **Modular Architecture**: Allows easy swapping of components (language models, data sources, processing steps) without disrupting the entire application, facilitating rapid experimentation and adaptation.\\n\\n2. **Unified Interface**: Provides a consistent interface for multiple LLMs from various providers, simplifying integration and allowing developers to focus on application building.\\n\\n3. **Memory Management**: Simplifies conversational memory management, maintaining context across interactions, which is useful for chatbots and virtual assistants.\\n\\n4. **Agent Functionality**: Introduces \"agents\" that can perform complex tasks by combining multiple LLM queries, data retrieval, and processing steps, with customization options.\\n\\n5. **Extensive Documentation and Examples**: Offers comprehensive resources that make the framework accessible to developers of all skill levels.\\n\\n---\\n\\n**Benefits of Using LangChain:**\\n\\n1. **Accelerated Development**: Reduces development time by abstracting complexities, allowing teams to focus on value-added features.\\n\\n2. **Scalability and Flexibility**: The modular design ensures applications can adapt to changing requirements and evolving language models.\\n\\n3. **Improved Productivity**: Handles integration and memory management, enabling developers to concentrate on building innovative solutions.\\n\\n4. **Enhanced User Experiences**: Facilitates the creation of sophisticated conversational AI systems, improving user interactions.\\n\\n5. **Future-Proof Investments**: The framework’s design adapts to the latest advancements in AI and LLMs, ensuring ongoing relevance and utility.',\n",
       "  'URL: https://medium.com/around-the-prompt/what-is-langchain-and-why-should-i-care-as-a-developer-b2d952c42b28\\n\\nSUMMARY: The provided text does not contain any information regarding Langchain features and benefits. It only indicates an error message about failing to retrieve a webpage with a status code of 403. Therefore, I cannot answer the question based on the text provided.',\n",
       "  'URL: https://www.ksolves.com/blog/artificial-intelligence/power-of-langchain-features-and-benefits\\n\\nSUMMARY: **LangChain Features:**\\n\\n1. **Modular Architecture**: Allows easy swapping of language models, data sources, and processing stages, facilitating quick experimentation and iterations.\\n   \\n2. **Unified Interface**: Provides a consistent interface for various language models, simplifying development and enabling seamless model switching without the need to learn each model\\'s intricacies.\\n\\n3. **Memory Management**: Enhances conversational AI by maintaining context and continuity, improving user experience in chatbots and virtual assistants.\\n\\n4. **Agentic Functionality**: Introduces \"agents\" that can perform complex tasks by integrating multiple LLM searches and processing stages, allowing for dynamic and intelligent applications.\\n\\n5. **Extensive Documentation and Examples**: Offers comprehensive resources and community support, making it accessible for developers of all experience levels.\\n\\n**Benefits of Using LangChain:**\\n\\n1. **Accelerated Development**: Reduces development time by abstracting LLM complexities, allowing teams to focus on value-added features.\\n\\n2. **Scalability and Flexibility**: Modular design ensures applications can adapt to changing requirements and evolving language models.\\n\\n3. **Improved Productivity**: Frees developers from managing LLM integration and memory, enabling a more efficient development lifecycle.\\n\\n4. **Enhanced User Experiences**: Facilitates the creation of sophisticated conversational AI systems, leading to more engaging user interactions.\\n\\n5. **Future-Proof Investments**: Its adaptability to the latest AI advancements ensures sustained relevance and utility of LangChain-powered applications.\\n\\nIn summary, LangChain is a powerful framework that streamlines LLM application development, enhancing productivity and user experience while providing flexibility and scalability.',\n",
       "  \"URL: https://www.reddit.com/r/LangChain/comments/12r5y1g/what_are_the_benefits_of_using_langchain_compared/\\n\\nSUMMARY: LangChain is an open-source framework designed to assist developers in creating applications utilizing large language models (LLMs). It is available in Python and JavaScript and can be found at [langchain.com](https://www.langchain.com/). \\n\\nBenefits of using LangChain compared to just using OpenAI's documentation include:\\n\\n- **Versatility**: LangChain offers a more comprehensive toolkit for developing LLM applications, potentially streamlining the integration of documents and improving interaction with them.\\n- **Use Cases**: While the text does not specify particular use cases where LangChain is a must, it suggests that it provides functionalities that go beyond the basic capabilities available through OpenAI’s documentation.\\n\\nOverall, LangChain aims to facilitate the transition of LLM applications from prototype to production, making it a valuable resource for developers looking to enhance their projects.\",\n",
       "  'URL: https://datasciencedojo.com/blog/what-is-langchain/\\n\\nSUMMARY: **LangChain Features:**\\n\\n1. **Modular Components:** LangChain offers a modular design, allowing developers to combine different functions like natural language processing (NLP), data retrieval, and user interaction easily. This modularity supports experimentation, enabling quick swaps between different language models without rewriting code.\\n\\n2. **Integration with External Data Sources:** The framework can connect with various data sources, including file storage services (Dropbox, Google Drive), web content (YouTube, PubMed), collaboration tools (Airtable, Trello), and databases (Pandas, MongoDB). This integration allows applications to provide context-aware, rich responses by leveraging external data.\\n\\n3. **Prompt Engineering:** LangChain supports advanced prompt engineering, helping developers craft clear and effective prompts for LLMs. It includes prompt templates for consistent formatting, improving the quality of interactions by refining how questions are posed to the models.\\n\\n**Benefits of Using LangChain:**\\n\\n- Simplifies the development and deployment of LLM-powered applications, making advanced AI capabilities more accessible to businesses.\\n- Enhances the ability to create context-aware and responsive applications, leading to improved user interactions and satisfaction.\\n- Facilitates easy experimentation and rapid iteration in AI development, crucial for optimizing application performance.\\n- Empowers businesses to build sophisticated applications that integrate real-time data, driving innovation across various sectors.',\n",
       "  'URL: https://www.techtarget.com/searchenterpriseai/definition/LangChain\\n\\nSUMMARY: ### LangChain Features and Benefits\\n\\n**Features:**\\n1. **Model Interaction (Model I/O):** Enables interaction with any language model, managing inputs and outputs.\\n2. **Prompt Templates:** Allows creation of structured prompts for LLMs, enhancing interaction and response accuracy.\\n3. **Data Connection and Retrieval:** Transforms and stores data for easy retrieval via queries.\\n4. **Chains:** Links multiple LLMs or components to build complex applications (LLM chains).\\n5. **Agents:** Orchestrates complex commands for LLMs to solve problems effectively.\\n6. **Memory:** Adds short-term and long-term memory capabilities to LLMs for contextual awareness.\\n7. **Retrieval Modules:** Supports development of retrieval-augmented generation (RAG) systems to improve model responses.\\n\\n**Benefits:**\\n1. **Open Source and Community Support:** Accessible on GitHub, allowing for community contributions and support.\\n2. **Modular Design:** Customizable components enable developers to tailor solutions to specific needs.\\n3. **Simplified Development:** Standardized interface allows easy switching between various LLMs, reducing integration complexity.\\n4. **Repurposing LLMs:** Enables domain-specific applications without retraining, enhancing responses with proprietary information.\\n5. **Interactive Applications:** Facilitates real-time communication for applications like chatbots and AI assistants.\\n\\nLangChain simplifies the development of generative AI applications by linking LLMs with external data sources, thus enhancing their capabilities and improving user interactions.',\n",
       "  'URL: https://python.langchain.com/docs/concepts/why_langchain/\\n\\nSUMMARY: **LangChain Features and Benefits:**\\n\\n1. **Standardized Component Interfaces**: LangChain provides a uniform interface for various components, allowing developers to easily switch between different AI models and services without needing to learn multiple APIs.\\n\\n2. **Orchestration**: It facilitates the efficient connection of multiple components and models, enabling the construction of complex applications that can perform diverse tasks.\\n\\n3. **Observability and Evaluation**: LangChain offers tools for monitoring and evaluating applications, helping developers understand their systems better and make informed decisions regarding prompt engineering, model selection, and balancing accuracy, latency, and cost.\\n\\n4. **Modular Ecosystem**: The LangChain ecosystem allows developers to pick and choose components that suit their specific use cases, promoting flexibility and customization in application development.\\n\\nOverall, LangChain aims to simplify the development of AI applications by addressing the challenges posed by diverse APIs, complex workflows, and the need for effective monitoring and evaluation.'],\n",
       " ['URL: https://www.reddit.com/r/LangChain/comments/18eukhc/i_just_had_the_displeasure_of_implementing/\\n\\nSUMMARY: The feedback on LangChain from a user with over a decade of engineering experience is largely negative. They describe it as \"arguably the worst library\" they\\'ve worked with, citing issues such as inconsistent abstractions, naming schemas, behavior, error management, chain life-cycle, callback handling, and unnecessary abstractions. The user indicates that LangChain tries to cater to beginner developers, which alienates more experienced coders. They strongly advise against using LangChain to preserve one\\'s sanity. There are no specific numerical ratings or statistics provided in the text.',\n",
       "  'URL: https://medium.com/llm-study-diary-a-beginners-path-through-ai/comprehensive-review-of-langchain-part-1-4734d61a49e1\\n\\nSUMMARY: The text does not provide any information about Langchain reviews and feedback. It only mentions a failure to retrieve a webpage with a status code of 403.',\n",
       "  'URL: https://news.ycombinator.com/item?id=40739982\\n\\nSUMMARY: The feedback on LangChain from various developers is largely negative, with many citing issues related to its complexity and abstraction layers. Key points include:\\n\\n1. **Complexity and Abstraction**: Developers find that LangChain introduces too many layers of abstraction, making it difficult to implement custom or original use cases. This often leads to confusion and a lack of understanding of the process.\\n\\n2. **Inflexibility**: Users report that while LangChain works well for standard use cases, it becomes cumbersome when modifications are needed, requiring significant effort to navigate its abstractions.\\n\\n3. **Basic Functionality**: Many applications using LLMs (Large Language Models) only require basic programming functionalities like string handling and API calls, which can be implemented without the overhead of a framework like LangChain.\\n\\n4. **Alternatives**: Some developers have chosen to build their own solutions or use lighter-weight libraries, appreciating the control and simplicity this offers. For instance, one developer mentioned building a tool in Go that allows for straightforward implementation without awkward abstractions.\\n\\n5. **Pragmatic Approach**: An intern shared their experience, emphasizing the importance of pragmatism and confidence in choosing not to use LangChain, which resonated with others who felt similarly.\\n\\n6. **Community Sentiment**: The overall sentiment reflects a growing skepticism about the necessity of using frameworks like LangChain, especially when simpler alternatives suffice for many use cases.\\n\\nIn summary, while LangChain aims to simplify the development of AI agents, many developers find it adds unnecessary complexity, leading them to seek alternative solutions.',\n",
       "  'URL: https://www.g2.com/products/langchain/reviews\\n\\nSUMMARY: The text provided does not contain any information about Langchain reviews or feedback. It simply indicates a failure to retrieve a webpage due to a 403 status code, which typically means access is forbidden. There are no factual details, numbers, or statistics available in the text.',\n",
       "  'URL: https://news.ycombinator.com/item?id=36645575\\n\\nSUMMARY: The feedback on Langchain is largely negative, with multiple users expressing that it is ineffective for building reusable LLM chains. A key criticism is that most of the work involves custom prompt tuning rather than reusability, with one user stating that only 5% of their effort was spent on setting up a Directed Acyclic Graph (DAG) framework, while 95% was on prompt tuning and data serialization. They argue that Langchain creates a mediocre framework that struggles with longer chains, resulting in poor output quality. \\n\\nUsers also highlighted challenges with prompt customization, noting that small changes can significantly impact results, and that the prompts often require manual tuning for each specific use case. One user mentioned that Langchain hides prompts, making it difficult to modify them. Additionally, there are concerns about the quality of default prompts provided by Langchain, with some finding their own prompts to be more effective.\\n\\nOverall, users concluded that current LLM technologies do not support the abstractions Langchain aims to provide, making it difficult to create valuable user-facing features at a production scale.',\n",
       "  'URL: https://www.g2.com/products/langchain/reviews?qs=pros-and-cons\\n\\nSUMMARY: The provided text does not contain any information about Langchain reviews and feedback. It simply states an error message indicating a failure to retrieve a webpage due to a status code 403.',\n",
       "  'URL: https://textcortex.com/post/langchain-review\\n\\nSUMMARY: The provided text does not contain any information about Langchain reviews and feedback. It simply indicates that there was a failure to retrieve a webpage due to a status code 403, which typically means access is forbidden.',\n",
       "  'URL: https://www.youtube.com/watch?v=bvMX-Zlfv68\\n\\nSUMMARY: The text does not provide specific reviews or feedback about LangChain. It simply states the title of a YouTube video that expresses a negative opinion about LangChain and a positive one about LangGraph. There are no factual details, numbers, or statistics included in the provided text.',\n",
       "  'URL: https://www.producthunt.com/products/langchain-course/reviews\\n\\nSUMMARY: The provided text does not contain any information about Langchain reviews and feedback. It simply indicates a failure to access a webpage with a status code of 403, which typically means that access is forbidden. No factual information, numbers, or stats are available in the text.']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_research_chain.invoke({\"question\": \"What is langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_SYSTEM_PROMPT = \"You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.\"  # noqa: E501\n",
    "\n",
    "\n",
    "# Report prompts from https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/master/prompts.py\n",
    "RESEARCH_REPORT_TEMPLATE = \"\"\"Information:\n",
    "--------\n",
    "{research_summary}\n",
    "--------\n",
    "Using the above information, answer the following question or topic: \"{question}\" in a detailed report -- \\\n",
    "The report should focus on the answer to the question, should be well structured, informative, \\\n",
    "in depth, with facts and numbers if available and a minimum of 1,200 words.\n",
    "You should strive to write the report as long as you can using all relevant and necessary information provided.\n",
    "You must write the report with markdown syntax.\n",
    "You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "Write all used source urls at the end of the report, and make sure to not add duplicated sources, but only one reference for each.\n",
    "You must write the report in apa format.\n",
    "Please do your best, this is very important to my career.\"\"\"  # noqa: E501\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", WRITER_SYSTEM_PROMPT),\n",
    "        (\"user\", RESEARCH_REPORT_TEMPLATE),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(\n",
    "    research_summary= full_research_chain | collapse_list_of_lists\n",
    ") | prompt | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableMap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's modify this example for something that uses the updated version of the pydantic library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5l/y8s3fc655417629rqwgxkhx80000gn/T/ipykernel_31597/2928158330.py:18: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retriever.get_relevant_documents(\"Where did harrison work?\")\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this example requires pydantic==\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "llm_chat = ChatOpenAI()\n",
    "\n",
    "vectorstore = Chroma.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retriever.get_relevant_documents(\"Where did harrison work?\")\n",
    "\n",
    "template = \"Answer the question based on this context: {context}\\n\\nQuestion: {question}\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"],\n",
    "    \n",
    "}) | prompt | llm_chat | output_parser\n",
    "\n",
    "\n",
    "\n",
    "chain.invoke({\"question\": \"What did harrison do?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect just the `RunnableMap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={}, page_content='harrison worked at kensho'),\n",
       "  Document(metadata={}, page_content='bears like to eat honey')],\n",
       " 'question': 'What did harrison do?'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "inputs_with_lambda = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"],\n",
    "})\n",
    "\n",
    "inputs_with_lambda.invoke({\"question\": \"What did harrison do?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableMap` is an alias for `RunnableParallel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={}, page_content='harrison worked at kensho'),\n",
       "  Document(metadata={}, page_content='bears like to eat honey')],\n",
       " 'question': 'What did harrison do?'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs_with_runnable_passthrough = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"],\n",
    "})\n",
    "\n",
    "inputs_with_runnable_passthrough.invoke({\"question\": \"What did harrison do?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get the same thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use tools with runnables in some neat and interesting ways. Let's see that in the next notebook (2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that what the `RunnableMap` is doing, is create the right inputs for the downstream of this chain which in this case is seeting the `context` key with a value of a list of `Document` objects, and the question key with itself.\n",
    "(which now we can probably change to something simpler with the `RunnablePassthrough` for example)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}