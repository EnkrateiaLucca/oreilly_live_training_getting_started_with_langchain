{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Engineering in LangChain & LangGraph\n",
    "\n",
    "Context engineering is about building **dynamic systems** that provide the right information and tools in the right format so that an LLM can accomplish its task. Unlike prompt engineering (static, handcrafted strings), context engineering encompasses dynamic context construction using memory, state, and retrieval.\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "1. **Static Context** - Immutable data that doesn't change during execution\n",
    "2. **Dynamic Context** - Mutable data that evolves as the application runs\n",
    "3. **Runtime Context** - Data scoped to a single run or invocation\n",
    "4. **Cross-Conversation Context** - Data that persists across multiple sessions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages and configure our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-community langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Static Context\n",
    "\n",
    "**Static context** is immutable data that doesn't change during execution. Examples include:\n",
    "\n",
    "- User metadata (name, preferences set at startup)\n",
    "- Database connections\n",
    "- Available tools\n",
    "- Configuration settings\n",
    "- System prompts\n",
    "\n",
    "In LangChain, static context is passed to an agent at the start of a run via the `context` argument or embedded in the system prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Static Context via System Prompts\n",
    "\n",
    "The simplest form of static context - information baked into the system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Static context embedded in system prompt\n",
    "STATIC_SYSTEM_PROMPT = \"\"\"You are a customer support agent for TechCorp.\n",
    "\n",
    "Company Information (STATIC CONTEXT):\n",
    "- Company: TechCorp Inc.\n",
    "- Support Hours: Mon-Fri 9am-5pm EST\n",
    "- Refund Policy: 30-day money-back guarantee\n",
    "- Current Promotion: 20% off with code SAVE20\n",
    "\n",
    "Use this information to help customers with their inquiries.\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=STATIC_SYSTEM_PROMPT),\n",
    "    HumanMessage(content=\"What's your refund policy?\")\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.2 Static Context via Closure (Module-Level Config)\n\nA simple pattern: define configuration once, and tools/functions access it via closure:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Static configuration - defined once, never changes during execution\nPRODUCT_CATALOG = {\n    \"Widget Pro\": {\"price\": 99.99, \"in_stock\": True},\n    \"Gadget Plus\": {\"price\": 149.99, \"in_stock\": True},\n    \"Device Max\": {\"price\": 299.99, \"in_stock\": False},\n}\n\nMAX_DISCOUNT_PERCENT = 25\nSUPPORT_EMAIL = \"support@techcorp.com\"\n\nfrom langchain_core.tools import tool\n\n@tool\ndef check_product(product_name: str) -> str:\n    \"\"\"Check if a product exists and its availability.\"\"\"\n    # Tools access static context via closure\n    if product_name in PRODUCT_CATALOG:\n        product = PRODUCT_CATALOG[product_name]\n        status = \"in stock\" if product[\"in_stock\"] else \"out of stock\"\n        return f\"{product_name}: ${product['price']} ({status})\"\n    return f\"Product '{product_name}' not found. Available: {list(PRODUCT_CATALOG.keys())}\"\n\n@tool\ndef calculate_price(product_name: str, discount_percent: int = 0) -> str:\n    \"\"\"Calculate final price with optional discount.\"\"\"\n    if product_name not in PRODUCT_CATALOG:\n        return f\"Product '{product_name}' not found.\"\n\n    base_price = PRODUCT_CATALOG[product_name][\"price\"]\n    # Static context (MAX_DISCOUNT_PERCENT) limits the discount\n    actual_discount = min(discount_percent, MAX_DISCOUNT_PERCENT)\n    final_price = base_price * (1 - actual_discount / 100)\n\n    result = f\"{product_name}: ${final_price:.2f}\"\n    if discount_percent > MAX_DISCOUNT_PERCENT:\n        result += f\" (discount capped at {MAX_DISCOUNT_PERCENT}%)\"\n    return result\n\n# Test the tools - they use static context\nprint(check_product.invoke({\"product_name\": \"Widget Pro\"}))\nprint(check_product.invoke({\"product_name\": \"Unknown Product\"}))\nprint(calculate_price.invoke({\"product_name\": \"Widget Pro\", \"discount_percent\": 50}))"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 1.3 Static Context with ReAct Agent\n\nLet's build a simple agent that uses these tools with static context:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from langgraph.prebuilt import create_react_agent\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n\n# Create a ReAct agent with our tools\n# The tools have access to static context (PRODUCT_CATALOG, MAX_DISCOUNT_PERCENT)\nshop_assistant = create_react_agent(\n    model=model,\n    tools=[check_product, calculate_price],\n    prompt=\"You are a helpful shop assistant. Use tools to help customers with product info and pricing.\"\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# The agent uses tools that access static context\nresult = shop_assistant.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"What's the price of Widget Pro with a 20% discount?\"}]\n})\n\nfor msg in result[\"messages\"]:\n    if hasattr(msg, 'pretty_print'):\n        msg.pretty_print()\n    else:\n        print(f\"{msg['role']}: {msg['content']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Another query - notice the static context (product catalog) is always available\nresult = shop_assistant.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Is Device Max available? And what products do you have?\"}]\n})\n\nprint(result[\"messages\"][-1].content)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Dynamic Context\n",
    "\n",
    "**Dynamic context** is mutable data that evolves as the application runs. Examples include:\n",
    "\n",
    "- Conversation history (messages accumulating over time)\n",
    "- Intermediate results from tool calls\n",
    "- Scratchpad notes the agent writes to itself\n",
    "- Computed values that change based on user actions\n",
    "\n",
    "In LangGraph, dynamic context is managed through the **state object**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Message History as Dynamic Context\n",
    "\n",
    "The most common form of dynamic context - the conversation grows with each interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Dynamic context: message history that grows over time\n",
    "conversation_history = [\n",
    "    SystemMessage(content=\"You are a helpful math tutor. Be concise.\")\n",
    "]\n",
    "\n",
    "def chat(user_input: str):\n",
    "    \"\"\"Add user message, get response, update history.\"\"\"\n",
    "    # Add user message to dynamic context\n",
    "    conversation_history.append(HumanMessage(content=user_input))\n",
    "    \n",
    "    # Model sees full context\n",
    "    response = model.invoke(conversation_history)\n",
    "    \n",
    "    # Add AI response to dynamic context\n",
    "    conversation_history.append(response)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "# Watch the context grow dynamically\n",
    "print(\"Turn 1:\", chat(\"What is 5 + 3?\"))\n",
    "print(f\"\\nContext size: {len(conversation_history)} messages\")\n",
    "\n",
    "print(\"\\nTurn 2:\", chat(\"Multiply that by 2\"))\n",
    "print(f\"\\nContext size: {len(conversation_history)} messages\")\n",
    "\n",
    "print(\"\\nTurn 3:\", chat(\"What was my first question?\"))\n",
    "print(f\"\\nContext size: {len(conversation_history)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Scratchpad: Agent's Working Memory\n",
    "\n",
    "A scratchpad allows the agent to write notes to itself during execution - dynamic context that evolves as the agent works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "class ScratchpadState(TypedDict):\n",
    "    \"\"\"State with a scratchpad for dynamic notes.\"\"\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "    scratchpad: List[str]  # Dynamic context: notes the agent writes\n",
    "    step_count: int        # Dynamic context: tracks progress\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "def analyze_and_note(state: ScratchpadState):\n",
    "    \"\"\"Agent analyzes input and writes to scratchpad.\"\"\"\n",
    "    user_msg = state[\"messages\"][-1].content\n",
    "    current_notes = state.get(\"scratchpad\", [])\n",
    "    step = state.get(\"step_count\", 0) + 1\n",
    "    \n",
    "    # Build prompt with current scratchpad context\n",
    "    scratchpad_context = \"\\n\".join(current_notes) if current_notes else \"(empty)\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze this request and write a brief note about what you learned.\n",
    "\n",
    "Current scratchpad:\n",
    "{scratchpad_context}\n",
    "\n",
    "User request: {user_msg}\n",
    "\n",
    "Respond with:\n",
    "1. A note to add to your scratchpad (start with \"NOTE:\")\n",
    "2. Your response to the user (start with \"RESPONSE:\")\"\"\"\n",
    "    \n",
    "    response = model.invoke([HumanMessage(content=prompt)])\n",
    "    content = response.content\n",
    "    \n",
    "    # Extract note and response\n",
    "    note = \"\"\n",
    "    reply = content\n",
    "    if \"NOTE:\" in content and \"RESPONSE:\" in content:\n",
    "        parts = content.split(\"RESPONSE:\")\n",
    "        note = parts[0].replace(\"NOTE:\", \"\").strip()\n",
    "        reply = parts[1].strip()\n",
    "    \n",
    "    # Update dynamic context\n",
    "    new_notes = current_notes + [f\"Step {step}: {note}\"] if note else current_notes\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"scratchpad\": new_notes,\n",
    "        \"step_count\": step\n",
    "    }\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(ScratchpadState)\n",
    "builder.add_node(\"analyze\", analyze_and_note)\n",
    "builder.add_edge(START, \"analyze\")\n",
    "builder.add_edge(\"analyze\", END)\n",
    "\n",
    "scratchpad_graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple interactions, watching scratchpad grow\n",
    "state = {\n",
    "    \"messages\": [HumanMessage(content=\"I need help planning a trip to Japan\")],\n",
    "    \"scratchpad\": [],\n",
    "    \"step_count\": 0\n",
    "}\n",
    "\n",
    "result = scratchpad_graph.invoke(state)\n",
    "print(\"After turn 1:\")\n",
    "print(f\"Scratchpad: {result['scratchpad']}\")\n",
    "print(f\"Step count: {result['step_count']}\")\n",
    "\n",
    "# Continue conversation\n",
    "result[\"messages\"].append(HumanMessage(content=\"I'm interested in traditional culture\"))\n",
    "result = scratchpad_graph.invoke(result)\n",
    "print(\"\\nAfter turn 2:\")\n",
    "print(f\"Scratchpad: {result['scratchpad']}\")\n",
    "print(f\"Step count: {result['step_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Tool Results as Dynamic Context\n",
    "\n",
    "When agents call tools, the results become part of the dynamic context that influences subsequent decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langgraph.prebuilt import create_react_agent\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\n\n# Dynamic context: a shopping cart that changes based on tool calls\nshopping_cart = []\n\n@tool\ndef add_to_cart(item: str, quantity: int = 1) -> str:\n    \"\"\"Add an item to the shopping cart.\"\"\"\n    for _ in range(quantity):\n        shopping_cart.append(item)\n    return f\"Added {quantity}x {item} to cart. Cart now has {len(shopping_cart)} items.\"\n\n@tool\ndef view_cart() -> str:\n    \"\"\"View the current shopping cart contents.\"\"\"\n    if not shopping_cart:\n        return \"Cart is empty.\"\n    from collections import Counter\n    counts = Counter(shopping_cart)\n    items = [f\"{item}: {count}\" for item, count in counts.items()]\n    return f\"Cart contents: {', '.join(items)}\"\n\n@tool\ndef clear_cart() -> str:\n    \"\"\"Clear all items from the cart.\"\"\"\n    shopping_cart.clear()\n    return \"Cart cleared!\"\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n\nshop_agent = create_react_agent(\n    model=model,\n    tools=[add_to_cart, view_cart, clear_cart],\n    prompt=\"You are a shopping assistant. Help users manage their cart using the available tools.\"\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Watch the dynamic context (cart) evolve\nclear_cart.invoke({})  # Start fresh\n\nprint(\"=== Shopping Session ===\")\nfor request in [\n    \"Add 2 apples to my cart\",\n    \"Also add a banana\",\n    \"What's in my cart?\"\n]:\n    print(f\"\\nUser: {request}\")\n    result = shop_agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": request}]})\n    print(f\"Agent: {result['messages'][-1].content}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Runtime Context\n",
    "\n",
    "**Runtime context** is data scoped to a single run or invocation. It exists only for the duration of one agent execution and is discarded afterward.\n",
    "\n",
    "Key characteristics:\n",
    "- Isolated to a single `invoke()` or `stream()` call\n",
    "- Not persisted between runs\n",
    "- Often includes request-specific data like user input, session tokens, or temporary calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Configurable Runtime Parameters\n",
    "\n",
    "Use `RunnableConfig` to pass runtime-specific parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated\n",
    "from langchain_core.tools import tool, InjectedToolArg\n",
    "\n",
    "@tool\n",
    "def get_personalized_greeting(\n",
    "    time_of_day: str,\n",
    "    config: Annotated[RunnableConfig, InjectedToolArg],\n",
    ") -> str:\n",
    "    \"\"\"Generate a personalized greeting based on runtime context.\"\"\"\n",
    "    # Access runtime-specific configuration\n",
    "    user_id = config.get(\"configurable\", {}).get(\"user_id\", \"guest\")\n",
    "    language = config.get(\"configurable\", {}).get(\"language\", \"en\")\n",
    "    \n",
    "    greetings = {\n",
    "        \"en\": {\"morning\": \"Good morning\", \"afternoon\": \"Good afternoon\", \"evening\": \"Good evening\"},\n",
    "        \"es\": {\"morning\": \"Buenos días\", \"afternoon\": \"Buenas tardes\", \"evening\": \"Buenas noches\"},\n",
    "        \"ja\": {\"morning\": \"おはようございます\", \"afternoon\": \"こんにちは\", \"evening\": \"こんばんは\"},\n",
    "    }\n",
    "    \n",
    "    greeting = greetings.get(language, greetings[\"en\"]).get(time_of_day, \"Hello\")\n",
    "    return f\"{greeting}, User {user_id}!\"\n",
    "\n",
    "# Test with different runtime contexts\n",
    "print(\"Runtime context 1 (English):\")\n",
    "print(get_personalized_greeting.invoke(\n",
    "    {\"time_of_day\": \"morning\"},\n",
    "    config={\"configurable\": {\"user_id\": \"123\", \"language\": \"en\"}}\n",
    "))\n",
    "\n",
    "print(\"\\nRuntime context 2 (Japanese):\")\n",
    "print(get_personalized_greeting.invoke(\n",
    "    {\"time_of_day\": \"morning\"},\n",
    "    config={\"configurable\": {\"user_id\": \"456\", \"language\": \"ja\"}}\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Runtime State in LangGraph\n",
    "\n",
    "LangGraph's state object acts as runtime context - it exists only during graph execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Optional\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class CalculationState(TypedDict):\n",
    "    \"\"\"Runtime state - exists only during this execution.\"\"\"\n",
    "    numbers: list[float]\n",
    "    operation: str\n",
    "    intermediate_result: Optional[float]\n",
    "    final_result: Optional[float]\n",
    "    error: Optional[str]\n",
    "\n",
    "def validate_input(state: CalculationState):\n",
    "    \"\"\"Validate input and set error if needed.\"\"\"\n",
    "    if not state[\"numbers\"]:\n",
    "        return {\"error\": \"No numbers provided\"}\n",
    "    if state[\"operation\"] not in [\"sum\", \"product\", \"average\"]:\n",
    "        return {\"error\": f\"Unknown operation: {state['operation']}\"}\n",
    "    return {\"error\": None}\n",
    "\n",
    "def calculate(state: CalculationState):\n",
    "    \"\"\"Perform calculation and store intermediate result.\"\"\"\n",
    "    if state.get(\"error\"):\n",
    "        return {}  # Skip if validation failed\n",
    "    \n",
    "    numbers = state[\"numbers\"]\n",
    "    op = state[\"operation\"]\n",
    "    \n",
    "    if op == \"sum\":\n",
    "        result = sum(numbers)\n",
    "    elif op == \"product\":\n",
    "        result = 1\n",
    "        for n in numbers:\n",
    "            result *= n\n",
    "    elif op == \"average\":\n",
    "        result = sum(numbers) / len(numbers)\n",
    "    \n",
    "    return {\"intermediate_result\": result}\n",
    "\n",
    "def finalize(state: CalculationState):\n",
    "    \"\"\"Round and finalize the result.\"\"\"\n",
    "    if state.get(\"error\"):\n",
    "        return {\"final_result\": None}\n",
    "    return {\"final_result\": round(state[\"intermediate_result\"], 2)}\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(CalculationState)\n",
    "builder.add_node(\"validate\", validate_input)\n",
    "builder.add_node(\"calculate\", calculate)\n",
    "builder.add_node(\"finalize\", finalize)\n",
    "\n",
    "builder.add_edge(START, \"validate\")\n",
    "builder.add_edge(\"validate\", \"calculate\")\n",
    "builder.add_edge(\"calculate\", \"finalize\")\n",
    "builder.add_edge(\"finalize\", END)\n",
    "\n",
    "calc_graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each invocation has its own isolated runtime context\n",
    "result1 = calc_graph.invoke({\n",
    "    \"numbers\": [1, 2, 3, 4, 5],\n",
    "    \"operation\": \"average\",\n",
    "    \"intermediate_result\": None,\n",
    "    \"final_result\": None,\n",
    "    \"error\": None\n",
    "})\n",
    "print(f\"Run 1 - Average of [1,2,3,4,5]: {result1['final_result']}\")\n",
    "\n",
    "result2 = calc_graph.invoke({\n",
    "    \"numbers\": [2, 3, 4],\n",
    "    \"operation\": \"product\",\n",
    "    \"intermediate_result\": None,\n",
    "    \"final_result\": None,\n",
    "    \"error\": None\n",
    "})\n",
    "print(f\"Run 2 - Product of [2,3,4]: {result2['final_result']}\")\n",
    "\n",
    "# Each run is completely isolated - no state leaks between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Thread-Scoped Runtime Context with Checkpointing\n",
    "\n",
    "Using `MemorySaver`, you can maintain runtime context within a single \"thread\" or session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "def chat_node(state: ChatState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "builder = StateGraph(ChatState)\n",
    "builder.add_node(\"chat\", chat_node)\n",
    "builder.add_edge(START, \"chat\")\n",
    "builder.add_edge(\"chat\", END)\n",
    "\n",
    "# Add checkpointer for runtime persistence within a thread\n",
    "checkpointer = MemorySaver()\n",
    "chat_graph = builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread 1: Maintain context within this runtime session\n",
    "config1 = {\"configurable\": {\"thread_id\": \"session_abc\"}}\n",
    "\n",
    "result = chat_graph.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is Alice. Remember this!\"}]},\n",
    "    config1\n",
    ")\n",
    "print(\"Thread 1, Turn 1:\", result[\"messages\"][-1].content)\n",
    "\n",
    "# Continue same thread - context persists\n",
    "result = chat_graph.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n",
    "    config1\n",
    ")\n",
    "print(\"Thread 1, Turn 2:\", result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread 2: Completely separate runtime context\n",
    "config2 = {\"configurable\": {\"thread_id\": \"session_xyz\"}}\n",
    "\n",
    "result = chat_graph.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n",
    "    config2\n",
    ")\n",
    "print(\"Thread 2 (new session):\", result[\"messages\"][-1].content)\n",
    "# This thread doesn't know Alice - runtime contexts are isolated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Cross-Conversation Context\n",
    "\n",
    "**Cross-conversation context** persists across multiple conversations or sessions. This is long-term memory that survives beyond a single thread.\n",
    "\n",
    "Examples:\n",
    "- User preferences learned over time\n",
    "- Historical interactions\n",
    "- Facts about the user (name, location, interests)\n",
    "- Accumulated knowledge from past sessions\n",
    "\n",
    "In LangGraph, this is managed through the **InMemoryStore** or persistent storage backends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 InMemoryStore Basics\n",
    "\n",
    "The `InMemoryStore` provides namespace-based storage for long-term memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "import uuid\n",
    "\n",
    "# Create the store\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Store information in a user-specific namespace\n",
    "user_id = \"user_123\"\n",
    "namespace = (user_id, \"preferences\")  # Tuple creates hierarchical namespace\n",
    "\n",
    "# Store some preferences\n",
    "store.put(namespace, \"food\", {\"preference\": \"vegetarian\", \"favorite_cuisine\": \"Italian\"})\n",
    "store.put(namespace, \"communication\", {\"preferred_time\": \"morning\", \"style\": \"formal\"})\n",
    "\n",
    "# Retrieve all items in namespace\n",
    "items = store.search(namespace)\n",
    "print(\"Stored preferences:\")\n",
    "for item in items:\n",
    "    print(f\"  {item.key}: {item.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve specific item\n",
    "food_pref = store.get(namespace, \"food\")\n",
    "print(f\"Food preference: {food_pref.value}\")\n",
    "\n",
    "# Update an item (upsert)\n",
    "store.put(namespace, \"food\", {\"preference\": \"vegan\", \"favorite_cuisine\": \"Japanese\"})\n",
    "updated = store.get(namespace, \"food\")\n",
    "print(f\"Updated food preference: {updated.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Cross-Conversation Agent with Long-Term Memory\n",
    "\n",
    "Let's build an agent that remembers facts about users across different conversation threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.store.base import BaseStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing import TypedDict, Annotated\n",
    "import json\n",
    "\n",
    "class MemoryState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "def chat_with_memory(state: MemoryState, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"Chat node that reads and writes to long-term memory.\"\"\"\n",
    "    user_id = config[\"configurable\"].get(\"user_id\", \"anonymous\")\n",
    "    namespace = (user_id, \"facts\")\n",
    "    \n",
    "    # Retrieve existing memories\n",
    "    memories = store.search(namespace)\n",
    "    memory_text = \"\\n\".join([f\"- {m.key}: {m.value}\" for m in memories])\n",
    "    \n",
    "    system_prompt = f\"\"\"You are a helpful assistant with long-term memory.\n",
    "    \n",
    "Known facts about this user:\n",
    "{memory_text if memory_text else \"(No facts stored yet)\"}\n",
    "\n",
    "IMPORTANT: When the user shares new personal information (name, preferences, interests, etc.),\n",
    "extract it and include it in your response in this format:\n",
    "[REMEMBER: key=value]\n",
    "\n",
    "For example: [REMEMBER: name=Alice] or [REMEMBER: favorite_color=blue]\"\"\"\n",
    "    \n",
    "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Parse and store any new facts\n",
    "    content = response.content\n",
    "    import re\n",
    "    facts = re.findall(r'\\[REMEMBER: (\\w+)=([^\\]]+)\\]', content)\n",
    "    for key, value in facts:\n",
    "        store.put(namespace, key, value)\n",
    "        content = content.replace(f\"[REMEMBER: {key}={value}]\", \"\")\n",
    "    \n",
    "    response.content = content.strip()\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Build the graph with both checkpointer and store\n",
    "builder = StateGraph(MemoryState)\n",
    "builder.add_node(\"chat\", chat_with_memory)\n",
    "builder.add_edge(START, \"chat\")\n",
    "builder.add_edge(\"chat\", END)\n",
    "\n",
    "checkpointer = MemorySaver()  # For runtime/thread context\n",
    "memory_store = InMemoryStore()  # For cross-conversation context\n",
    "\n",
    "memory_agent = builder.compile(checkpointer=checkpointer, store=memory_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation 1: User shares information\n",
    "config = {\"configurable\": {\"thread_id\": \"conv_1\", \"user_id\": \"user_alice\"}}\n",
    "\n",
    "result = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Alice and I love hiking.\"}]},\n",
    "    config\n",
    ")\n",
    "print(\"Conversation 1:\")\n",
    "print(f\"User: Hi! My name is Alice and I love hiking.\")\n",
    "print(f\"Agent: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what was stored\n",
    "stored_facts = memory_store.search((\"user_alice\", \"facts\"))\n",
    "print(\"\\nStored cross-conversation facts:\")\n",
    "for fact in stored_facts:\n",
    "    print(f\"  {fact.key}: {fact.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation 2: NEW thread, but same user - facts persist!\n",
    "config = {\"configurable\": {\"thread_id\": \"conv_2\", \"user_id\": \"user_alice\"}}\n",
    "\n",
    "result = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What do you remember about me?\"}]},\n",
    "    config\n",
    ")\n",
    "print(\"Conversation 2 (NEW thread, same user):\")\n",
    "print(f\"User: What do you remember about me?\")\n",
    "print(f\"Agent: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation 3: Different user - no cross-contamination\n",
    "config = {\"configurable\": {\"thread_id\": \"conv_3\", \"user_id\": \"user_bob\"}}\n",
    "\n",
    "result = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What do you know about me?\"}]},\n",
    "    config\n",
    ")\n",
    "print(\"Conversation 3 (DIFFERENT user - Bob):\")\n",
    "print(f\"User: What do you know about me?\")\n",
    "print(f\"Agent: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4.3 Simplified Long-Term Memory Pattern\n\nHere's a simpler pattern using module-level storage that persists across function calls:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langgraph.prebuilt import create_react_agent\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\n\n# Simple in-memory user database (cross-conversation persistence)\nUSER_MEMORIES = {}\n\n@tool\ndef remember_fact(user_id: str, fact_type: str, fact_value: str) -> str:\n    \"\"\"Store a fact about a user for future conversations.\n    \n    Args:\n        user_id: The user's identifier\n        fact_type: Category of the fact (e.g., 'name', 'preference', 'hobby')\n        fact_value: The actual fact to remember\n    \"\"\"\n    if user_id not in USER_MEMORIES:\n        USER_MEMORIES[user_id] = {}\n    USER_MEMORIES[user_id][fact_type] = fact_value\n    return f\"Remembered: {user_id}'s {fact_type} is '{fact_value}'\"\n\n@tool\ndef recall_facts(user_id: str) -> str:\n    \"\"\"Recall all stored facts about a user.\n    \n    Args:\n        user_id: The user's identifier\n    \"\"\"\n    if user_id not in USER_MEMORIES or not USER_MEMORIES[user_id]:\n        return f\"No memories stored for user '{user_id}'\"\n    \n    facts = [f\"- {k}: {v}\" for k, v in USER_MEMORIES[user_id].items()]\n    return f\"Memories for {user_id}:\\n\" + \"\\n\".join(facts)\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n\nmemory_agent = create_react_agent(\n    model=model,\n    tools=[remember_fact, recall_facts],\n    prompt=\"\"\"You are a helpful assistant with long-term memory.\n\nWhen users share personal information, use remember_fact to store it.\nWhen users ask what you know about them, use recall_facts.\nAlways use the user_id provided in the conversation.\"\"\"\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Session 1: User shares information\nprint(\"=== Session 1: Charlie introduces himself ===\")\nresult = memory_agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hi! I'm user charlie_123. My name is Charlie, I'm a software engineer, and I love pizza.\"}]\n})\nprint(f\"Agent: {result['messages'][-1].content}\")\n\n# Check what was stored\nprint(f\"\\nStored memories: {USER_MEMORIES}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Session 2: Completely new conversation - but memories persist!\nprint(\"=== Session 2: New conversation, same user ===\")\nresult = memory_agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"I'm charlie_123. What do you remember about me?\"}]\n})\nprint(f\"Agent: {result['messages'][-1].content}\")\n\n# Session 3: Different user - isolated memories\nprint(\"\\n=== Session 3: Different user (bob_456) ===\")\nresult = memory_agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"I'm bob_456. What do you know about me?\"}]\n})\nprint(f\"Agent: {result['messages'][-1].content}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary: Context Types Comparison\n",
    "\n",
    "| Context Type | Mutability | Lifetime | LangChain Mechanism | Example |\n",
    "|-------------|------------|----------|--------------------|---------|\n",
    "| **Static** | Immutable | Application lifetime | System prompts, `context` arg | User tier, API keys, tools |\n",
    "| **Dynamic** | Mutable | Within a run | StateGraph state | Message history, scratchpad |\n",
    "| **Runtime** | Varies | Single invocation | `RunnableConfig`, thread_id | Request-specific params |\n",
    "| **Cross-Conversation** | Persistent | Multiple sessions | `InMemoryStore` | User preferences, facts |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Think of context as RAM** - Deliberately curate what enters the LLM's context window at each step\n",
    "\n",
    "2. **Four strategies for context management**:\n",
    "   - **Write**: Save context outside the window (scratchpad, store)\n",
    "   - **Select**: Pull relevant context in (search, filtering)\n",
    "   - **Compress**: Summarize to reduce tokens\n",
    "   - **Isolate**: Separate contexts across agents/fields\n",
    "\n",
    "3. **Choose the right tool**:\n",
    "   - `MemorySaver` for thread/session persistence (runtime context)\n",
    "   - `InMemoryStore` for cross-conversation persistence (long-term memory)\n",
    "   - State fields for dynamic in-run context\n",
    "   - System prompts/dataclasses for static context\n",
    "\n",
    "---\n",
    "\n",
    "**Resources:**\n",
    "- [LangChain Context Documentation](https://docs.langchain.com/oss/python/concepts/context)\n",
    "- [Context Engineering Blog Post](https://blog.langchain.com/context-engineering-for-agents/)\n",
    "- [LangGraph Memory Guide](https://docs.langchain.com/oss/python/langgraph/memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}