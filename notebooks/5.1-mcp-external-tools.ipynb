{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Context Protocol (MCP) with LangChain 1.0\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EnkrateiaLucca/oreilly_live_training_getting_started_with_langchain/blob/main/notebooks/5.1-mcp-external-tools.ipynb)\n",
    "\n",
    "**Note**: MCP may not work in Google Colab due to subprocess and stdio transport requirements. For best results, run this notebook locally.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook introduces the **Model Context Protocol (MCP)**, an open standard that revolutionizes how AI applications interact with external tools and data sources.\n",
    "\n",
    "### What is MCP?\n",
    "\n",
    "Model Context Protocol (MCP) is an open standard created by Anthropic (and now donated to the Agentic AI Foundation under the Linux Foundation in December 2025) that:\n",
    "\n",
    "- **Standardizes** how AI applications connect to external tools and data sources\n",
    "- **Simplifies** integration by providing a universal interface (think \"USB-C for AI\")\n",
    "- **Enables reusability** - write an MCP server once, use it across any MCP-compatible application\n",
    "- **Supports ecosystem growth** - hundreds of pre-built MCP servers are available for popular services\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "1. **No more custom integrations** - Use pre-built MCP servers instead of writing custom code for each API\n",
    "2. **Multi-server management** - Connect to multiple MCP servers simultaneously\n",
    "3. **Framework adoption** - Supported by LangChain, CrewAI, Pydantic.AI, and more\n",
    "4. **Industry backing** - Adopted by OpenAI, Anthropic, Google, Microsoft, AWS, and others\n",
    "\n",
    "### When to Use MCP?\n",
    "\n",
    "**Use MCP when:**\n",
    "- You need to integrate with services that have existing MCP servers (GitHub, Slack, databases, filesystems, etc.)\n",
    "- You want to reuse tools across multiple projects or frameworks\n",
    "- You're building production applications that need standardized tool interfaces\n",
    "\n",
    "**Use custom tools when:**\n",
    "- You have simple, project-specific tool requirements\n",
    "- You're prototyping and need quick iteration\n",
    "- No suitable MCP server exists for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /// script\n",
    "# requires-python = \">=3.12\"\n",
    "# dependencies = [\n",
    "#     \"langchain>=1.0.0\",\n",
    "#     \"langchain-core>=1.0.0\",\n",
    "#     \"langchain-openai\",\n",
    "#     \"langchain-mcp-adapters>=0.1.0\",\n",
    "#     \"langgraph>=1.0.0\",\n",
    "# ]\n",
    "# ///\n",
    "\n",
    "# Install required packages\n",
    "# %pip install -qU langchain>=1.0.0\n",
    "# %pip install -qU langchain-core>=1.0.0\n",
    "# %pip install -qU langchain-openai\n",
    "# %pip install -qU langchain-mcp-adapters>=0.1.0\n",
    "# %pip install -qU langgraph>=1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Key Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Optional: Enable LangSmith tracing\n",
    "# _set_env(\"LANGCHAIN_API_KEY\")\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding MCP Architecture\n",
    "\n",
    "MCP follows a client-server architecture:\n",
    "\n",
    "```\n",
    "┌─────────────────┐\n",
    "│  Your LangChain │\n",
    "│   Application   │\n",
    "└────────┬────────┘\n",
    "         │\n",
    "         │ uses\n",
    "         ↓\n",
    "┌─────────────────┐\n",
    "│  MCP Client     │ ← MultiServerMCPClient\n",
    "│  (langchain-mcp)│\n",
    "└────────┬────────┘\n",
    "         │\n",
    "         │ connects to (stdio/HTTP)\n",
    "         │\n",
    "    ┌────┴────┬─────────┬──────────┐\n",
    "    ↓         ↓         ↓          ↓\n",
    "┌────────┐ ┌────────┐ ┌─────┐ ┌──────┐\n",
    "│  File  │ │  Time  │ │ Git │ │Slack │\n",
    "│ System │ │ Server │ │ MCP │ │ MCP  │\n",
    "│  MCP   │ │  MCP   │ │     │ │      │\n",
    "└────────┘ └────────┘ └─────┘ └──────┘\n",
    "   MCP Servers (provide tools)\n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "- **MCP Client** - Connects to multiple MCP servers and loads their tools\n",
    "- **MCP Servers** - Provide tools, resources, and prompts through standardized protocol\n",
    "- **Transport** - Communication method (stdio for local processes, HTTP for remote servers)\n",
    "- **Tools** - Functions that can be called by LLMs (automatically converted to LangChain format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic MCP Client Setup\n",
    "\n",
    "Let's start with a conceptual example showing how to configure an MCP client. This demonstrates the pattern even if MCP servers aren't available in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCEPTUAL EXAMPLE - Shows the MCP client configuration pattern\n",
    "\n",
    "# This is how you would configure MultiServerMCPClient with MCP servers:\n",
    "\n",
    "mcp_config_example = \"\"\"\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Configure MCP client with stdio transport (for local processes)\n",
    "client = MultiServerMCPClient(\n",
    "    servers={\n",
    "        \"filesystem\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/tmp\"],\n",
    "        },\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\"-y\", \"@modelcontextprotocol/server-time\"],\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "# MCP requires async context\n",
    "async with client:\n",
    "    # Get available tools from all connected servers\n",
    "    tools = await client.get_tools()\n",
    "    print(f\"Loaded {len(tools)} tools from MCP servers\")\n",
    "    \n",
    "    # Each tool includes:\n",
    "    # - name: The tool identifier\n",
    "    # - description: What the tool does\n",
    "    # - input_schema: Parameters the tool accepts\n",
    "    \n",
    "    for tool in tools:\n",
    "        print(f\"- {tool.name}: {tool.description}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"MCP Client Configuration Pattern:\")\n",
    "print(mcp_config_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Configuration Elements\n",
    "\n",
    "1. **Transport Type**: How to communicate with the MCP server\n",
    "   - `stdio` - For local processes (most common)\n",
    "   - `http` - For remote servers (streamable HTTP in protocol 2025-03-26+)\n",
    "\n",
    "2. **Command & Args**: How to start the MCP server process\n",
    "   - Often uses `npx` to run Node.js-based MCP servers\n",
    "   - Can use Python, Go, or any executable that implements MCP protocol\n",
    "\n",
    "3. **Server Naming**: Choose descriptive names for each server (e.g., \"filesystem\", \"time\")\n",
    "   - Helps identify which server provides which tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Inspecting MCP Tool Schemas\n",
    "\n",
    "MCP tools come with rich schemas that LangChain can automatically convert to its tool format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCEPTUAL EXAMPLE - Demonstrates tool inspection pattern\n",
    "\n",
    "tool_inspection_example = \"\"\"\n",
    "async with client:\n",
    "    tools = await client.get_tools()\n",
    "    \n",
    "    # Inspect the first tool\n",
    "    if tools:\n",
    "        tool = tools[0]\n",
    "        print(f\"Tool Name: {tool.name}\")\n",
    "        print(f\"Description: {tool.description}\")\n",
    "        print(f\"Input Schema: {tool.input_schema}\")\n",
    "        \n",
    "        # Example output for filesystem 'read_file' tool:\n",
    "        # Tool Name: read_file\n",
    "        # Description: Read the complete contents of a file from the file system\n",
    "        # Input Schema: {\n",
    "        #     \"type\": \"object\",\n",
    "        #     \"properties\": {\n",
    "        #         \"path\": {\"type\": \"string\", \"description\": \"Path to file\"}\n",
    "        #     },\n",
    "        #     \"required\": [\"path\"]\n",
    "        # }\n",
    "        \n",
    "    # Get tools from a specific server\n",
    "    filesystem_tools = [t for t in tools if 'filesystem' in t.name or 'file' in t.name]\n",
    "    print(f\"\\nFilesystem-related tools: {len(filesystem_tools)}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Tool Inspection Pattern:\")\n",
    "print(tool_inspection_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Using MCP Tools with LangChain Agents\n",
    "\n",
    "The real power of MCP comes from using these tools with LangChain agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCEPTUAL EXAMPLE - Shows MCP + LangChain agent integration\n",
    "\n",
    "agent_integration_example = \"\"\"\n",
    "from langchain.agents import create_agent\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Configure MCP client\n",
    "client = MultiServerMCPClient(\n",
    "    servers={\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\"-y\", \"@modelcontextprotocol/server-time\"],\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "async def run_agent_with_mcp():\n",
    "    # IMPORTANT: MCP requires async context\n",
    "    async with client:\n",
    "        # Get tools from MCP servers\n",
    "        mcp_tools = await client.get_tools()\n",
    "        \n",
    "        # Create agent with MCP tools\n",
    "        agent = create_agent(\n",
    "            model=\"openai:gpt-4o-mini\",\n",
    "            tools=mcp_tools,  # MCP tools are automatically LangChain-compatible\n",
    "            prompt=\"You are a helpful assistant with access to external tools.\",\n",
    "        )\n",
    "        \n",
    "        # Invoke agent - it can now use MCP tools\n",
    "        result = await agent.ainvoke({\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the current date and time?\"\n",
    "            }]\n",
    "        })\n",
    "        \n",
    "        print(result[\"messages\"][-1].content)\n",
    "        # Expected: Agent uses time server's get_current_time tool\n",
    "        # Returns: \"The current date and time is...\"\n",
    "\n",
    "# Run the async function\n",
    "import asyncio\n",
    "asyncio.run(run_agent_with_mcp())\n",
    "\"\"\"\n",
    "\n",
    "print(\"MCP + Agent Integration Pattern:\")\n",
    "print(agent_integration_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Notes on Async Patterns\n",
    "\n",
    "MCP requires async/await patterns because:\n",
    "1. **MCP servers run as separate processes** - Communication is inherently async\n",
    "2. **Multiple servers** - Need to coordinate multiple async connections\n",
    "3. **Better performance** - Non-blocking I/O for tool calls\n",
    "\n",
    "**Pattern to follow:**\n",
    "```python\n",
    "async with client:  # Establishes connections to all MCP servers\n",
    "    tools = await client.get_tools()  # Async operation\n",
    "    result = await agent.ainvoke(...)  # Use async invoke\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Combining MCP Tools with Custom Tools\n",
    "\n",
    "You can mix MCP tools with your own custom LangChain tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCEPTUAL EXAMPLE - Mixing MCP and custom tools\n",
    "\n",
    "mixed_tools_example = \"\"\"\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Define custom tools with @tool decorator\n",
    "@tool\n",
    "def calculate_tip(bill_amount: float, tip_percentage: float) -> float:\n",
    "    \\\"\\\"\\\"Calculate tip amount for a restaurant bill.\\\"\\\"\\\"\n",
    "    return bill_amount * (tip_percentage / 100)\n",
    "\n",
    "@tool\n",
    "def convert_temperature(celsius: float) -> dict:\n",
    "    \\\"\\\"\\\"Convert Celsius to Fahrenheit and Kelvin.\\\"\\\"\\\"\n",
    "    return {\n",
    "        \"fahrenheit\": (celsius * 9/5) + 32,\n",
    "        \"kelvin\": celsius + 273.15\n",
    "    }\n",
    "\n",
    "custom_tools = [calculate_tip, convert_temperature]\n",
    "\n",
    "# Configure MCP client\n",
    "client = MultiServerMCPClient(\n",
    "    servers={\n",
    "        \"filesystem\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/tmp\"],\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "async def run_agent_with_mixed_tools():\n",
    "    async with client:\n",
    "        # Get MCP tools\n",
    "        mcp_tools = await client.get_tools()\n",
    "        \n",
    "        # Combine MCP tools with custom tools\n",
    "        all_tools = mcp_tools + custom_tools\n",
    "        \n",
    "        print(f\"Total tools available: {len(all_tools)}\")\n",
    "        print(f\"- MCP tools: {len(mcp_tools)}\")\n",
    "        print(f\"- Custom tools: {len(custom_tools)}\")\n",
    "        \n",
    "        # Create agent with all tools\n",
    "        agent = create_agent(\n",
    "            model=\"openai:gpt-4o-mini\",\n",
    "            tools=all_tools,\n",
    "            prompt=\"You have access to filesystem operations and utility calculations.\",\n",
    "        )\n",
    "        \n",
    "        # Agent can now use both MCP and custom tools\n",
    "        result = await agent.ainvoke({\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Create a file at /tmp/test.txt with the text 'Hello', then calculate a 20% tip on a $50 bill.\"\n",
    "            }]\n",
    "        })\n",
    "        \n",
    "        print(result[\"messages\"][-1].content)\n",
    "        # Agent will:\n",
    "        # 1. Use MCP filesystem tool to create file\n",
    "        # 2. Use custom calculate_tip tool for the calculation\n",
    "\n",
    "import asyncio\n",
    "asyncio.run(run_agent_with_mixed_tools())\n",
    "\"\"\"\n",
    "\n",
    "print(\"Mixed Tools Pattern:\")\n",
    "print(mixed_tools_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Selection Behavior\n",
    "\n",
    "When you provide multiple tools to an agent:\n",
    "1. **LLM decides** which tool to use based on tool descriptions\n",
    "2. **Clear descriptions are crucial** - Make tool descriptions specific and distinct\n",
    "3. **No conflicts** - MCP and custom tools work seamlessly together\n",
    "4. **Tool chaining** - Agent can use multiple tools in sequence to complete a task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Example: Custom Tools as MCP Alternatives\n",
    "\n",
    "Since MCP requires specific server setup, let's create a practical example using custom tools that simulate common MCP server functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Get the current date and time in ISO format.\"\"\"\n",
    "    return datetime.now().isoformat()\n",
    "\n",
    "@tool\n",
    "def read_file(path: str) -> str:\n",
    "    \"\"\"Read contents of a file. Returns the file contents as a string.\"\"\"\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def write_file(path: str, content: str) -> str:\n",
    "    \"\"\"Write content to a file. Creates the file if it doesn't exist.\"\"\"\n",
    "    try:\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(content)\n",
    "        return f\"Successfully wrote to {path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing file: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def list_directory(path: str) -> str:\n",
    "    \"\"\"List files and directories in the specified path.\"\"\"\n",
    "    import os\n",
    "    try:\n",
    "        items = os.listdir(path)\n",
    "        return json.dumps(items, indent=2)\n",
    "    except Exception as e:\n",
    "        return f\"Error listing directory: {str(e)}\"\n",
    "\n",
    "# These tools simulate common MCP server capabilities\n",
    "simulated_mcp_tools = [get_current_time, read_file, write_file, list_directory]\n",
    "\n",
    "print(\"Created simulated MCP tools:\")\n",
    "for tool in simulated_mcp_tools:\n",
    "    print(f\"- {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Simulated MCP Tools with an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Create agent with our simulated MCP tools\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=simulated_mcp_tools,\n",
    "    prompt=\"You are a helpful assistant with access to file system and time tools.\",\n",
    ")\n",
    "\n",
    "# Test the agent\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the current time? Format your response nicely.\"\n",
    "    }]\n",
    "})\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Step Tool Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-step operations\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Create a file called /tmp/test_mcp.txt with the current timestamp, then read it back to me.\"\n",
    "    }]\n",
    "})\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available MCP Servers\n",
    "\n",
    "The MCP ecosystem includes hundreds of pre-built servers. Here are some popular ones:\n",
    "\n",
    "### Official MCP Servers (by Anthropic)\n",
    "- **@modelcontextprotocol/server-filesystem** - File system operations\n",
    "- **@modelcontextprotocol/server-time** - Time and date utilities\n",
    "- **@modelcontextprotocol/server-memory** - Key-value memory storage\n",
    "\n",
    "### Community MCP Servers\n",
    "- **GitHub MCP** - GitHub API integration (repos, issues, PRs)\n",
    "- **Slack MCP** - Send messages, read channels\n",
    "- **PostgreSQL MCP** - Database queries and operations\n",
    "- **Google Drive MCP** - File access and management\n",
    "- **Notion MCP** - Access Notion workspaces\n",
    "- **Puppeteer MCP** - Browser automation\n",
    "- **Git MCP** - Git operations\n",
    "\n",
    "### Finding MCP Servers\n",
    "\n",
    "1. **Official Registry**: https://github.com/modelcontextprotocol\n",
    "2. **npm Search**: Search for \"@modelcontextprotocol\" or \"mcp-server\"\n",
    "3. **GitHub Topics**: Look for repositories tagged with \"model-context-protocol\"\n",
    "\n",
    "### Installing MCP Servers\n",
    "\n",
    "Most MCP servers are Node.js packages installed via npm/npx:\n",
    "\n",
    "```bash\n",
    "# No installation needed - use npx to run directly\n",
    "npx -y @modelcontextprotocol/server-time\n",
    "\n",
    "# Or install globally\n",
    "npm install -g @modelcontextprotocol/server-filesystem\n",
    "```\n",
    "\n",
    "Some are Python-based:\n",
    "```bash\n",
    "pip install mcp-server-git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World MCP Setup Example\n",
    "\n",
    "Here's a complete example of how you would set up MCP in a production environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_setup = \"\"\"\n",
    "# production_agent.py\n",
    "# /// script\n",
    "# requires-python = \">=3.12\"\n",
    "# dependencies = [\n",
    "#     \"langchain>=1.0.0\",\n",
    "#     \"langchain-openai\",\n",
    "#     \"langchain-mcp-adapters>=0.1.0\",\n",
    "#     \"langgraph>=1.0.0\",\n",
    "# ]\n",
    "# ///\n",
    "\n",
    "import asyncio\n",
    "from langchain.agents import create_agent\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Custom business logic tools\n",
    "@tool\n",
    "def calculate_discount(price: float, discount_percent: float) -> float:\n",
    "    \\\"\\\"\\\"Calculate final price after discount.\\\"\\\"\\\"\n",
    "    return price * (1 - discount_percent / 100)\n",
    "\n",
    "# Configure MCP servers\n",
    "mcp_client = MultiServerMCPClient(\n",
    "    servers={\n",
    "        \"github\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@modelcontextprotocol/server-github\",\n",
    "            ],\n",
    "            \"env\": {\n",
    "                \"GITHUB_TOKEN\": \"your-github-token\"\n",
    "            }\n",
    "        },\n",
    "        \"filesystem\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@modelcontextprotocol/server-filesystem\",\n",
    "                \"/workspace\"\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    async with mcp_client:\n",
    "        # Load MCP tools\n",
    "        mcp_tools = await mcp_client.get_tools()\n",
    "        \n",
    "        # Combine with custom tools\n",
    "        all_tools = mcp_tools + [calculate_discount]\n",
    "        \n",
    "        # Create production agent\n",
    "        agent = create_agent(\n",
    "            model=\"openai:gpt-4o\",\n",
    "            tools=all_tools,\n",
    "            prompt=\\\"\\\"\\\"You are a development assistant with access to:\n",
    "            - GitHub operations (create issues, PRs, etc.)\n",
    "            - File system operations\n",
    "            - Business calculations\n",
    "            \n",
    "            Always be helpful and precise in your operations.\\\"\\\"\\\",\n",
    "        )\n",
    "        \n",
    "        # Example usage\n",
    "        result = await agent.ainvoke({\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Create a GitHub issue in my repo 'myproject' titled 'Add MCP integration' with a detailed description.\"\n",
    "            }]\n",
    "        })\n",
    "        \n",
    "        print(result[\"messages\"][-1].content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\"\"\"\n",
    "\n",
    "print(\"Production Setup Example:\")\n",
    "print(production_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Tips\n",
    "\n",
    "### 1. Error Handling\n",
    "Always wrap MCP operations in try-except blocks:\n",
    "```python\n",
    "async with client:\n",
    "    try:\n",
    "        tools = await client.get_tools()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load MCP tools: {e}\")\n",
    "        # Fall back to custom tools only\n",
    "```\n",
    "\n",
    "### 2. Environment Variables\n",
    "Store MCP server credentials in environment variables:\n",
    "```python\n",
    "import os\n",
    "\"env\": {\n",
    "    \"GITHUB_TOKEN\": os.getenv(\"GITHUB_TOKEN\"),\n",
    "    \"API_KEY\": os.getenv(\"SERVICE_API_KEY\"),\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. Development vs Production\n",
    "- **Development**: Use simulated tools or custom tools for faster iteration\n",
    "- **Production**: Use MCP servers for standardized, production-ready integrations\n",
    "\n",
    "### 4. Tool Description Quality\n",
    "When mixing MCP and custom tools, ensure descriptions are clear and specific:\n",
    "```python\n",
    "@tool\n",
    "def custom_search(query: str) -> str:\n",
    "    \\\"\\\"\\\"Search our internal knowledge base (NOT web search).\\\"\\\"\\\"\n",
    "    # Clear distinction from MCP search tools\n",
    "```\n",
    "\n",
    "### 5. Monitoring and Logging\n",
    "Enable LangSmith tracing to monitor MCP tool usage:\n",
    "```python\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"mcp-production\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **MCP is the future of tool integration** - Standardized, reusable, ecosystem-driven\n",
    "2. **Works seamlessly with LangChain** - MCP tools automatically become LangChain tools\n",
    "3. **Requires async patterns** - Always use `async with` and `await`\n",
    "4. **Mix with custom tools** - Combine MCP servers with project-specific tools\n",
    "5. **Growing ecosystem** - Hundreds of servers available, backed by major AI companies\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|---------------|\n",
    "| Production app with external APIs | Use MCP servers |\n",
    "| Quick prototype | Use custom tools |\n",
    "| Reusable tool across projects | Create/use MCP server |\n",
    "| Business-specific logic | Use custom tools |\n",
    "| Standard operations (GitHub, DB, etc.) | Use MCP servers |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Explore MCP servers**: Browse https://github.com/modelcontextprotocol\n",
    "2. **Try locally**: Install Node.js and test MCP servers on your machine\n",
    "3. **Build an MCP server**: Create a custom server for your unique needs\n",
    "4. **Join the community**: Follow MCP developments and contribute\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **LangChain MCP Adapters**: https://github.com/langchain-ai/langchain-mcp-adapters\n",
    "- **MCP Documentation**: https://docs.langchain.com/oss/python/langchain/mcp\n",
    "- **MCP Specification**: https://github.com/modelcontextprotocol/specification\n",
    "- **Community Servers**: https://github.com/topics/model-context-protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note**: This notebook focused on concepts and patterns due to MCP's subprocess requirements. For hands-on experience, run these examples in a local Python environment with Node.js installed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
