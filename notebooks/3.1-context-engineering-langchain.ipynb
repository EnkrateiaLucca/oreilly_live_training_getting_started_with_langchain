{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Engineering in LangChain 1.0 & LangGraph\n",
    "\n",
    "Context engineering is about building **dynamic systems** that provide the right information and tools in the right format so that an LLM can accomplish its task. Think of the LLM as a CPU and its context window as RAM - we need to carefully curate what goes into that limited working memory.\n",
    "\n",
    "This notebook covers the **LangChain 1.0+** patterns for context management:\n",
    "\n",
    "1. **Static Context** - Immutable data that doesn't change during execution\n",
    "2. **Dynamic Context** - Mutable data that evolves as the application runs  \n",
    "3. **Runtime Context** - Data scoped to a single run using `context_schema`\n",
    "4. **Cross-Conversation Context** - Data that persists across sessions using `InjectedStore`\n",
    "5. **Unified Tool Access** - Using `InjectedState`, `InjectedStore`, and `ToolRuntime`\n",
    "\n",
    "---\n",
    "\n",
    "## Key LangChain 1.0+ Concepts\n",
    "\n",
    "| Annotation | Purpose | Hidden from LLM? |\n",
    "|------------|---------|------------------|\n",
    "| `InjectedState` | Inject current graph state into tools | Yes |\n",
    "| `InjectedStore` | Inject persistent memory store into tools | Yes |\n",
    "| `ToolRuntime` | Unified access to state, store, context, config | Yes |\n",
    "| `context_schema` | Define run-scoped context for agents | N/A |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages and configure our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain>=1.0.0 langchain-openai langchain-community langgraph>=0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Static Context\n",
    "\n",
    "**Static context** is immutable data that doesn't change during execution. Examples include:\n",
    "\n",
    "- User metadata (name, preferences set at startup)\n",
    "- Database connections\n",
    "- Available tools\n",
    "- Configuration settings\n",
    "- System prompts\n",
    "\n",
    "In LangChain 1.0, static context is typically embedded in system prompts or accessed via closure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Static Context via System Prompts\n",
    "\n",
    "The simplest form of static context - information baked into the system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At TechCorp Inc., we offer a 30-day money-back guarantee. If you are not satisfied with your purchase, you can request a refund within 30 days of your purchase date. If you have any specific questions or need further assistance with the refund process, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Static context embedded in system prompt\n",
    "STATIC_SYSTEM_PROMPT = \"\"\"You are a customer support agent for TechCorp.\n",
    "\n",
    "Company Information (STATIC CONTEXT):\n",
    "- Company: TechCorp Inc.\n",
    "- Support Hours: Mon-Fri 9am-5pm EST\n",
    "- Refund Policy: 30-day money-back guarantee\n",
    "- Current Promotion: 20% off with code SAVE20\n",
    "\n",
    "Use this information to help customers with their inquiries.\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=STATIC_SYSTEM_PROMPT),\n",
    "    HumanMessage(content=\"What's your refund policy?\")\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Static Context via Closure (Module-Level Config)\n",
    "\n",
    "A simple pattern: define configuration once, and tools/functions access it via closure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Widget Pro: $99.99 (in stock)\n",
      "Product 'Unknown Product' not found. Available: ['Widget Pro', 'Gadget Plus', 'Device Max']\n",
      "Widget Pro: $74.99 (discount capped at 25%)\n"
     ]
    }
   ],
   "source": [
    "# Static configuration - defined once, never changes during execution\n",
    "PRODUCT_CATALOG = {\n",
    "    \"Widget Pro\": {\"price\": 99.99, \"in_stock\": True},\n",
    "    \"Gadget Plus\": {\"price\": 149.99, \"in_stock\": True},\n",
    "    \"Device Max\": {\"price\": 299.99, \"in_stock\": False},\n",
    "}\n",
    "\n",
    "MAX_DISCOUNT_PERCENT = 25\n",
    "SUPPORT_EMAIL = \"support@techcorp.com\"\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def check_product(product_name: str) -> str:\n",
    "    \"\"\"Check if a product exists and its availability.\"\"\"\n",
    "    # Tools access static context via closure\n",
    "    if product_name in PRODUCT_CATALOG:\n",
    "        product = PRODUCT_CATALOG[product_name]\n",
    "        status = \"in stock\" if product[\"in_stock\"] else \"out of stock\"\n",
    "        return f\"{product_name}: ${product['price']} ({status})\"\n",
    "    return f\"Product '{product_name}' not found. Available: {list(PRODUCT_CATALOG.keys())}\"\n",
    "\n",
    "@tool\n",
    "def calculate_price(product_name: str, discount_percent: int = 0) -> str:\n",
    "    \"\"\"Calculate final price with optional discount.\"\"\"\n",
    "    if product_name not in PRODUCT_CATALOG:\n",
    "        return f\"Product '{product_name}' not found.\"\n",
    "    \n",
    "    base_price = PRODUCT_CATALOG[product_name][\"price\"]\n",
    "    # Static context (MAX_DISCOUNT_PERCENT) limits the discount\n",
    "    actual_discount = min(discount_percent, MAX_DISCOUNT_PERCENT)\n",
    "    final_price = base_price * (1 - actual_discount / 100)\n",
    "    \n",
    "    result = f\"{product_name}: ${final_price:.2f}\"\n",
    "    if discount_percent > MAX_DISCOUNT_PERCENT:\n",
    "        result += f\" (discount capped at {MAX_DISCOUNT_PERCENT}%)\"\n",
    "    return result\n",
    "\n",
    "# Test the tools - they use static context\n",
    "print(check_product.invoke({\"product_name\": \"Widget Pro\"}))\n",
    "print(check_product.invoke({\"product_name\": \"Unknown Product\"}))\n",
    "print(calculate_price.invoke({\"product_name\": \"Widget Pro\", \"discount_percent\": 50}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Static Context with ReAct Agent\n",
    "\n",
    "Let's build a simple agent that uses these tools with static context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Create a ReAct agent with our tools\n",
    "# The tools have access to static context (PRODUCT_CATALOG, MAX_DISCOUNT_PERCENT)\n",
    "shop_assistant = create_agent(\n",
    "    model=\"openai:gpt-5-mini\",\n",
    "    tools=[check_product, calculate_price],\n",
    "    system_prompt=\"You are a helpful shop assistant. Use tools to help customers with product info and pricing.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's the price of Widget Pro with a 20% discount?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  calculate_price (call_6BFGcFzRIaNblb07KFj9YIVK)\n",
      " Call ID: call_6BFGcFzRIaNblb07KFj9YIVK\n",
      "  Args:\n",
      "    product_name: Widget Pro\n",
      "    discount_percent: 20\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: calculate_price\n",
      "\n",
      "Widget Pro: $79.99\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "With a 20% discount, the price of the Widget Pro is $79.99.\n"
     ]
    }
   ],
   "source": [
    "# The agent uses tools that access static context\n",
    "result = shop_assistant.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's the price of Widget Pro with a 20% discount?\"}]\n",
    "})\n",
    "\n",
    "for msg in result[\"messages\"]:\n",
    "    if hasattr(msg, 'pretty_print'):\n",
    "        msg.pretty_print()\n",
    "    else:\n",
    "        print(f\"{msg['role']}: {msg['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Max is currently out of stock; its price is $299.99.\n",
      "\n",
      "Would you like me to:\n",
      "- Show all available products and prices now,\n",
      "- Look for similar/alternative models (e.g., Device Pro, Device Mini), or\n",
      "- Set a restock notification for Device Max?\n",
      "\n",
      "Tell me which option (or any filters like price range, features, or category) and Iâ€™ll fetch the list.\n"
     ]
    }
   ],
   "source": [
    "# Another query - notice the static context (product catalog) is always available\n",
    "result = shop_assistant.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Is Device Max available? And what products do you have?\"}]\n",
    "})\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Dynamic Context with InjectedState\n",
    "\n",
    "**Dynamic context** is mutable data that evolves as the application runs. In LangChain 1.0+, we use `InjectedState` to give tools access to the current graph state without exposing it to the LLM.\n",
    "\n",
    "**Key Benefits:**\n",
    "- Tools can read/write state without the LLM seeing implementation details\n",
    "- Reduces prompt size (20-30% reduction in routing latency)\n",
    "- Enhanced tool selection stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Basic InjectedState Pattern\n",
    "\n",
    "The `InjectedState` annotation marks parameters that should be injected automatically, hidden from the LLM's view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.prebuilt import create_react_agent, InjectedState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Define our state with dynamic context\n",
    "class ShoppingState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    cart: list[str]  # Dynamic context: items in cart\n",
    "    total_items: int  # Dynamic context: computed value\n",
    "\n",
    "\n",
    "@tool\n",
    "def add_to_cart(\n",
    "    item: str,\n",
    "    state: Annotated[dict, InjectedState]  # Injected - not visible to LLM\n",
    ") -> str:\n",
    "    \"\"\"Add an item to the shopping cart.\"\"\"\n",
    "    # Access dynamic context from state\n",
    "    current_cart = state.get(\"cart\", [])\n",
    "    new_cart = current_cart + [item]\n",
    "    return f\"Added '{item}' to cart. Cart now has {len(new_cart)} items: {new_cart}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def view_cart(\n",
    "    state: Annotated[dict, InjectedState]  # Injected - not visible to LLM\n",
    ") -> str:\n",
    "    \"\"\"View the current shopping cart contents.\"\"\"\n",
    "    cart = state.get(\"cart\", [])\n",
    "    if not cart:\n",
    "        return \"Your cart is empty.\"\n",
    "    return f\"Cart contents ({len(cart)} items): {', '.join(cart)}\"\n",
    "\n",
    "\n",
    "# Notice: The LLM only sees \"item\" parameter for add_to_cart,\n",
    "# and no parameters for view_cart. The state injection is hidden.\n",
    "print(\"add_to_cart schema (LLM view):\")\n",
    "print(add_to_cart.get_input_schema().model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Injecting Specific State Fields\n",
    "\n",
    "Instead of injecting the entire state, you can inject specific fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_cart_summary schema (LLM view):\n",
      "{'description': 'Get a summary of the shopping cart.', 'properties': {'cart': {'items': {}, 'title': 'Cart', 'type': 'array'}}, 'required': ['cart'], 'title': 'get_cart_summary', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def get_cart_summary(\n",
    "    cart: Annotated[list, InjectedState(\"cart\")]  # Inject only the 'cart' field\n",
    ") -> str:\n",
    "    \"\"\"Get a summary of the shopping cart.\"\"\"\n",
    "    if not cart:\n",
    "        return \"Cart is empty.\"\n",
    "    from collections import Counter\n",
    "    counts = Counter(cart)\n",
    "    summary = [f\"{item}: {count}\" for item, count in counts.items()]\n",
    "    return f\"Cart summary: {', '.join(summary)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def check_cart_size(\n",
    "    total_items: Annotated[int, InjectedState(\"total_items\")]  # Inject specific field\n",
    ") -> str:\n",
    "    \"\"\"Check if the cart has items.\"\"\"\n",
    "    if total_items == 0:\n",
    "        return \"Your cart is empty. Add some items!\"\n",
    "    return f\"You have {total_items} item(s) in your cart.\"\n",
    "\n",
    "\n",
    "# Both tools have no visible parameters to the LLM\n",
    "print(\"get_cart_summary schema (LLM view):\")\n",
    "print(get_cart_summary.get_input_schema().model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Building a Stateful Agent with InjectedState\n",
    "\n",
    "Let's create a complete agent that maintains dynamic state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent, InjectedState, ToolNode\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from typing import Annotated, Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# State with dynamic context\n",
    "class CartState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    cart: list[str]\n",
    "\n",
    "\n",
    "@tool\n",
    "def add_item(\n",
    "    item: str,\n",
    "    quantity: int = 1,\n",
    "    state: Annotated[dict, InjectedState] = None\n",
    ") -> str:\n",
    "    \"\"\"Add item(s) to the cart.\n",
    "    \n",
    "    Args:\n",
    "        item: Name of the item to add\n",
    "        quantity: How many to add (default 1)\n",
    "    \"\"\"\n",
    "    current_cart = state.get(\"cart\", []) if state else []\n",
    "    items_to_add = [item] * quantity\n",
    "    new_cart = current_cart + items_to_add\n",
    "    return f\"Added {quantity}x {item}. Cart: {new_cart}\"\n",
    "\n",
    "\n",
    "@tool  \n",
    "def show_cart(\n",
    "    state: Annotated[dict, InjectedState] = None\n",
    ") -> str:\n",
    "    \"\"\"Display current cart contents.\"\"\"\n",
    "    cart = state.get(\"cart\", []) if state else []\n",
    "    if not cart:\n",
    "        return \"Cart is empty.\"\n",
    "    from collections import Counter\n",
    "    counts = Counter(cart)\n",
    "    items = [f\"{item} x{count}\" for item, count in counts.items()]\n",
    "    return f\"Cart ({len(cart)} total): {', '.join(items)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def clear_cart(\n",
    "    state: Annotated[dict, InjectedState] = None\n",
    ") -> str:\n",
    "    \"\"\"Clear all items from the cart.\"\"\"\n",
    "    return \"Cart cleared!\"\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "tools = [add_item, show_cart, clear_cart]\n",
    "\n",
    "# Create agent with state schema\n",
    "cart_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    system_prompt=\"You are a shopping assistant. Help users manage their cart.\",\n",
    "    state_schema=CartState\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final response:\n",
      "I've added 2 apples and 1 banana to your cart. If you need anything else, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Test the stateful agent\n",
    "result = cart_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Add 2 apples and 1 banana to my cart\"}],\n",
    "    \"cart\": []  # Initialize empty cart\n",
    "})\n",
    "\n",
    "print(\"Final response:\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Runtime Context with context_schema\n",
    "\n",
    "**Runtime context** is data scoped to a single run or invocation. In LangChain 1.0+, use `context_schema` to define immutable per-run data like user IDs, database handles, or session configuration.\n",
    "\n",
    "**Key difference from state:**\n",
    "- State is mutable and flows through the graph\n",
    "- Context is immutable configuration for this specific run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Defining Runtime Context Schema\n",
    "\n",
    "Use a dataclass to define the context your agent needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.config import get_config\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing import Annotated\n",
    "from langchain_core.tools import InjectedToolArg\n",
    "\n",
    "@dataclass\n",
    "class UserContext:\n",
    "    \"\"\"Runtime context - immutable data for this specific run.\"\"\"\n",
    "    user_id: str\n",
    "    user_tier: str  # \"free\", \"premium\", \"enterprise\"\n",
    "    language: str = \"en\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_personalized_greeting(\n",
    "    time_of_day: str,\n",
    "    config: Annotated[RunnableConfig, InjectedToolArg]\n",
    ") -> str:\n",
    "    \"\"\"Get a personalized greeting based on user context.\n",
    "    \n",
    "    Args:\n",
    "        time_of_day: morning, afternoon, or evening\n",
    "    \"\"\"\n",
    "    # Access runtime context from config\n",
    "    ctx = config.get(\"configurable\", {})\n",
    "    user_id = ctx.get(\"user_id\", \"guest\")\n",
    "    user_tier = ctx.get(\"user_tier\", \"free\")\n",
    "    language = ctx.get(\"language\", \"en\")\n",
    "    \n",
    "    greetings = {\n",
    "        \"en\": {\"morning\": \"Good morning\", \"afternoon\": \"Good afternoon\", \"evening\": \"Good evening\"},\n",
    "        \"es\": {\"morning\": \"Buenos dias\", \"afternoon\": \"Buenas tardes\", \"evening\": \"Buenas noches\"},\n",
    "        \"ja\": {\"morning\": \"Ohayo gozaimasu\", \"afternoon\": \"Konnichiwa\", \"evening\": \"Konbanwa\"},\n",
    "    }\n",
    "    \n",
    "    greeting = greetings.get(language, greetings[\"en\"]).get(time_of_day, \"Hello\")\n",
    "    tier_suffix = \" (Premium Member)\" if user_tier == \"premium\" else \"\"\n",
    "    \n",
    "    return f\"{greeting}, User {user_id}{tier_suffix}!\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def check_feature_access(\n",
    "    feature: str,\n",
    "    config: Annotated[RunnableConfig, InjectedToolArg]\n",
    ") -> str:\n",
    "    \"\"\"Check if user has access to a feature based on their tier.\n",
    "    \n",
    "    Args:\n",
    "        feature: Name of the feature to check\n",
    "    \"\"\"\n",
    "    ctx = config.get(\"configurable\", {})\n",
    "    user_tier = ctx.get(\"user_tier\", \"free\")\n",
    "    \n",
    "    # Feature access by tier\n",
    "    tier_features = {\n",
    "        \"free\": [\"basic_search\", \"view_products\"],\n",
    "        \"premium\": [\"basic_search\", \"view_products\", \"advanced_search\", \"priority_support\"],\n",
    "        \"enterprise\": [\"basic_search\", \"view_products\", \"advanced_search\", \"priority_support\", \"api_access\", \"bulk_export\"]\n",
    "    }\n",
    "    \n",
    "    available = tier_features.get(user_tier, [])\n",
    "    if feature in available:\n",
    "        return f\"Access granted to '{feature}' for {user_tier} tier.\"\n",
    "    return f\"Access denied. '{feature}' requires upgrade from {user_tier} tier.\"\n",
    "\n",
    "\n",
    "# Test runtime context injection\n",
    "print(\"English premium user:\")\n",
    "print(get_personalized_greeting.invoke(\n",
    "    {\"time_of_day\": \"morning\"},\n",
    "    config={\"configurable\": {\"user_id\": \"alice123\", \"user_tier\": \"premium\", \"language\": \"en\"}}\n",
    "))\n",
    "\n",
    "print(\"\\nJapanese free user:\")\n",
    "print(get_personalized_greeting.invoke(\n",
    "    {\"time_of_day\": \"morning\"},\n",
    "    config={\"configurable\": {\"user_id\": \"bob456\", \"user_tier\": \"free\", \"language\": \"ja\"}}\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Using context_schema with Agents\n",
    "\n",
    "The `context_schema` parameter lets you define runtime context for your agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "@dataclass\n",
    "class AppContext:\n",
    "    \"\"\"Application runtime context.\"\"\"\n",
    "    user_id: str\n",
    "    session_id: str\n",
    "    user_tier: str = \"free\"\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create agent with context schema\n",
    "context_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[get_personalized_greeting, check_feature_access],\n",
    "    prompt=\"You are a helpful assistant. Use tools to personalize the experience.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with different runtime contexts\n",
    "print(\"=== Premium User Session ===\")\n",
    "result = context_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Greet me! It's morning. Also check if I can use api_access.\"}]},\n",
    "    config={\"configurable\": {\"user_id\": \"premium_alice\", \"session_id\": \"sess_001\", \"user_tier\": \"premium\"}}\n",
    ")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Free User Session ===\")\n",
    "result = context_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Greet me! It's afternoon. Can I use api_access?\"}]},\n",
    "    config={\"configurable\": {\"user_id\": \"free_bob\", \"session_id\": \"sess_002\", \"user_tier\": \"free\"}}\n",
    ")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Cross-Conversation Context with InjectedStore\n",
    "\n",
    "**Cross-conversation context** persists across multiple sessions. In LangChain 1.0+, use `InjectedStore` to give tools access to persistent storage.\n",
    "\n",
    "**Key Benefits:**\n",
    "- Long-term memory across conversations\n",
    "- User preferences, facts, and history\n",
    "- Hidden from LLM's tool schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Basic InjectedStore Pattern\n",
    "\n",
    "The `InjectedStore` annotation provides tools with access to the persistent store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Any\n",
    "from langgraph.prebuilt import InjectedStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def remember_user_fact(\n",
    "    user_id: str,\n",
    "    fact_type: str,\n",
    "    fact_value: str,\n",
    "    store: Annotated[Any, InjectedStore()]  # Injected - hidden from LLM\n",
    ") -> str:\n",
    "    \"\"\"Remember a fact about a user for future conversations.\n",
    "    \n",
    "    Args:\n",
    "        user_id: The user's identifier\n",
    "        fact_type: Category like 'name', 'preference', 'hobby'\n",
    "        fact_value: The actual fact to remember\n",
    "    \"\"\"\n",
    "    namespace = (\"users\", user_id, \"facts\")\n",
    "    store.put(namespace, fact_type, {\"value\": fact_value})\n",
    "    return f\"Remembered: {user_id}'s {fact_type} is '{fact_value}'\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def recall_user_facts(\n",
    "    user_id: str,\n",
    "    store: Annotated[Any, InjectedStore()]  # Injected - hidden from LLM\n",
    ") -> str:\n",
    "    \"\"\"Recall all stored facts about a user.\n",
    "    \n",
    "    Args:\n",
    "        user_id: The user's identifier\n",
    "    \"\"\"\n",
    "    namespace = (\"users\", user_id, \"facts\")\n",
    "    items = store.search(namespace)\n",
    "    \n",
    "    if not items:\n",
    "        return f\"No memories stored for user '{user_id}'\"\n",
    "    \n",
    "    facts = [f\"- {item.key}: {item.value['value']}\" for item in items]\n",
    "    return f\"Memories for {user_id}:\\n\" + \"\\n\".join(facts)\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_specific_fact(\n",
    "    user_id: str,\n",
    "    fact_type: str,\n",
    "    store: Annotated[Any, InjectedStore()]  # Injected - hidden from LLM\n",
    ") -> str:\n",
    "    \"\"\"Get a specific fact about a user.\n",
    "    \n",
    "    Args:\n",
    "        user_id: The user's identifier\n",
    "        fact_type: The type of fact to retrieve\n",
    "    \"\"\"\n",
    "    namespace = (\"users\", user_id, \"facts\")\n",
    "    item = store.get(namespace, fact_type)\n",
    "    \n",
    "    if item is None:\n",
    "        return f\"No '{fact_type}' stored for {user_id}\"\n",
    "    return f\"{user_id}'s {fact_type}: {item.value['value']}\"\n",
    "\n",
    "\n",
    "# Check that store is hidden from LLM\n",
    "print(\"remember_user_fact schema (LLM sees):\")\n",
    "print(remember_user_fact.get_input_schema().model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Building a Memory-Enabled Agent\n",
    "\n",
    "Create an agent with persistent cross-conversation memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create persistent store\n",
    "memory_store = InMemoryStore()\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create agent with store for cross-conversation memory\n",
    "memory_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[remember_user_fact, recall_user_facts, get_specific_fact],\n",
    "    prompt=\"\"\"You are a helpful assistant with long-term memory.\n",
    "When users share personal information, use remember_user_fact to store it.\n",
    "When users ask what you know about them, use recall_user_facts.\n",
    "Always use the user_id from the conversation.\"\"\",\n",
    "    store=memory_store  # Inject the store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session 1: User shares information\n",
    "print(\"=== Session 1: Alice introduces herself ===\")\n",
    "result = memory_agent.invoke({\n",
    "    \"messages\": [{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Hi! I'm user alice_123. My name is Alice, I work as a data scientist, and I love hiking.\"\n",
    "    }]\n",
    "})\n",
    "print(f\"Agent: {result['messages'][-1].content}\")\n",
    "\n",
    "# Check what was stored\n",
    "print(\"\\nStored in memory store:\")\n",
    "for item in memory_store.search((\"users\", \"alice_123\", \"facts\")):\n",
    "    print(f\"  {item.key}: {item.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session 2: NEW conversation thread - but memories persist!\n",
    "print(\"=== Session 2: New conversation, same user ===\")\n",
    "result = memory_agent.invoke({\n",
    "    \"messages\": [{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"I'm alice_123. What do you remember about me?\"\n",
    "    }]\n",
    "})\n",
    "print(f\"Agent: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session 3: Different user - isolated memories\n",
    "print(\"=== Session 3: Different user (bob_456) ===\")\n",
    "result = memory_agent.invoke({\n",
    "    \"messages\": [{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"I'm bob_456. What do you know about me?\"\n",
    "    }]\n",
    "})\n",
    "print(f\"Agent: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Thread-Scoped vs Cross-Conversation Memory\n",
    "\n",
    "Combine `MemorySaver` (thread-scoped) with `InMemoryStore` (cross-conversation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Thread-scoped: conversation history within a session\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Cross-conversation: long-term user facts\n",
    "long_term_store = InMemoryStore()\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Agent with both memory types\n",
    "full_memory_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[remember_user_fact, recall_user_facts],\n",
    "    prompt=\"You are a helpful assistant with both conversation history and long-term memory.\",\n",
    "    checkpointer=checkpointer,  # Thread-scoped (conversation history)\n",
    "    store=long_term_store       # Cross-conversation (user facts)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread 1: Build conversation history AND store facts\n",
    "thread_config = {\"configurable\": {\"thread_id\": \"conversation_001\"}}\n",
    "\n",
    "print(\"=== Thread 1, Turn 1 ===\")\n",
    "result = full_memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi, I'm charlie_789. My favorite color is blue.\"}]},\n",
    "    config=thread_config\n",
    ")\n",
    "print(f\"Agent: {result['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\n=== Thread 1, Turn 2 ===\")\n",
    "result = full_memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What was my favorite color again?\"}]},\n",
    "    config=thread_config\n",
    ")\n",
    "print(f\"Agent: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread 2: NEW conversation - no history, but long-term memory persists\n",
    "new_thread_config = {\"configurable\": {\"thread_id\": \"conversation_002\"}}\n",
    "\n",
    "print(\"=== Thread 2 (NEW conversation) ===\")\n",
    "result = full_memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"I'm charlie_789. Do you remember my favorite color?\"}]},\n",
    "    config=new_thread_config\n",
    ")\n",
    "print(f\"Agent: {result['messages'][-1].content}\")\n",
    "print(\"\\n(Note: Agent uses recall_user_facts to access long-term memory, not conversation history)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Scratchpad Pattern (Working Memory)\n",
    "\n",
    "A **scratchpad** is dynamic context where the agent writes notes to itself during execution. This is useful for:\n",
    "\n",
    "- Tracking intermediate results\n",
    "- Building up analysis over multiple steps\n",
    "- Maintaining focus on multi-step tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "class ScratchpadState(TypedDict):\n",
    "    \"\"\"State with a scratchpad for dynamic notes.\"\"\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "    scratchpad: List[str]  # Dynamic context: notes the agent writes\n",
    "    step_count: int        # Dynamic context: tracks progress\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "def analyze_and_note(state: ScratchpadState):\n",
    "    \"\"\"Agent analyzes input and writes to scratchpad.\"\"\"\n",
    "    user_msg = state[\"messages\"][-1].content\n",
    "    current_notes = state.get(\"scratchpad\", [])\n",
    "    step = state.get(\"step_count\", 0) + 1\n",
    "    \n",
    "    # Build prompt with current scratchpad context\n",
    "    scratchpad_context = \"\\n\".join(current_notes) if current_notes else \"(empty)\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze this request and write a brief note about what you learned.\n",
    "\n",
    "Current scratchpad:\n",
    "{scratchpad_context}\n",
    "\n",
    "User request: {user_msg}\n",
    "\n",
    "Respond with:\n",
    "1. A note to add to your scratchpad (start with \"NOTE:\")\n",
    "2. Your response to the user (start with \"RESPONSE:\")\"\"\"\n",
    "    \n",
    "    response = model.invoke([HumanMessage(content=prompt)])\n",
    "    content = response.content\n",
    "    \n",
    "    # Extract note and response\n",
    "    note = \"\"\n",
    "    reply = content\n",
    "    if \"NOTE:\" in content and \"RESPONSE:\" in content:\n",
    "        parts = content.split(\"RESPONSE:\")\n",
    "        note = parts[0].replace(\"NOTE:\", \"\").strip()\n",
    "        reply = parts[1].strip()\n",
    "    \n",
    "    # Update scratchpad (dynamic context)\n",
    "    new_notes = current_notes + [f\"Step {step}: {note}\"] if note else current_notes\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"scratchpad\": new_notes,\n",
    "        \"step_count\": step\n",
    "    }\n",
    "\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(ScratchpadState)\n",
    "builder.add_node(\"analyze\", analyze_and_note)\n",
    "builder.add_edge(START, \"analyze\")\n",
    "builder.add_edge(\"analyze\", END)\n",
    "\n",
    "scratchpad_graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple interactions, watching scratchpad grow\n",
    "state = {\n",
    "    \"messages\": [HumanMessage(content=\"I need help planning a trip to Japan\")],\n",
    "    \"scratchpad\": [],\n",
    "    \"step_count\": 0\n",
    "}\n",
    "\n",
    "result = scratchpad_graph.invoke(state)\n",
    "print(\"After turn 1:\")\n",
    "print(f\"Scratchpad: {result['scratchpad']}\")\n",
    "print(f\"Step count: {result['step_count']}\")\n",
    "\n",
    "# Continue conversation - scratchpad accumulates\n",
    "result[\"messages\"].append(HumanMessage(content=\"I'm interested in traditional culture and food\"))\n",
    "result = scratchpad_graph.invoke(result)\n",
    "print(\"\\nAfter turn 2:\")\n",
    "print(f\"Scratchpad: {result['scratchpad']}\")\n",
    "print(f\"Step count: {result['step_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary: Context Types in LangChain 1.0+\n",
    "\n",
    "| Context Type | Mutability | Lifetime | LangChain 1.0 Mechanism | Example |\n",
    "|-------------|------------|----------|------------------------|--------|\n",
    "| **Static** | Immutable | Application lifetime | System prompts, closure | Product catalog, API limits |\n",
    "| **Dynamic** | Mutable | Within a run | `InjectedState`, StateGraph | Shopping cart, scratchpad |\n",
    "| **Runtime** | Immutable per-run | Single invocation | `context_schema`, `RunnableConfig` | User ID, tier, language |\n",
    "| **Cross-Conversation** | Persistent | Multiple sessions | `InjectedStore`, `InMemoryStore` | User facts, preferences |\n",
    "\n",
    "---\n",
    "\n",
    "## Key LangChain 1.0+ Patterns\n",
    "\n",
    "### 1. InjectedState - Access graph state in tools\n",
    "```python\n",
    "@tool\n",
    "def my_tool(query: str, state: Annotated[dict, InjectedState]) -> str:\n",
    "    # state is injected, hidden from LLM\n",
    "    cart = state.get(\"cart\", [])\n",
    "    ...\n",
    "```\n",
    "\n",
    "### 2. InjectedStore - Access persistent storage in tools\n",
    "```python\n",
    "@tool\n",
    "def save_fact(key: str, value: str, store: Annotated[Any, InjectedStore()]) -> str:\n",
    "    # store is injected, hidden from LLM\n",
    "    store.put((\"facts\",), key, value)\n",
    "    ...\n",
    "```\n",
    "\n",
    "### 3. context_schema - Define run-scoped context\n",
    "```python\n",
    "@dataclass\n",
    "class AppContext:\n",
    "    user_id: str\n",
    "    tier: str\n",
    "\n",
    "agent = create_react_agent(..., context_schema=AppContext)\n",
    "agent.invoke({...}, context=AppContext(user_id=\"alice\", tier=\"premium\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Four Strategies for Context Management\n",
    "\n",
    "1. **Write** - Save context externally (scratchpad, store)\n",
    "2. **Select** - Pull relevant context in (search, filtering)\n",
    "3. **Compress** - Summarize to reduce tokens\n",
    "4. **Isolate** - Separate contexts across agents/fields\n",
    "\n",
    "---\n",
    "\n",
    "**Resources:**\n",
    "- [LangChain Tools Documentation](https://docs.langchain.com/oss/python/langchain/tools)\n",
    "- [LangGraph Agents Reference](https://reference.langchain.com/python/langgraph/agents/)\n",
    "- [Context Engineering Repository](https://github.com/langchain-ai/context_engineering)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
