{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a LLM app\n",
    "\n",
    "1. Organize your parameters for experimentation\n",
    "   1. Atomic task that you're trying to solve\n",
    "   2. One or more prompts that solve either the whole thing or part of it\n",
    "   3. Define a metric that comprises performance\n",
    "      1. Summarization: g-eval method that involves consistency, relevancy, fluency, etc...\n",
    "      2. Extraction: produce a test coverage dataset with examples of examples of successful extraction and then work from there\n",
    "      3. General applications: LLM-as-a-judge just uses gpt-4o or gpt-4o-mini or whatever LLM to judge the outputs of other LLMs and score them for comparison and performance tracking\n",
    "   4. RAG: \n",
    "      1. Hallucination\n",
    "      2. Latency\n",
    "      3. Token\n",
    "      4. Token/cost \n",
    "      5. Benchmarks for your specific use case context/scenarion\n",
    "      6. Faithfulness (accuracy concerning the retrieved context, avoiding hallucinations), Context Relevancy (relevance of the retrieved context to the query), Answer Relevancy (relevance of the generated answer to the query)\n",
    "      7. Build your own small test dataset for whatever your task is, and iterate and grow it, by mixing manual annotations with semi-automated annotations and performance tracking\n",
    "2. Do a lot of few-shot examples in your prompts."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
