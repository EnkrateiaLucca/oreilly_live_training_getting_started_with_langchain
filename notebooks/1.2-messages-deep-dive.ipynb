{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Messages in LangChain 1.0\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EnkrateiaLucca/oreilly_live_training_getting_started_with_langchain/blob/main/notebooks/1.2-messages-deep-dive.ipynb)\n",
    "\n",
    "Messages are the fundamental unit of communication in LangChain 1.0. This notebook explores:\n",
    "\n",
    "- **Message types**: HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "- **Input flexibility**: Multiple ways to send messages\n",
    "- **Message metadata**: Token usage, response info, content blocks\n",
    "- **Debugging**: Using `.pretty_print()` for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if running in Colab)\n",
    "# !pip install langchain>=1.0.0 langchain-openai>=0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set API key\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Message Types Overview\n",
    "\n",
    "LangChain provides several message types, each serving a specific purpose in the conversation:\n",
    "\n",
    "| Type | Purpose | When to Use |\n",
    "|------|---------|-------------|\n",
    "| `SystemMessage` | Set behavior/context | Once at start of conversation |\n",
    "| `HumanMessage` | User input | Every user turn |\n",
    "| `AIMessage` | Model response | Returned by the model |\n",
    "| `ToolMessage` | Tool execution results | After tool calls |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "\n",
    "# Create different message types\n",
    "system_msg = SystemMessage(content=\"You are a helpful coding assistant.\")\n",
    "human_msg = HumanMessage(content=\"What is a decorator in Python?\")\n",
    "ai_msg = AIMessage(content=\"A decorator is a function that wraps another function...\")\n",
    "\n",
    "print(f\"System: {system_msg.type} -> {system_msg.content[:50]}...\")\n",
    "print(f\"Human: {human_msg.type} -> {human_msg.content[:50]}...\")\n",
    "print(f\"AI: {ai_msg.type} -> {ai_msg.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Input Format Flexibility\n",
    "\n",
    "LangChain 1.0 accepts messages in multiple formats - choose what's most convenient for your use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Format 1: Simple string (converted to HumanMessage)\n",
    "response1 = llm.invoke(\"What is 2 + 2?\")\n",
    "print(\"Format 1 (string):\")\n",
    "print(f\"  Input type: str\")\n",
    "print(f\"  Response: {response1.content}\\n\")\n",
    "\n",
    "# Format 2: Dictionary format (OpenAI-style)\n",
    "response2 = llm.invoke([{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}])\n",
    "print(\"Format 2 (dict):\")\n",
    "print(f\"  Input type: dict\")\n",
    "print(f\"  Response: {response2.content}\\n\")\n",
    "\n",
    "# Format 3: Explicit LangChain message objects\n",
    "response3 = llm.invoke([HumanMessage(content=\"What is 2 + 2?\")])\n",
    "print(\"Format 3 (HumanMessage):\")\n",
    "print(f\"  Input type: HumanMessage\")\n",
    "print(f\"  Response: {response3.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-turn Conversations\n",
    "\n",
    "Build up conversation history by including previous messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a Python tutor. Give concise answers.\"),\n",
    "    HumanMessage(content=\"What is a list comprehension?\"),\n",
    "    AIMessage(content=\"A list comprehension is a concise way to create lists: [x*2 for x in range(5)]\"),\n",
    "    HumanMessage(content=\"Can I add conditions to it?\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Message Metadata\n",
    "\n",
    "Messages contain rich metadata that's useful for debugging, cost tracking, and understanding model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a response with full metadata\n",
    "response = llm.invoke(\"Explain async/await in Python in 2 sentences.\")\n",
    "\n",
    "print(\"=== Message Properties ===\")\n",
    "print(f\"Content: {response.content}\\n\")\n",
    "print(f\"Type: {response.type}\")\n",
    "print(f\"ID: {response.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token usage metadata - useful for cost tracking\n",
    "print(\"=== Token Usage ===\")\n",
    "if response.usage_metadata:\n",
    "    print(f\"Input tokens: {response.usage_metadata.get('input_tokens', 'N/A')}\")\n",
    "    print(f\"Output tokens: {response.usage_metadata.get('output_tokens', 'N/A')}\")\n",
    "    print(f\"Total tokens: {response.usage_metadata.get('total_tokens', 'N/A')}\")\n",
    "else:\n",
    "    print(\"No usage metadata available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response metadata - model info and settings\n",
    "print(\"=== Response Metadata ===\")\n",
    "if response.response_metadata:\n",
    "    print(f\"Model: {response.response_metadata.get('model_name', 'N/A')}\")\n",
    "    print(f\"Finish reason: {response.response_metadata.get('finish_reason', 'N/A')}\")\n",
    "    print(f\"System fingerprint: {response.response_metadata.get('system_fingerprint', 'N/A')[:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Calculation Example\n",
    "\n",
    "Use token metadata to calculate API costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini pricing (as of late 2024)\n",
    "INPUT_COST_PER_1M = 0.15  # $0.15 per 1M input tokens\n",
    "OUTPUT_COST_PER_1M = 0.60  # $0.60 per 1M output tokens\n",
    "\n",
    "def calculate_cost(response):\n",
    "    \"\"\"Calculate cost from response metadata.\"\"\"\n",
    "    if not response.usage_metadata:\n",
    "        return None\n",
    "    \n",
    "    input_tokens = response.usage_metadata.get('input_tokens', 0)\n",
    "    output_tokens = response.usage_metadata.get('output_tokens', 0)\n",
    "    \n",
    "    input_cost = (input_tokens / 1_000_000) * INPUT_COST_PER_1M\n",
    "    output_cost = (output_tokens / 1_000_000) * OUTPUT_COST_PER_1M\n",
    "    \n",
    "    return {\n",
    "        'input_cost': input_cost,\n",
    "        'output_cost': output_cost,\n",
    "        'total_cost': input_cost + output_cost\n",
    "    }\n",
    "\n",
    "cost = calculate_cost(response)\n",
    "if cost:\n",
    "    print(f\"Estimated cost: ${cost['total_cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Debugging with pretty_print()\n",
    "\n",
    "The `.pretty_print()` method provides a formatted view of messages - invaluable for debugging complex chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print a single message\n",
    "print(\"=== Single Message ===\")\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print conversation history\n",
    "conversation = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What's the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "    HumanMessage(content=\"What's its population?\"),\n",
    "]\n",
    "\n",
    "print(\"=== Conversation History ===\")\n",
    "for msg in conversation:\n",
    "    msg.pretty_print()\n",
    "    print()  # Add spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tool Messages\n",
    "\n",
    "When agents use tools, the results are returned as `ToolMessage` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get current weather for a city.\"\"\"\n",
    "    # Simulated response\n",
    "    return f\"Weather in {city}: 72Â°F, sunny\"\n",
    "\n",
    "# Simulate a tool call and response\n",
    "tool_call_id = \"call_abc123\"\n",
    "\n",
    "# This is what an AI message with tool calls looks like\n",
    "ai_with_tool = AIMessage(\n",
    "    content=\"\",\n",
    "    tool_calls=[{\n",
    "        \"id\": tool_call_id,\n",
    "        \"name\": \"get_weather\",\n",
    "        \"args\": {\"city\": \"San Francisco\"}\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Execute the tool\n",
    "result = get_weather.invoke({\"city\": \"San Francisco\"})\n",
    "\n",
    "# Create tool message with the result\n",
    "tool_msg = ToolMessage(\n",
    "    content=result,\n",
    "    tool_call_id=tool_call_id,\n",
    "    name=\"get_weather\"\n",
    ")\n",
    "\n",
    "print(\"Tool Message:\")\n",
    "print(f\"  Type: {tool_msg.type}\")\n",
    "print(f\"  Tool Call ID: {tool_msg.tool_call_id}\")\n",
    "print(f\"  Content: {tool_msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Content Blocks (Multimodal)\n",
    "\n",
    "LangChain 1.0 supports multimodal content through content blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text and image in the same message (GPT-4 Vision)\n",
    "multimodal_message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Python_logo_51.svg/200px-Python_logo_51.svg.png\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Multimodal message content blocks:\")\n",
    "for i, block in enumerate(multimodal_message.content):\n",
    "    print(f\"  Block {i}: type={block.get('type')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke with multimodal content (requires vision-capable model)\n",
    "vision_llm = ChatOpenAI(model=\"gpt-4o-mini\")  # gpt-4o-mini supports vision\n",
    "\n",
    "response = vision_llm.invoke([multimodal_message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Example: Conversation Manager\n",
    "\n",
    "Here's a practical pattern for managing conversations with proper message handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationManager:\n",
    "    \"\"\"Simple conversation manager with token tracking.\"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt: str, model: str = \"gpt-4o-mini\"):\n",
    "        self.llm = ChatOpenAI(model=model, temperature=0.7)\n",
    "        self.messages = [SystemMessage(content=system_prompt)]\n",
    "        self.total_tokens = 0\n",
    "        self.total_cost = 0.0\n",
    "    \n",
    "    def chat(self, user_input: str) -> str:\n",
    "        \"\"\"Send a message and get a response.\"\"\"\n",
    "        # Add user message\n",
    "        self.messages.append(HumanMessage(content=user_input))\n",
    "        \n",
    "        # Get response\n",
    "        response = self.llm.invoke(self.messages)\n",
    "        \n",
    "        # Track tokens\n",
    "        if response.usage_metadata:\n",
    "            tokens = response.usage_metadata.get('total_tokens', 0)\n",
    "            self.total_tokens += tokens\n",
    "        \n",
    "        # Add AI response to history\n",
    "        self.messages.append(response)\n",
    "        \n",
    "        return response.content\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get conversation statistics.\"\"\"\n",
    "        return {\n",
    "            \"turns\": len([m for m in self.messages if isinstance(m, HumanMessage)]),\n",
    "            \"total_tokens\": self.total_tokens,\n",
    "            \"message_count\": len(self.messages)\n",
    "        }\n",
    "    \n",
    "    def show_history(self):\n",
    "        \"\"\"Print conversation history.\"\"\"\n",
    "        for msg in self.messages:\n",
    "            msg.pretty_print()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the conversation manager\n",
    "conv = ConversationManager(\n",
    "    system_prompt=\"You are a helpful Python tutor. Keep answers concise.\"\n",
    ")\n",
    "\n",
    "# Have a conversation\n",
    "print(\"User: What is a generator?\")\n",
    "print(f\"AI: {conv.chat('What is a generator?')}\\n\")\n",
    "\n",
    "print(\"User: Show me an example\")\n",
    "print(f\"AI: {conv.chat('Show me an example')}\\n\")\n",
    "\n",
    "# Check stats\n",
    "print(\"\\n=== Conversation Stats ===\")\n",
    "stats = conv.get_stats()\n",
    "print(f\"Turns: {stats['turns']}\")\n",
    "print(f\"Total tokens: {stats['total_tokens']}\")\n",
    "print(f\"Messages in history: {stats['message_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Message Types**: Use `SystemMessage` for behavior, `HumanMessage` for user input, `AIMessage` for responses\n",
    "2. **Input Flexibility**: Strings, dicts, or explicit message objects all work\n",
    "3. **Metadata**: Use `usage_metadata` for token tracking and cost calculation\n",
    "4. **Debugging**: `.pretty_print()` is your friend for inspecting messages\n",
    "5. **Multimodal**: Content blocks support text + images in the same message\n",
    "\n",
    "**Next Steps:**\n",
    "- Explore streaming patterns in the next notebook\n",
    "- Learn how agents handle tool messages automatically"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
