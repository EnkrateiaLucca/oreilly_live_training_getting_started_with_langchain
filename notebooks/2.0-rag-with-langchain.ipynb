{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build a RAG Agent with LangChain\n",
        "\n",
        "This notebook demonstrates how to build a Retrieval Augmented Generation (RAG) application with LangChain.\n",
        "\n",
        "**What is RAG?**\n",
        "RAG is a technique that enables LLMs to answer questions about specific source information by:\n",
        "1. **Indexing**: Creating a searchable index from your data sources\n",
        "2. **Retrieval and Generation**: Retrieving relevant context and using it to generate answers\n",
        "\n",
        "We'll cover:\n",
        "- **Indexing Pipeline**: Loading, splitting, and storing documents\n",
        "- **RAG Agents**: Using agents with retrieval tools for flexible querying\n",
        "- **RAG Chains**: Two-step chains for fast, simple queries\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, let's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install -qU langchain langchain-text-splitters langchain-community bs4 langchain-openai langchain-chroma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 1: Indexing\n",
        "\n",
        "The indexing pipeline prepares your data for retrieval. This typically happens in a separate process before querying.\n",
        "\n",
        "## 1.1 Loading Documents\n",
        "\n",
        "We'll use the LLM Powered Autonomous Agents blog post by Lilian Weng as our data source.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 document(s)\n",
            "First document length: 22843 characters\n",
            "\n",
            "First 500 characters:\n",
            "Skip to main contentSkip to footerResearchEconomic FuturesCommitmentsLearnNewsTry ClaudeEngineering at AnthropicWriting effective tools for agents — with agentsPublished Sep 11, 2025Agents are only as effective as the tools we give them. We share how to write high-quality tools and evaluations, and how you can boost performance by using Claude to optimize its tools for itself.The Model Context Protocol (MCP) can empower LLM agents with potentially hundreds of tools to solve real-world tasks. But...\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Load the blog post content without relying on bs4 SoupStrainer that may not match the site's structure.\n",
        "# Instead, use the default loader behavior and rely on the loader to extract all the text from the page.\n",
        "\n",
        "loader = WebBaseLoader(\"https://www.anthropic.com/engineering/writing-tools-for-agents\")\n",
        "docs = loader.load()\n",
        "\n",
        "if not docs:\n",
        "    raise ValueError(\"No documents were loaded. The page may be protected by anti-bot measures, or the loader could not parse the content.\")\n",
        "\n",
        "print(f\"Loaded {len(docs)} document(s)\")\n",
        "print(f\"First document length: {len(docs[0].page_content)} characters\")\n",
        "print(f\"\\nFirst 500 characters:\\n{docs[0].page_content[:500]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://www.anthropic.com/engineering/writing-tools-for-agents', 'title': 'Writing effective tools for AI agents—using AI agents \\\\ Anthropic', 'description': 'Writing effective tools for AI agents—using AI agents', 'language': 'en'}, page_content='Skip to main contentSkip to footerResearchEconomic FuturesCommitmentsLearnNewsTry ClaudeEngineering at AnthropicWriting effective tools for agents — with agentsPublished Sep 11, 2025Agents are only as effective as the tools we give them. We share how to write high-quality tools and evaluations, and how you can boost performance by using Claude to optimize its tools for itself.The Model Context Protocol (MCP) can empower LLM agents with potentially hundreds of tools to solve real-world tasks. But how do we make those tools maximally effective?In this post, we describe our most effective techniques for improving performance in a variety of agentic AI systems1.We begin by covering how you can:Build and test prototypes of your toolsCreate and run comprehensive evaluations of your tools with agentsCollaborate with agents like Claude Code to automatically increase the performance of your toolsWe conclude with key principles for writing high-quality tools we’ve identified along the way:Choosing the right tools to implement (and not to implement)Namespacing tools to define clear boundaries in functionalityReturning meaningful context from tools back to agentsOptimizing tool responses for token efficiencyPrompt-engineering tool descriptions and specsBuilding an evaluation allows you to systematically measure the performance of your tools. You can use Claude Code to automatically optimize your tools against this evaluation.What is a tool?In computing, deterministic systems produce the same output every time given identical inputs, while non-deterministic systems—like agents—can generate varied responses even with the same starting conditions.When we traditionally write software, we’re establishing a contract between deterministic systems. For instance, a function call like getWeather(“NYC”) will always fetch the weather in New York City in the exact same manner every time it is called.Tools are a new kind of software which reflects a contract between deterministic systems and non-deterministic agents. When a user asks \"Should I bring an umbrella today?,” an agent might call the weather tool, answer from general knowledge, or even ask a clarifying question about location first. Occasionally, an agent might hallucinate or even fail to grasp how to use a tool.This means fundamentally rethinking our approach when writing software for agents: instead of writing tools and MCP servers the way we’d write functions and APIs for other developers or systems, we need to design them for agents.Our goal is to increase the surface area over which agents can be effective in solving a wide range of tasks by using tools to pursue a variety of successful strategies. Fortunately, in our experience, the tools that are most “ergonomic” for agents also end up being surprisingly intuitive to grasp as humans.How to write toolsIn this section, we describe how you can collaborate with agents both to write and to improve the tools you give them. Start by standing up a quick prototype of your tools and testing them locally. Next, run a comprehensive evaluation to measure subsequent changes. Working alongside agents, you can repeat the process of evaluating and improving your tools until your agents achieve strong performance on real-world tasks.Building a prototypeIt can be difficult to anticipate which tools agents will find ergonomic and which tools they won’t without getting hands-on yourself. Start by standing up a quick prototype of your tools. If you’re using Claude Code to write your tools (potentially in one-shot), it helps to give Claude documentation for any software libraries, APIs, or SDKs (including potentially the MCP SDK) your tools will rely on. LLM-friendly documentation can commonly be found in flat llms.txt files on official documentation sites (here’s our API’s).Wrapping your tools in a local MCP server or Desktop extension (DXT) will allow you to connect and test your tools in Claude Code or the Claude Desktop app.To connect your local MCP server to Claude Code, run claude mcp add <name> <command> [args...].To connect your local MCP server or DXT to the Claude Desktop app, navigate to Settings > Developer or Settings > Extensions, respectively.Tools can also be passed directly into Anthropic API calls for programmatic testing.Test the tools yourself to identify any rough edges. Collect feedback from your users to build an intuition around the use-cases and prompts you expect your tools to enable.Running an evaluationNext, you need to measure how well Claude uses your tools by running an evaluation. Start by generating lots of evaluation tasks, grounded in real world uses. We recommend collaborating with an agent to help analyze your results and determine how to improve your tools. See this process end-to-end in our tool evaluation cookbook.Held-out test set performance of our internal Slack toolsGenerating evaluation tasksWith your early prototype, Claude Code can quickly explore your tools and create dozens of prompt and response pairs. Prompts should be inspired by real-world uses and be based on realistic data sources and services (for example, internal knowledge bases and microservices). We recommend you avoid overly simplistic or superficial “sandbox” environments that don’t stress-test your tools with sufficient complexity. Strong evaluation tasks might require multiple tool calls—potentially dozens.Here are some examples of strong tasks:Schedule a meeting with Jane next week to discuss our latest Acme Corp project. Attach the notes from our last project planning meeting and reserve a conference room.Customer ID 9182 reported that they were charged three times for a single purchase attempt. Find all relevant log entries and determine if any other customers were affected by the same issue.Customer Sarah Chen just submitted a cancellation request. Prepare a retention offer. Determine: (1) why they\\'re leaving, (2) what retention offer would be most compelling, and (3) any risk factors we should be aware of before making an offer.And here are some weaker tasks:Schedule a meeting with jane@acme.corp next week.Search the payment logs for purchase_complete and customer_id=9182.Find the cancellation request by Customer ID 45892.Each evaluation prompt should be paired with a verifiable response or outcome. Your verifier can be as simple as an exact string comparison between ground truth and sampled responses, or as advanced as enlisting Claude to judge the response. Avoid overly strict verifiers that reject correct responses due to spurious differences like formatting, punctuation, or valid alternative phrasings.For each prompt-response pair, you can optionally also specify the tools you expect an agent to call in solving the task, to measure whether or not agents are successful in grasping each tool’s purpose during evaluation. However, because there might be multiple valid paths to solving tasks correctly, try to avoid overspecifying or overfitting to strategies.Running the evaluationWe recommend running your evaluation programmatically with direct LLM API calls. Use simple agentic loops (while-loops wrapping alternating LLM API and tool calls): one loop for each evaluation task. Each evaluation agent should be given a single task prompt and your tools.In your evaluation agents’ system prompts, we recommend instructing agents to output not just structured response blocks (for verification), but also reasoning and feedback blocks. Instructing agents to output these before tool call and response blocks may increase LLMs’ effective intelligence by triggering chain-of-thought (CoT) behaviors.If you’re running your evaluation with Claude, you can turn on interleaved thinking for similar functionality “off-the-shelf”. This will help you probe why agents do or don’t call certain tools and highlight specific areas of improvement in tool descriptions and specs.As well as top-level accuracy, we recommend collecting other metrics like the total runtime of individual tool calls and tasks, the total number of tool calls, the total token consumption, and tool errors. Tracking tool calls can help reveal common workflows that agents pursue and offer some opportunities for tools to consolidate.Held-out test set performance of our internal Asana toolsAnalyzing resultsAgents are your helpful partners in spotting issues and providing feedback on everything from contradictory tool descriptions to inefficient tool implementations and confusing tool schemas. However, keep in mind that what agents omit in their feedback and responses can often be more important than what they include. LLMs don’t always say what they mean.Observe where your agents get stumped or confused. Read through your evaluation agents’ reasoning and feedback (or CoT) to identify rough edges. Review the raw transcripts (including tool calls and tool responses) to catch any behavior not explicitly described in the agent’s CoT. Read between the lines; remember that your evaluation agents don’t necessarily know the correct answers and strategies.Analyze your tool calling metrics. Lots of redundant tool calls might suggest some rightsizing of pagination or token limit parameters is warranted; lots of tool errors for invalid parameters might suggest tools could use clearer descriptions or better examples. When we launched Claude’s web search tool, we identified that Claude was needlessly appending 2025 to the tool’s query parameter, biasing search results and degrading performance (we steered Claude in the right direction by improving the tool description).Collaborating with agentsYou can even let agents analyze your results and improve your tools for you. Simply concatenate the transcripts from your evaluation agents and paste them into Claude Code. Claude is an expert at analyzing transcripts and refactoring lots of tools all at once—for example, to ensure tool implementations and descriptions remain self-consistent when new changes are made.In fact, most of the advice in this post came from repeatedly optimizing our internal tool implementations with Claude Code. Our evaluations were created on top of our internal workspace, mirroring the complexity of our internal workflows, including real projects, documents, and messages.We relied on held-out test sets to ensure we did not overfit to our “training” evaluations. These test sets revealed that we could extract additional performance improvements even beyond what we achieved with \"expert\" tool implementations—whether those tools were manually written by our researchers or generated by Claude itself.In the next section, we’ll share some of what we learned from this process.Principles for writing effective toolsIn this section, we distill our learnings into a few guiding principles for writing effective tools.Choosing the right tools for agentsMore tools don’t always lead to better outcomes. A common error we’ve observed is tools that merely wrap existing software functionality or API endpoints—whether or not the tools are appropriate for agents. This is because agents have distinct “affordances” to traditional software—that is, they have different ways of perceiving the potential actions they can take with those toolsLLM agents have limited \"context\" (that is, there are limits to how much information they can process at once), whereas computer memory is cheap and abundant. Consider the task of searching for a contact in an address book. Traditional software programs can efficiently store and process a list of contacts one at a time, checking each one before moving on.However, if an LLM agent uses a tool that returns ALL contacts and then has to read through each one token-by-token, it\\'s wasting its limited context space on irrelevant information (imagine searching for a contact in your address book by reading each page from top-to-bottom—that is, via brute-force search). The better and more natural approach (for agents and humans alike) is to skip to the relevant page first (perhaps finding it alphabetically).We recommend building a few thoughtful tools targeting specific high-impact workflows, which match your evaluation tasks and scaling up from there. In the address book case, you might choose to implement a search_contacts or message_contact tool instead of a list_contacts tool.Tools can consolidate functionality, handling potentially multiple discrete operations (or API calls) under the hood. For example, tools can enrich tool responses with related metadata or handle frequently chained, multi-step tasks in a single tool call.Here are some examples:Instead of implementing a list_users, list_events, and create_event tools, consider implementing a schedule_event tool which finds availability and schedules an event.Instead of implementing a read_logs tool, consider implementing a search_logs tool which only returns relevant log lines and some surrounding context.Instead of implementing get_customer_by_id, list_transactions, and list_notes tools, implement a get_customer_context tool which compiles all of a customer’s recent & relevant information all at once.Make sure each tool you build has a clear, distinct purpose. Tools should enable agents to subdivide and solve tasks in much the same way that a human would, given access to the same underlying resources, and simultaneously reduce the context that would have otherwise been consumed by intermediate outputs.Too many tools or overlapping tools can also distract agents from pursuing efficient strategies. Careful, selective planning of the tools you build (or don’t build) can really pay off.Namespacing your toolsYour AI agents will potentially gain access to dozens of MCP servers and hundreds of different tools–including those by other developers. When tools overlap in function or have a vague purpose, agents can get confused about which ones to use.Namespacing (grouping related tools under common prefixes) can help delineate boundaries between lots of tools; MCP clients sometimes do this by default. For example, namespacing tools by service (e.g., asana_search, jira_search) and by resource (e.g., asana_projects_search, asana_users_search), can help agents select the right tools at the right time.We have found selecting between prefix- and suffix-based namespacing to have non-trivial effects on our tool-use evaluations. Effects vary by LLM and we encourage you to choose a naming scheme according to your own evaluations.Agents might call the wrong tools, call the right tools with the wrong parameters, call too few tools, or process tool responses incorrectly. By selectively implementing tools whose names reflect natural subdivisions of tasks, you simultaneously reduce the number of tools and tool descriptions loaded into the agent’s context and offload agentic computation from the agent’s context back into the tool calls themselves. This reduces an agent’s overall risk of making mistakes.Returning meaningful context from your toolsIn the same vein, tool implementations should take care to return only high signal information back to agents. They should prioritize contextual relevance over flexibility, and eschew low-level technical identifiers (for example: uuid, 256px_image_url, mime_type). Fields like name, image_url, and file_type are much more likely to directly inform agents’ downstream actions and responses.Agents also tend to grapple with natural language names, terms, or identifiers significantly more successfully than they do with cryptic identifiers. We’ve found that merely resolving arbitrary alphanumeric UUIDs to more semantically meaningful and interpretable language (or even a 0-indexed ID scheme) significantly improves Claude’s precision in retrieval tasks by reducing hallucinations.In some instances, agents may require the flexibility to interact with both natural language and technical identifiers outputs, if only to trigger downstream tool calls (for example, search_user(name=’jane’) → send_message(id=12345)). You can enable both by exposing a simple response_format enum parameter in your tool, allowing your agent to control whether tools return “concise” or “detailed” responses (images below).You can add more formats for even greater flexibility, similar to GraphQL where you can choose exactly which pieces of information you want to receive. Here is an example ResponseFormat enum to control tool response verbosity:enum ResponseFormat {\\n   DETAILED = \"detailed\",\\n   CONCISE = \"concise\"\\n}CopyHere’s an example of a detailed tool response (206 tokens):Here’s an example of a concise tool response (72 tokens):Slack threads and thread replies are identified by unique thread_ts which are required to fetch thread replies. thread_ts and other IDs (channel_id, user_id) can be retrieved from a “detailed” tool response to enable further tool calls that require these. “concise” tool responses return only thread content and exclude IDs. In this example, we use ~⅓ of the tokens with “concise” tool responses.Even your tool response structure—for example XML, JSON, or Markdown—can have an impact on evaluation performance: there is no one-size-fits-all solution. This is because LLMs are trained on next-token prediction and tend to perform better with formats that match their training data. The optimal response structure will vary widely by task and agent. We encourage you to select the best response structure based on your own evaluation.Optimizing tool responses for token efficiencyOptimizing the quality of context is important. But so is optimizing the quantity of context returned back to agents in tool responses.We suggest implementing some combination of pagination, range selection, filtering, and/or truncation with sensible default parameter values for any tool responses that could use up lots of context. For Claude Code, we restrict tool responses to 25,000 tokens by default. We expect the effective context length of agents to grow over time, but the need for context-efficient tools to remain.If you choose to truncate responses, be sure to steer agents with helpful instructions. You can directly encourage agents to pursue more token-efficient strategies, like making many small and targeted searches instead of a single, broad search for a knowledge retrieval task. Similarly, if a tool call raises an error (for example, during input validation), you can prompt-engineer your error responses to clearly communicate specific and actionable improvements, rather than opaque error codes or tracebacks.Here’s an example of a truncated tool response:Here’s an example of an unhelpful error response:Here’s an example of a helpful error response:Tool truncation and error responses can steer agents towards more token-efficient tool-use behaviors (using filters or pagination) or give examples of correctly formatted tool inputs.Prompt-engineering your tool descriptionsWe now come to one of the most effective methods for improving tools: prompt-engineering your tool descriptions and specs. Because these are loaded into your agents’ context, they can collectively steer agents toward effective tool-calling behaviors.When writing tool descriptions and specs, think of how you would describe your tool to a new hire on your team. Consider the context that you might implicitly bring—specialized query formats, definitions of niche terminology, relationships between underlying resources—and make it explicit. Avoid ambiguity by clearly describing (and enforcing with strict data models) expected inputs and outputs. In particular, input parameters should be unambiguously named: instead of a parameter named user, try a parameter named user_id.With your evaluation you can measure the impact of your prompt engineering with greater confidence. Even small refinements to tool descriptions can yield dramatic improvements. Claude Sonnet 3.5 achieved state-of-the-art performance on the SWE-bench Verified evaluation after we made precise refinements to tool descriptions, dramatically reducing error rates and improving task completion.You can find other best practices for tool definitions in our Developer Guide. If you’re building tools for Claude, we also recommend reading about how tools are dynamically loaded into Claude’s system prompt. Lastly, if you’re writing tools for an MCP server, tool annotations help disclose which tools require open-world access or make destructive changes.Looking aheadTo build effective tools for agents, we need to re-orient our software development practices from predictable, deterministic patterns to non-deterministic ones.Through the iterative, evaluation-driven process we’ve described in this post, we\\'ve identified consistent patterns in what makes tools successful: Effective tools are intentionally and clearly defined, use agent context judiciously, can be combined together in diverse workflows, and enable agents to intuitively solve real-world tasks.In the future, we expect the specific mechanisms through which agents interact with the world to evolve—from updates to the MCP protocol to upgrades to the underlying LLMs themselves. With a systematic, evaluation-driven approach to improving tools for agents, we can ensure that as agents become more capable, the tools they use will evolve alongside them.AcknowledgementsWritten by Ken Aizawa with valuable contributions from colleagues across Research (Barry Zhang, Zachary Witten, Daniel Jiang, Sami Al-Sheikh, Matt Bell, Maggie Vo), MCP (Theodora Chu, John Welsh, David Soria Parra, Adam Jones), Product Engineering (Santiago Seira), Marketing (Molly Vorwerck), Design (Drew Roper), and Applied AI (Christian Ryan, Alexander Bricken).1Beyond training the underlying LLMs themselves.Looking to learn more?Explore coursesGet the developer newsletterProduct updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.Please provide your email address if you’d like to receive our monthly developer newsletter. You can unsubscribe at any time.ProductsClaudeClaude CodeClaude and SlackClaude in ExcelSkillsMax planTeam planEnterprise planDownload appPricingLog in to ClaudeModelsOpusSonnetHaikuSolutionsAI agentsCode modernizationCodingCustomer supportEducationFinancial servicesGovernmentLife sciencesNonprofitsClaude Developer PlatformOverviewDeveloper docsPricingAmazon BedrockGoogle Cloud’s Vertex AIConsole loginLearnBlogClaude partner networkCoursesConnectorsCustomer storiesEngineering at AnthropicEventsPowered by ClaudeService partnersStartups programUse casesCompanyAnthropicCareersEconomic FuturesResearchNewsResponsible Scaling PolicySecurity and complianceTransparencyHelp and securityAvailabilityStatusSupport centerTerms and policiesPrivacy policyResponsible disclosure policyTerms of service: CommercialTerms of service: ConsumerUsage policy© 2025 Anthropic PBCWriting effective tools for AI agents—using AI agents \\\\ Anthropic')]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Splitting Documents\n",
        "\n",
        "We need to split the documents into smaller chunks that can be efficiently retrieved. The `RecursiveCharacterTextSplitter` is a good default choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split into 30 chunks\n",
            "\n",
            "First chunk (length: 989):\n",
            "Skip to main contentSkip to footerResearchEconomic FuturesCommitmentsLearnNewsTry ClaudeEngineering at AnthropicWriting effective tools for agents — with agentsPublished Sep 11, 2025Agents are only as effective as the tools we give them. We share how to write high-quality tools and evaluations, and ...\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Split into {len(all_splits)} chunks\")\n",
        "print(f\"\\nFirst chunk (length: {len(all_splits[0].page_content)}):\")\n",
        "print(all_splits[0].page_content[:300] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Storing Documents in a Vector Store\n",
        "\n",
        "We'll use Chroma as our vector store. The documents are embedded and stored for semantic search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store created with 30 documents\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Create embeddings and vector store\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vector_store = Chroma.from_documents(\n",
        "    documents=all_splits,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"rag-tutorial\"\n",
        ")\n",
        "\n",
        "print(f\"Vector store created with {len(all_splits)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: Retrieval and Generation\n",
        "\n",
        "Now that we've indexed our data, we can build RAG applications. We'll demonstrate two approaches:\n",
        "\n",
        "1. **RAG Agents**: Flexible agents that decide when to search\n",
        "2. **RAG Chains**: Fast two-step chains that always search\n",
        "\n",
        "## 2.1 RAG Agents\n",
        "\n",
        "RAG agents use tools to retrieve context. The agent decides when to call the retrieval tool, allowing for:\n",
        "- Multiple searches in support of a single query\n",
        "- Contextual search queries\n",
        "- Skipping searches for simple queries\n",
        "\n",
        "Let's create a retrieval tool and build an agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain_core.tools import tool\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the model\n",
        "model = init_chat_model(\"openai:gpt-4o-mini\")\n",
        "\n",
        "# Create a retrieval tool\n",
        "@tool(response_format=\"content_and_artifact\")\n",
        "def retrieve_context(query: str):\n",
        "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
        "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
        "    serialized = \"\\n\\n\".join(\n",
        "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
        "        for doc in retrieved_docs\n",
        "    )\n",
        "    return serialized, retrieved_docs\n",
        "\n",
        "tools = [retrieve_context]\n",
        "\n",
        "# Create the agent with custom instructions\n",
        "prompt = (\n",
        "    \"You have access to a tool that retrieves context from a blog post. \"\n",
        "    \"Use the tool to help answer user queries.\"\n",
        ")\n",
        "agent = create_agent(model, tools, system_prompt=prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the RAG Agent\n",
        "\n",
        "Let's test with a simple query:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the challenge of writing tools for agents?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  retrieve_context (call_tJPdZ20bYivtSc66fqnSvTB1)\n",
            " Call ID: call_tJPdZ20bYivtSc66fqnSvTB1\n",
            "  Args:\n",
            "    query: challenge of writing tools for agents\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: retrieve_context\n",
            "\n",
            "Source: {'description': 'Writing effective tools for AI agents—using AI agents', 'source': 'https://www.anthropic.com/engineering/writing-tools-for-agents', 'title': 'Writing effective tools for AI agents—using AI agents \\\\ Anthropic', 'language': 'en'}\n",
            "Content: performance improvements even beyond what we achieved with \"expert\" tool implementations—whether those tools were manually written by our researchers or generated by Claude itself.In the next section, we’ll share some of what we learned from this process.Principles for writing effective toolsIn this section, we distill our learnings into a few guiding principles for writing effective tools.Choosing the right tools for agentsMore tools don’t always lead to better outcomes. A common error we’ve observed is tools that merely wrap existing software functionality or API endpoints—whether or not the tools are appropriate for agents. This is because agents have distinct “affordances” to traditional software—that is, they have different ways of perceiving the potential actions they can take with those toolsLLM agents have limited \"context\" (that is, there are limits to how much information they can process at once), whereas computer memory is cheap and abundant. Consider the task of searching\n",
            "\n",
            "Source: {'language': 'en', 'title': 'Writing effective tools for AI agents—using AI agents \\\\ Anthropic', 'description': 'Writing effective tools for AI agents—using AI agents', 'source': 'https://www.anthropic.com/engineering/writing-tools-for-agents'}\n",
            "Content: Skip to main contentSkip to footerResearchEconomic FuturesCommitmentsLearnNewsTry ClaudeEngineering at AnthropicWriting effective tools for agents — with agentsPublished Sep 11, 2025Agents are only as effective as the tools we give them. We share how to write high-quality tools and evaluations, and how you can boost performance by using Claude to optimize its tools for itself.The Model Context Protocol (MCP) can empower LLM agents with potentially hundreds of tools to solve real-world tasks. But how do we make those tools maximally effective?In this post, we describe our most effective techniques for improving performance in a variety of agentic AI systems1.We begin by covering how you can:Build and test prototypes of your toolsCreate and run comprehensive evaluations of your tools with agentsCollaborate with agents like Claude Code to automatically increase the performance of your toolsWe conclude with key principles for writing high-quality tools we’ve identified along the\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The challenge of writing tools for agents stems from several key factors:\n",
            "\n",
            "1. **Understanding Affordances**: Agents have different ways of perceiving and interacting with tools compared to traditional software. Tools need to leverage the unique capabilities of agents rather than just replicate existing functionalities.\n",
            "\n",
            "2. **Context Limitations**: Agents, particularly those based on large language models (LLMs), have limited \"context\". This means there is a constraint on the amount of information they can process at one time, complicating tool design.\n",
            "\n",
            "3. **Tool Selection**: More tools do not necessarily lead to better outcomes. It's a common mistake to create tools that merely wrap existing software or APIs without ensuring they are appropriate for the agent's specific needs.\n",
            "\n",
            "4. **Optimizing Performance**: Writing effective tools involves continuous evaluation and optimization, often requiring collaboration between humans and agents to enhance tool performance.\n",
            "\n",
            "These challenges highlight the need for careful consideration in the design and implementation of tools used by AI agents.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is the challenge of writing tools for agents?\"\n",
        "\n",
        "for step in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    step[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Step Retrieval\n",
        "\n",
        "The agent can perform multiple searches to answer complex questions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the standard method for Task Decomposition?\n",
            "\n",
            "Once you get the answer, look up common extensions of that method.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  retrieve_context (call_biQTXtyzjbnpGaBVSjYxv6CW)\n",
            " Call ID: call_biQTXtyzjbnpGaBVSjYxv6CW\n",
            "  Args:\n",
            "    query: standard method for Task Decomposition\n",
            "  retrieve_context (call_ixoM5v66AKZC4X1D7lcK7uLf)\n",
            " Call ID: call_ixoM5v66AKZC4X1D7lcK7uLf\n",
            "  Args:\n",
            "    query: common extensions of Task Decomposition\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: retrieve_context\n",
            "\n",
            "Source: {'language': 'en', 'title': 'Writing effective tools for AI agents—using AI agents \\\\ Anthropic', 'source': 'https://www.anthropic.com/engineering/writing-tools-for-agents', 'description': 'Writing effective tools for AI agents—using AI agents'}\n",
            "Content: tool evaluation cookbook.Held-out test set performance of our internal Slack toolsGenerating evaluation tasksWith your early prototype, Claude Code can quickly explore your tools and create dozens of prompt and response pairs. Prompts should be inspired by real-world uses and be based on realistic data sources and services (for example, internal knowledge bases and microservices). We recommend you avoid overly simplistic or superficial “sandbox” environments that don’t stress-test your tools with sufficient complexity. Strong evaluation tasks might require multiple tool calls—potentially dozens.Here are some examples of strong tasks:Schedule a meeting with Jane next week to discuss our latest Acme Corp project. Attach the notes from our last project planning meeting and reserve a conference room.Customer ID 9182 reported that they were charged three times for a single purchase attempt. Find all relevant log entries and determine if any other customers were affected by the same\n",
            "\n",
            "Source: {'title': 'Writing effective tools for AI agents—using AI agents \\\\ Anthropic', 'source': 'https://www.anthropic.com/engineering/writing-tools-for-agents', 'description': 'Writing effective tools for AI agents—using AI agents', 'language': 'en'}\n",
            "Content: task. Each evaluation agent should be given a single task prompt and your tools.In your evaluation agents’ system prompts, we recommend instructing agents to output not just structured response blocks (for verification), but also reasoning and feedback blocks. Instructing agents to output these before tool call and response blocks may increase LLMs’ effective intelligence by triggering chain-of-thought (CoT) behaviors.If you’re running your evaluation with Claude, you can turn on interleaved thinking for similar functionality “off-the-shelf”. This will help you probe why agents do or don’t call certain tools and highlight specific areas of improvement in tool descriptions and specs.As well as top-level accuracy, we recommend collecting other metrics like the total runtime of individual tool calls and tasks, the total number of tool calls, the total token consumption, and tool errors. Tracking tool calls can help reveal common workflows that agents pursue and offer some opportunities\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The standard method for Task Decomposition involves breaking down larger tasks into smaller, manageable components. This approach allows individuals or teams to focus on each component separately, making it easier to manage complexity, allocate resources, and track progress. While specific frameworks may vary, typically the method includes:\n",
            "\n",
            "1. **Identifying the Main Task:** Clearly define what the overall task is.\n",
            "2. **Subdividing the Task:** Break the main task down into smaller, discrete tasks or subtasks. Each subtask should ideally be well-defined and manageable.\n",
            "3. **Defining Relationships:** Determine how these subtasks are related and whether they can be worked on in parallel or need to be completed sequentially.\n",
            "4. **Setting Goals:** Establish clear objectives and timelines for each subtask.\n",
            "5. **Assigning Resources:** Allocate the necessary resources, including personnel, tools, and time for each subtask.\n",
            "6. **Monitoring and Adjusting:** Regularly review progress on each subtask and adjust plans as necessary to stay on track for the main task.\n",
            "\n",
            "### Common Extensions of Task Decomposition\n",
            "\n",
            "While the blog post didn’t explicitly detail common extensions, in general, task decomposition methods may be complemented or extended by:\n",
            "\n",
            "- **Agile Methodologies:** These often include iterative task decomposition, allowing for flexibility and adaptation through sprints or cycles.\n",
            "- **Hierarchical Task Analysis (HTA):** This strategy involves not just breaking tasks down into subtasks but also analyzing the relationships between them in a structured hierarchy.\n",
            "- **Mind Mapping:** A visual technique that can be applied to task decomposition, allowing individuals to see the connections between various components.\n",
            "- **Use of Software Tools:** Utilizing project management tools like Trello or Asana that facilitate task decomposition through visual boards and lists.\n",
            "\n",
            "These extensions enhance the basic principles of task decomposition by providing frameworks or tools that can aid in the effective execution of complex projects.\n"
          ]
        }
      ],
      "source": [
        "query = (\n",
        "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
        "    \"Once you get the answer, look up common extensions of that method.\"\n",
        ")\n",
        "\n",
        "for event in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Benefits of RAG Agents:**\n",
        "- ✅ Search only when needed\n",
        "- ✅ Contextual search queries\n",
        "- ✅ Multiple searches allowed\n",
        "\n",
        "**Trade-offs:**\n",
        "- ⚠️ Two inference calls (one for tool call, one for response)\n",
        "- ⚠️ Reduced control (LLM may skip or add unnecessary searches)\n",
        "\n",
        "---\n",
        "\n",
        "## 2.2 RAG Chains\n",
        "\n",
        "RAG chains use a two-step approach:\n",
        "1. Always run a search (using the user query)\n",
        "2. Incorporate results as context in a single LLM call\n",
        "\n",
        "This results in **one inference call per query**, trading flexibility for speed.\n",
        "\n",
        "### Using Dynamic Prompts Middleware\n",
        "\n",
        "We can implement a RAG chain using middleware to inject context:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
        "\n",
        "@dynamic_prompt\n",
        "def prompt_with_context(request: ModelRequest) -> str:\n",
        "    \"\"\"Inject context into state messages.\"\"\"\n",
        "    last_query = request.state[\"messages\"][-1].text\n",
        "    retrieved_docs = vector_store.similarity_search(last_query)\n",
        "\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "\n",
        "    system_message = (\n",
        "        \"You are a helpful assistant. Use the following context in your response:\"\n",
        "        f\"\\n\\n{docs_content}\"\n",
        "    )\n",
        "\n",
        "    return system_message\n",
        "\n",
        "# Create agent with middleware (no tools needed)\n",
        "rag_chain = create_agent(model, tools=[], middleware=[prompt_with_context])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is task decomposition?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Task decomposition is the process of breaking down a complex task into smaller, more manageable subtasks or components. This approach allows for easier handling, understanding, and execution of the overall task. By dividing a task into simpler parts, each subtask can be tackled individually, making it less overwhelming and more organized. \n",
            "\n",
            "In the context of agents or artificial intelligence, task decomposition can help improve the accuracy and efficiency of executing tasks by allowing agents to focus on one subtask at a time, potentially reducing errors and improving outcomes. Each subtask can also be handled by different tools or processes, enhancing flexibility and operability. The idea is to create a structured approach that leads to clearer paths for problem-solving and decision-making in complex scenarios.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is task decomposition?\"\n",
        "\n",
        "for step in rag_chain.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    step[\"messages\"][-1].pretty_print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Returning Source Documents\n",
        "\n",
        "If you need access to the source documents (e.g., for citations), you can use middleware to store them in the state:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "from langchain_core.documents import Document\n",
        "from langchain.agents.middleware import AgentMiddleware, AgentState\n",
        "\n",
        "\n",
        "class State(AgentState):\n",
        "    context: list[Document]\n",
        "\n",
        "\n",
        "class RetrieveDocumentsMiddleware(AgentMiddleware[State]):\n",
        "    state_schema = State\n",
        "\n",
        "    def before_model(self, state: AgentState) -> dict[str, Any] | None:\n",
        "        last_message = state[\"messages\"][-1]\n",
        "        retrieved_docs = vector_store.similarity_search(last_message.text)\n",
        "\n",
        "        docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "\n",
        "        augmented_message_content = (\n",
        "            f\"{last_message.text}\\n\\n\"\n",
        "            \"Use the following context to answer the query:\\n\"\n",
        "            f\"{docs_content}\"\n",
        "        )\n",
        "        return {\n",
        "            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\n",
        "            \"context\": retrieved_docs,\n",
        "        }\n",
        "\n",
        "\n",
        "rag_chain_with_sources = create_agent(\n",
        "    model,\n",
        "    tools=[],\n",
        "    middleware=[RetrieveDocumentsMiddleware()],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "Task decomposition is the process of breaking down a complex task into smaller, more manageable subtasks. This approach helps streamline workflows by assigning specific, well-defined tasks to agents or tools, reducing the likelihood of errors and improving efficiency. In the context provided, task decomposition serves several purposes:\n",
            "\n",
            "1. **Reducing Complexity**: By segmenting a larger task into smaller components, agents can focus on individual parts, which helps mitigate the risk of making mistakes. This is particularly important because agents may miscall tools or parameters when faced with an extensive, intricate task.\n",
            "\n",
            "2. **Improving Tool Utilization**: When tasks are well-defined and correspond to specific tools that match these subdivisions, agents can better select and utilize the appropriate tools, thereby enhancing the effectiveness of tool calls. This means that agents don’t have to manage an overwhelming number of tool descriptions and can instead leverage relevant tools more efficiently.\n",
            "\n",
            "3. **Enhancing Clarity in Responses**: Implementing clear and structured tasks allows agents to return meaningful and relevant context from the tools they use. This prioritization helps agents process results better, making adjustments based on high-signal information rather than technical identifiers that may not directly inform their subsequent actions.\n",
            "\n",
            "4. **Facilitating Evaluation and Improvement**: Task decomposition also allows for more effective evaluation of agents’ performance. By analyzing metrics related to individual subtasks (such as the number of tool calls, runtime, and error rates), developers can identify strengths and weaknesses in agent performance and tool efficiency.\n",
            "\n",
            "In summary, task decomposition is essential for enhancing the operational capabilities of agents by simplifying complex processes, improving tool usage, and facilitating systematic evaluation and ongoing enhancement of agent performance.\n",
            "\n",
            "\n",
            "Retrieved 4 source document(s)\n",
            "\n",
            "First source metadata: {'description': 'Writing effective tools for AI agents—using AI agents', 'title': 'Writing effective tools for AI agents—using AI agents \\\\ Anthropic', 'language': 'en', 'source': 'https://www.anthropic.com/engineering/writing-tools-for-agents'}\n"
          ]
        }
      ],
      "source": [
        "query = \"What is task decomposition?\"\n",
        "\n",
        "result = rag_chain_with_sources.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
        ")\n",
        "\n",
        "# Access the final response\n",
        "print(\"Response:\")\n",
        "print(result[\"messages\"][-1].content)\n",
        "\n",
        "# Access the source documents\n",
        "print(f\"\\n\\nRetrieved {len(result.get('context', []))} source document(s)\")\n",
        "if result.get(\"context\"):\n",
        "    print(f\"\\nFirst source metadata: {result['context'][0].metadata}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "**RAG Agents** are best for:\n",
        "- Complex queries requiring multiple searches\n",
        "- When you want the LLM to decide when to search\n",
        "- General-purpose applications\n",
        "\n",
        "**RAG Chains** are best for:\n",
        "- Simple queries with predictable search needs\n",
        "- When speed is critical (single LLM call)\n",
        "- Constrained settings where you always want to search\n",
        "\n",
        "**Next Steps:**\n",
        "- Add conversational memory for multi-turn interactions\n",
        "- Stream tokens for responsive user experiences\n",
        "- Add structured responses\n",
        "- Deploy with LangSmith Deployment\n",
        "- Explore advanced RAG patterns with LangGraph\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
