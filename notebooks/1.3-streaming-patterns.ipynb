{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain 1.0+ Streaming Patterns\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EnkrateiaLucca/oreilly_live_training_getting_started_with_langchain/blob/main/notebooks/1.3-streaming-patterns.ipynb)\n",
    "\n",
    "In this notebook, we'll explore LangChain's streaming capabilities and why they matter for building responsive LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain 1.0+ Setup\n",
    "# %pip install -qU langchain>=1.0.0\n",
    "# %pip install -qU langchain-core>=1.0.0\n",
    "# %pip install -qU langchain-openai\n",
    "# %pip install -qU langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Streaming Matters\n",
    "\n",
    "Streaming is crucial for building responsive LLM applications because:\n",
    "\n",
    "1. **Low Latency**: Users see results immediately instead of waiting for the full response\n",
    "2. **Better UX**: Real-time feedback makes the application feel more interactive\n",
    "3. **Transparency**: Users can see the model \"thinking\" and generating responses\n",
    "4. **Early Termination**: Users can stop generation if the response is going off-track\n",
    "\n",
    "LangChain provides multiple streaming modes to handle different use cases, from simple token streaming to complex agent workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Invocation vs Streaming\n",
    "\n",
    "Let's start by comparing `invoke()` (no streaming) with `stream()` (token-by-token streaming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming: invoke()\n",
    "\n",
    "With `invoke()`, you get the complete response all at once after waiting for the full generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a short poem about streaming data in real-time.\"\n",
    "\n",
    "# Non-streaming - wait for complete response\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming: stream()\n",
    "\n",
    "With `stream()`, you get tokens as they are generated, enabling real-time display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Streaming - display tokens as they arrive\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating the Difference with a Longer Response\n",
    "\n",
    "The streaming effect is more noticeable with longer outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_prompt = \"\"\"Explain the concept of streaming in large language models,\n",
    "including why it's important for user experience, how it works technically,\n",
    "and what are some common implementation patterns.\"\"\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"STREAMING RESPONSE:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for chunk in llm.stream(long_prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Streaming with Chains (LCEL)\n",
    "\n",
    "LangChain Expression Language (LCEL) chains also support streaming. The streaming flows through the entire chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a chain: prompt | model | parser\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Answer this question: {question}\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt_template | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream through the entire chain\n",
    "question = \"What are the main benefits of using streaming in LLM applications?\"\n",
    "\n",
    "for chunk in chain.stream({\"question\": question}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming with Multiple Output Parsers\n",
    "\n",
    "You can stream even when using output parsers that transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain with string output parser\n",
    "list_prompt = ChatPromptTemplate.from_template(\n",
    "    \"List 5 {topic} in a numbered list format.\"\n",
    ")\n",
    "\n",
    "list_chain = list_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"Streaming a list:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for chunk in list_chain.stream({\"topic\": \"benefits of async programming\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming with Tools and Agents\n",
    "\n",
    "LangChain 1.0 introduces `create_agent` for building agents. Agents can stream both their reasoning and tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "@tool\n",
    "def multiply_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers together.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [get_word_length, multiply_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent prompt\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Use tools when necessary.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "# Create agent\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "agent = create_tool_calling_agent(llm_with_tools, tools, agent_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Agent Streaming\n",
    "\n",
    "Stream the agent's execution to see its reasoning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream agent execution\n",
    "for chunk in agent_executor.stream(\n",
    "    {\"input\": \"What is the length of the word 'streaming' multiplied by 3?\"}\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Streaming: stream_events()\n",
    "\n",
    "For more granular control, `stream_events()` (also called `astream_events()` for async) provides detailed event-by-event streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream events for detailed monitoring\n",
    "async def demonstrate_stream_events():\n",
    "    \"\"\"Demonstrate streaming events with astream_events.\"\"\"\n",
    "    async for event in chain.astream_events(\n",
    "        {\"question\": \"What is LangChain?\"},\n",
    "        version=\"v2\"\n",
    "    ):\n",
    "        kind = event[\"event\"]\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                print(content, end=\"\", flush=True)\n",
    "\n",
    "# Run in notebook\n",
    "import asyncio\n",
    "await demonstrate_stream_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Async Streaming with astream()\n",
    "\n",
    "For production applications, async streaming provides better performance and resource utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async streaming example\n",
    "async def async_stream_example():\n",
    "    \"\"\"Demonstrate async streaming.\"\"\"\n",
    "    prompt = \"Explain the difference between sync and async streaming in 3 sentences.\"\n",
    "    \n",
    "    print(\"Async streaming:\")\n",
    "    async for chunk in llm.astream(prompt):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Execute async function\n",
    "await async_stream_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Chain Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_chain_stream():\n",
    "    \"\"\"Stream through an async chain.\"\"\"\n",
    "    async for chunk in chain.astream({\"question\": \"What are async operations?\"}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "await async_chain_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production Pattern: FastAPI Streaming Endpoint\n",
    "\n",
    "Here's a complete example of how to build a streaming endpoint with FastAPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example FastAPI streaming endpoint (for reference)\n",
    "example_code = '''\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize chain\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Answer: {question}\"\n",
    ")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "@app.post(\"/stream\")\n",
    "async def stream_response(question: str):\n",
    "    \"\"\"Stream LLM response to client.\"\"\"\n",
    "    async def generate():\n",
    "        async for chunk in chain.astream({\"question\": question}):\n",
    "            # Send Server-Sent Events format\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "            await asyncio.sleep(0.01)  # Small delay for better streaming\n",
    "    \n",
    "    return StreamingResponse(\n",
    "        generate(),\n",
    "        media_type=\"text/event-stream\"\n",
    "    )\n",
    "\n",
    "# Run with: uvicorn app:app --reload\n",
    "'''\n",
    "\n",
    "print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Example: Interactive Streaming Assistant\n",
    "\n",
    "Let's build a practical streaming assistant that handles multi-turn conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "class StreamingAssistant:\n",
    "    \"\"\"A streaming conversational assistant.\"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"gpt-4o-mini\"):\n",
    "        self.llm = ChatOpenAI(model=model, temperature=0.7)\n",
    "        self.history = []\n",
    "    \n",
    "    def stream_response(self, user_input: str):\n",
    "        \"\"\"Stream a response and maintain conversation history.\"\"\"\n",
    "        # Add user message to history\n",
    "        self.history.append(HumanMessage(content=user_input))\n",
    "        \n",
    "        # Stream response\n",
    "        full_response = \"\"\n",
    "        for chunk in self.llm.stream(self.history):\n",
    "            content = chunk.content\n",
    "            full_response += content\n",
    "            print(content, end=\"\", flush=True)\n",
    "        \n",
    "        print()  # New line after streaming\n",
    "        \n",
    "        # Add AI response to history\n",
    "        self.history.append(AIMessage(content=full_response))\n",
    "        \n",
    "        return full_response\n",
    "\n",
    "# Demo the assistant\n",
    "assistant = StreamingAssistant()\n",
    "\n",
    "print(\"Assistant: \", end=\"\")\n",
    "assistant.stream_response(\"Hi! What is streaming in LLMs?\")\n",
    "\n",
    "print(\"\\nAssistant: \", end=\"\")\n",
    "assistant.stream_response(\"Can you give me a practical example?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Streaming Best Practices\n",
    "\n",
    "Key patterns for production streaming applications:\n",
    "\n",
    "1. **Always use async in production**: `astream()` is more efficient than `stream()`\n",
    "2. **Handle errors gracefully**: Wrap streams in try-except blocks\n",
    "3. **Implement timeout mechanisms**: Prevent infinite streaming\n",
    "4. **Use Server-Sent Events (SSE)**: Standard format for HTTP streaming\n",
    "5. **Buffer chunks appropriately**: Balance between latency and throughput\n",
    "6. **Monitor token usage**: Track costs even during streaming\n",
    "7. **Implement cancellation**: Allow users to stop generation\n",
    "8. **Test streaming thoroughly**: Ensure chunks are delivered correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. **Why streaming matters**: Low latency and better UX\n",
    "2. **Basic streaming**: `invoke()` vs `stream()`\n",
    "3. **Chain streaming**: Using `stream()` with LCEL chains\n",
    "4. **Agent streaming**: Streaming tool calls and reasoning\n",
    "5. **Advanced streaming**: `stream_events()` for fine-grained control\n",
    "6. **Async streaming**: `astream()` for production use\n",
    "7. **Production patterns**: FastAPI streaming endpoint example\n",
    "8. **Best practices**: Guidelines for building robust streaming apps\n",
    "\n",
    "Streaming is essential for building responsive, production-ready LLM applications. Start with basic `stream()` for prototypes, then move to `astream()` for production deployments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
