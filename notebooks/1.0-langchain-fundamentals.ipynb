{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Fundamentals: Agents, Messages & Streaming\n",
    "\n",
    "This notebook covers the core building blocks of LangChain 1.0:\n",
    "\n",
    "1. **Building Agents** - The `create_agent()` API for building AI agents\n",
    "2. **Messages** - The fundamental unit of context for LLM communication  \n",
    "3. **Streaming** - Reducing latency by streaming responses\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-community langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Building Agents with `create_agent()`\n",
    "\n",
    "In LangChain 1.0, the `create_agent()` function is the primary way to build AI agents. It provides a clean, declarative API that handles:\n",
    "\n",
    "- Model selection and configuration\n",
    "- Tool binding\n",
    "- System prompts\n",
    "- Runtime context (dependency injection)\n",
    "- Middleware (like human-in-the-loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Your First Agent\n",
    "\n",
    "Let's create a simple agent that can answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Create a simple agent with a system prompt\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    system_prompt=\"You are a helpful assistant that explains concepts clearly and concisely.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the agent with a question\n",
    "result = agent.invoke({\"messages\": \"What is LangChain?\"})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Agent with Tools\n",
    "\n",
    "Agents become powerful when they can use tools. Let's create a SQL agent that can query a database.\n",
    "\n",
    "First, we'll set up a SQLite database and define a runtime context for dependency injection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Load the Chinook sample database (music store data)\n",
    "db = SQLDatabase.from_uri(\"sqlite:///./assets-resources/Chinook.db\")\n",
    "\n",
    "# Define runtime context for dependency injection\n",
    "@dataclass\n",
    "class RuntimeContext:\n",
    "    db: SQLDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.runtime import get_runtime\n",
    "\n",
    "@tool\n",
    "def execute_sql(query: str) -> str:\n",
    "    \"\"\"Execute a SQLite SELECT query and return results.\"\"\"\n",
    "    runtime = get_runtime(RuntimeContext)\n",
    "    db = runtime.context.db\n",
    "    \n",
    "    try:\n",
    "        return db.run(query)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a careful SQLite analyst.\n",
    "\n",
    "Rules:\n",
    "- Think step-by-step.\n",
    "- When you need data, call the tool `execute_sql` with ONE SELECT query.\n",
    "- Read-only only; no INSERT/UPDATE/DELETE/ALTER/DROP/CREATE.\n",
    "- Limit to 5 rows unless the user explicitly asks otherwise.\n",
    "- If the tool returns 'Error:', revise the SQL and try again.\n",
    "- Prefer explicit column lists; avoid SELECT *.\n",
    "\"\"\"\n",
    "\n",
    "sql_agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[execute_sql],\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    context_schema=RuntimeContext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the agent's ReAct loop\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(sql_agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a query - the agent will discover the schema and answer\n",
    "question = \"Which table has the largest number of entries?\"\n",
    "\n",
    "for step in sql_agent.stream(\n",
    "    {\"messages\": question},\n",
    "    context=RuntimeContext(db=db),\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another query - notice the agent self-corrects on errors\n",
    "question = \"Which genre on average has the longest tracks?\"\n",
    "\n",
    "for step in sql_agent.stream(\n",
    "    {\"messages\": question},\n",
    "    context=RuntimeContext(db=db),\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Your Own Query\n",
    "\n",
    "Modify the question below to explore the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the top 5 customers by total spending?\"\n",
    "\n",
    "for step in sql_agent.stream(\n",
    "    {\"messages\": question},\n",
    "    context=RuntimeContext(db=db),\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Messages - The Communication Backbone\n",
    "\n",
    "Messages are the fundamental unit of context in LangChain. They represent the input and output of models, carrying both content and metadata needed to represent conversation state.\n",
    "\n",
    "## Message Types\n",
    "\n",
    "- **HumanMessage**: Input from users\n",
    "- **AIMessage**: Responses from the model\n",
    "- **SystemMessage**: Instructions for the model\n",
    "- **ToolMessage**: Results from tool executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\", \n",
    "    system_prompt=\"You are a helpful coding assistant.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using HumanMessage explicitly\n",
    "human_msg = HumanMessage(\"Explain what a decorator is in Python in one sentence.\")\n",
    "\n",
    "result = agent.invoke({\"messages\": [human_msg]})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the message types\n",
    "for msg in result[\"messages\"]:\n",
    "    print(f\"{msg.type}: {msg.content[:80]}...\" if len(msg.content) > 80 else f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Alternative Message Formats\n",
    "\n",
    "LangChain supports multiple ways to specify messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simple string (inferred as HumanMessage)\n",
    "result = agent.invoke({\"messages\": \"What is a list comprehension?\"})\n",
    "print(\"String input:\", result[\"messages\"][-1].content[:100], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Dictionary format\n",
    "result = agent.invoke(\n",
    "    {\"messages\": {\"role\": \"user\", \"content\": \"What is a generator?\"}}\n",
    ")\n",
    "print(\"Dict input:\", result[\"messages\"][-1].content[:100], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tool Messages in Action\n",
    "\n",
    "When an agent uses tools, ToolMessages capture the results. Let's see this with a haiku-checking tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def check_haiku_lines(text: str) -> str:\n",
    "    \"\"\"Check if the given haiku text has exactly 3 lines.\n",
    "    Returns validation result.\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in text.strip().splitlines() if line.strip()]\n",
    "    print(f\"Checking haiku with {len(lines)} lines\")\n",
    "    \n",
    "    if len(lines) != 3:\n",
    "        return f\"Incorrect! This haiku has {len(lines)} lines. A haiku must have exactly 3 lines.\"\n",
    "    return \"Correct! This haiku has 3 lines.\"\n",
    "\n",
    "haiku_agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[check_haiku_lines],\n",
    "    system_prompt=\"You are a poet who only writes Haiku. Always check your work before presenting it.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = haiku_agent.invoke({\"messages\": \"Write me a haiku about programming\"})\n",
    "\n",
    "# View all messages including tool calls\n",
    "for msg in result[\"messages\"]:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Message Metadata\n",
    "\n",
    "Messages contain rich metadata about model usage, tokens, and more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final AI message\n",
    "final_message = result[\"messages\"][-1]\n",
    "\n",
    "print(\"Content:\", final_message.content)\n",
    "print(\"\\nUsage metadata:\", final_message.usage_metadata)\n",
    "print(\"\\nResponse metadata keys:\", list(final_message.response_metadata.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Streaming - Reducing Latency\n",
    "\n",
    "Streaming delivers information to users before the final result is ready. LangChain supports multiple streaming modes:\n",
    "\n",
    "- **`values`**: Stream complete state updates after each node\n",
    "- **`messages`**: Stream token-by-token (lowest latency)\n",
    "- **`custom`**: Stream arbitrary data from tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 No Streaming (invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without streaming - waits for complete response\n",
    "result = agent.invoke({\"messages\": \"Tell me a short joke\"})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Stream Mode: `values`\n",
    "\n",
    "Streams the complete state after each step in the agent's execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in agent.stream(\n",
    "    {\"messages\": \"Tell me a dad joke\"},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Stream Mode: `messages` (Token-by-Token)\n",
    "\n",
    "The lowest latency option - perfect for chatbots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, metadata in agent.stream(\n",
    "    {\"messages\": \"Write a short poem about Python programming.\"},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Custom Streaming from Tools\n",
    "\n",
    "You can stream custom data from within your tools using `get_stream_writer()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.config import get_stream_writer\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    writer = get_stream_writer()\n",
    "    \n",
    "    # Stream progress updates\n",
    "    writer(f\"Looking up weather data for {city}...\")\n",
    "    writer(f\"Connecting to weather service...\")\n",
    "    writer(f\"Data retrieved for {city}!\")\n",
    "    \n",
    "    return f\"It's sunny and 72Â°F in {city}!\"\n",
    "\n",
    "weather_agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[get_weather],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream both values and custom data\n",
    "for chunk in weather_agent.stream(\n",
    "    {\"messages\": \"What's the weather in San Francisco?\"},\n",
    "    stream_mode=[\"values\", \"custom\"],\n",
    "):\n",
    "    stream_type, data = chunk\n",
    "    if stream_type == \"custom\":\n",
    "        print(f\"[Progress] {data}\")\n",
    "    elif stream_type == \"values\":\n",
    "        # Print only the final answer\n",
    "        last_msg = data[\"messages\"][-1]\n",
    "        if hasattr(last_msg, 'content') and last_msg.content:\n",
    "            print(f\"[Answer] {last_msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream only custom progress updates\n",
    "print(\"Custom updates only:\")\n",
    "for chunk in weather_agent.stream(\n",
    "    {\"messages\": \"What's the weather in Tokyo?\"},\n",
    "    stream_mode=[\"custom\"],\n",
    "):\n",
    "    print(f\"  {chunk[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. **`create_agent()`** - The unified API for building agents with:\n",
    "   - Model selection (`model=\"openai:gpt-4o-mini\"`)\n",
    "   - Tool binding (`tools=[...]`)\n",
    "   - System prompts\n",
    "   - Runtime context for dependency injection\n",
    "\n",
    "2. **Messages** - The communication backbone:\n",
    "   - `HumanMessage`, `AIMessage`, `SystemMessage`, `ToolMessage`\n",
    "   - Multiple input formats (string, dict, Message objects)\n",
    "   - Rich metadata (usage, tokens, model info)\n",
    "\n",
    "3. **Streaming** - Reducing latency:\n",
    "   - `values`: Complete state after each step\n",
    "   - `messages`: Token-by-token streaming\n",
    "   - `custom`: Arbitrary data from tools\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [Notebook 2: Tools and Memory](./2.0-tools-and-memory.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
