{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 LangChain 1.0 Middleware & Modern Agent Patterns\n",
    "\n",
    "This notebook showcases the **brand new features in LangChain 1.0**, including:\n",
    "\n",
    "- **`create_agent`**: The new unified API for building agents\n",
    "- **Middleware System**: Hook into the agent loop for human-in-the-loop, summarization, and custom logic\n",
    "- **Structured Outputs with ToolStrategy**: Constrain agent responses to specific schemas\n",
    "- **Model String Format**: Simplified model initialization\n",
    "\n",
    "These features represent a major evolution in how we build LLM applications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain>=1.0.0 langgraph>=1.0.0\n",
    "%pip install -qU langchain-openai langchain-anthropic\n",
    "%pip install -qU langchain-tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-1.0-middleware-demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The New `create_agent` API\n",
    "\n",
    "LangChain 1.0 introduces `create_agent` - a unified way to create agents that replaces:\n",
    "- `AgentExecutor`\n",
    "- `create_react_agent`\n",
    "- `create_tool_calling_agent`\n",
    "- `langgraph.prebuilt.create_react_agent`\n",
    "\n",
    "### Key Benefits:\n",
    "1. **Simpler API**: Single function for all agent types\n",
    "2. **Model Strings**: Use `\"openai:gpt-4o-mini\"` instead of instantiating classes\n",
    "3. **Built-in Middleware**: Human-in-the-loop, summarization, etc.\n",
    "4. **Returns a Graph**: Full LangGraph `CompiledStateGraph` under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the current weather for a location.\"\"\"\n",
    "    # Simulated weather data\n",
    "    weather_data = {\n",
    "        \"san francisco\": \"62°F, foggy\",\n",
    "        \"new york\": \"75°F, sunny\",\n",
    "        \"london\": \"55°F, rainy\",\n",
    "        \"tokyo\": \"82°F, humid\",\n",
    "    }\n",
    "    return weather_data.get(location.lower(), f\"Weather data unavailable for {location}\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate a mathematical expression.\"\"\"\n",
    "    try:\n",
    "        # Safe evaluation of math expressions\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "# Create an agent with the new API - note the model string format!\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",  # Provider:model format\n",
    "    tools=[get_weather, calculate],\n",
    "    system_prompt=\"You are a helpful assistant. Be concise in your responses.\"\n",
    ")\n",
    "\n",
    "print(f\"Agent type: {type(agent)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the agent - uses messages format\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]\n",
    "})\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation\n",
    "result = agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The weather in San Francisco is 62°F and foggy.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What about New York? And how much warmer is it there?\"}\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream the response\n",
    "print(\"Streaming response:\")\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Calculate (25 * 4) + 17\"}]},\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Web Search Agent with TavilySearch\n",
    "\n",
    "Let's create a research agent that can search the web using the new `langchain-tavily` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "# Create the Tavily search tool with configuration\n",
    "tavily_search = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    "    include_answer=True,  # Get a direct answer when possible\n",
    ")\n",
    "\n",
    "# Create a research agent\n",
    "research_agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[tavily_search],\n",
    "    system_prompt=\"\"\"You are a research assistant. \n",
    "    When answering questions:\n",
    "    1. Use the search tool to find current information\n",
    "    2. Cite your sources with URLs\n",
    "    3. Be concise but comprehensive\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research a current topic\n",
    "result = research_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What are the key features of LangChain 1.0?\"}]\n",
    "})\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Middleware System\n",
    "\n",
    "LangChain 1.0's middleware system lets you hook into the agent loop at three points:\n",
    "\n",
    "| Hook | Timing | Use Cases |\n",
    "|------|--------|----------|\n",
    "| `before_model` | Before each model call | Logging, rate limiting, context injection |\n",
    "| `after_model` | After each model call | Response filtering, metrics collection |\n",
    "| `modify_model_request` | Before model call | Modify tools, prompts, or model settings |\n",
    "\n",
    "### Built-in Middleware:\n",
    "- **SummarizationMiddleware**: Compress long conversations\n",
    "- **HumanInTheLoopMiddleware**: Require approval before tool execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Summarization Middleware\n",
    "\n",
    "Automatically compresses conversation history when it gets too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "\n",
    "# Create agent with summarization middleware\n",
    "summarizing_agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[get_weather, calculate],\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=\"openai:gpt-4o-mini\",  # Model for summarization\n",
    "            trigger=(\"tokens\", 4000),     # Trigger when > 4000 tokens\n",
    "            keep=(\"messages\", 20),        # Keep last 20 messages\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Agent with summarization created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a conversation (summarization triggers automatically)\n",
    "result = summarizing_agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather in all the cities you know about?\"}\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Custom Middleware\n",
    "\n",
    "Let's create a custom middleware that logs all tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import AgentMiddleware\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class LoggingMiddleware(AgentMiddleware):\n",
    "    \"\"\"Middleware that logs all model calls and tool invocations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.call_count = 0\n",
    "        self.tool_calls = []\n",
    "    \n",
    "    def before_model(self, state, config):\n",
    "        \"\"\"Called before each model invocation.\"\"\"\n",
    "        self.call_count += 1\n",
    "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Model call #{self.call_count}\")\n",
    "        print(f\"  Messages: {len(state.get('messages', []))}\")\n",
    "        return None  # Return None to continue normally\n",
    "    \n",
    "    def after_model(self, state, config):\n",
    "        \"\"\"Called after each model invocation.\"\"\"\n",
    "        last_message = state.get('messages', [])[-1] if state.get('messages') else None\n",
    "        if last_message and hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            for tc in last_message.tool_calls:\n",
    "                self.tool_calls.append(tc['name'])\n",
    "                print(f\"  Tool called: {tc['name']}\")\n",
    "        return None\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            \"total_calls\": self.call_count,\n",
    "            \"tool_calls\": self.tool_calls\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logging middleware instance\n",
    "logging_middleware = LoggingMiddleware()\n",
    "\n",
    "# Create agent with custom middleware\n",
    "logged_agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[get_weather, calculate],\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    "    middleware=[logging_middleware],\n",
    ")\n",
    "\n",
    "# Run some queries\n",
    "result = logged_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Tokyo and what's 100 * 15?\"}]\n",
    "})\n",
    "\n",
    "print(f\"\\nFinal answer: {result['messages'][-1].content}\")\n",
    "print(f\"\\nStats: {logging_middleware.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Structured Outputs with ToolStrategy\n",
    "\n",
    "LangChain 1.0 integrates structured outputs directly into the agent loop using `response_format`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "\n",
    "class WeatherReport(BaseModel):\n",
    "    \"\"\"Structured weather report.\"\"\"\n",
    "    location: str = Field(description=\"The location queried\")\n",
    "    temperature: str = Field(description=\"Current temperature\")\n",
    "    conditions: str = Field(description=\"Weather conditions\")\n",
    "    recommendation: str = Field(description=\"What to wear or bring\")\n",
    "\n",
    "\n",
    "# Create agent with structured output\n",
    "structured_agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a weather assistant. Always provide practical recommendations.\",\n",
    "    response_format=ToolStrategy(WeatherReport),  # Constrain output!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get structured weather report\n",
    "result = structured_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}]\n",
    "})\n",
    "\n",
    "# The response is now structured!\n",
    "response = result[\"response\"]\n",
    "print(f\"Location: {response.location}\")\n",
    "print(f\"Temperature: {response.temperature}\")\n",
    "print(f\"Conditions: {response.conditions}\")\n",
    "print(f\"Recommendation: {response.recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Persistence and Memory\n",
    "\n",
    "Add persistence to maintain conversation state across invocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Create a checkpointer for memory\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Create agent with memory\n",
    "memory_agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[get_weather, calculate],\n",
    "    system_prompt=\"You are a helpful assistant. Remember details from our conversation.\",\n",
    "    checkpointer=memory,  # Enable persistence!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration with thread_id for session management\n",
    "config = {\"configurable\": {\"thread_id\": \"user-123\"}}\n",
    "\n",
    "# First message\n",
    "result = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is Alice and I live in Tokyo.\"}]},\n",
    "    config=config\n",
    ")\n",
    "print(f\"Response 1: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up - the agent remembers!\n",
    "result = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather where I live?\"}]},\n",
    "    config=config\n",
    ")\n",
    "print(f\"Response 2: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yet another follow-up\n",
    "result = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name again?\"}]},\n",
    "    config=config\n",
    ")\n",
    "print(f\"Response 3: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Multi-Provider Support\n",
    "\n",
    "The model string format makes it easy to switch providers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agents with different providers using the same API\n",
    "\n",
    "# OpenAI agent\n",
    "openai_agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a weather assistant.\"\n",
    ")\n",
    "\n",
    "# You can also use Anthropic (if you have ANTHROPIC_API_KEY set)\n",
    "# anthropic_agent = create_agent(\n",
    "#     model=\"anthropic:claude-sonnet-4-5-20250929\",\n",
    "#     tools=[get_weather],\n",
    "#     system_prompt=\"You are a weather assistant.\"\n",
    "# )\n",
    "\n",
    "print(\"Agents created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OpenAI agent\n",
    "result = openai_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Weather in London please!\"}]\n",
    "})\n",
    "print(f\"OpenAI: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Complete Example - Research Assistant\n",
    "\n",
    "Let's put it all together with a research assistant that:\n",
    "- Searches the web\n",
    "- Has conversation memory\n",
    "- Logs all actions\n",
    "- Returns structured responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ResearchSummary(BaseModel):\n",
    "    \"\"\"Structured research summary.\"\"\"\n",
    "    topic: str = Field(description=\"The research topic\")\n",
    "    key_findings: List[str] = Field(description=\"Main findings (3-5 bullet points)\")\n",
    "    sources: List[str] = Field(description=\"URLs of sources used\")\n",
    "    confidence: str = Field(description=\"Confidence level: high/medium/low\")\n",
    "\n",
    "\n",
    "# Create comprehensive research agent\n",
    "research_assistant = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[tavily_search],\n",
    "    system_prompt=\"\"\"You are a research assistant. When researching:\n",
    "    1. Search for multiple perspectives\n",
    "    2. Verify information from multiple sources\n",
    "    3. Note your confidence level based on source quality\n",
    "    4. Always cite your sources\"\"\",\n",
    "    checkpointer=MemorySaver(),\n",
    "    middleware=[LoggingMiddleware()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research a topic\n",
    "config = {\"configurable\": {\"thread_id\": \"research-session-1\"}}\n",
    "\n",
    "result = research_assistant.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What are the main differences between LangChain and LangGraph?\"}]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up question\n",
    "result = research_assistant.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Based on what you found, which should I use for a simple chatbot?\"}]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: LangChain 1.0 Key Changes\n",
    "\n",
    "### What's New\n",
    "\n",
    "| Feature | Pre-v1 | LangChain 1.0 |\n",
    "|---------|--------|---------------|\n",
    "| Agent Creation | `AgentExecutor`, `create_react_agent` | `create_agent()` |\n",
    "| Model Init | `ChatOpenAI(model=\"gpt-4o-mini\")` | `\"openai:gpt-4o-mini\"` |\n",
    "| Middleware | Not available | `SummarizationMiddleware`, custom middleware |\n",
    "| Structured Output | Separate chains | `response_format=ToolStrategy(Schema)` |\n",
    "| Tavily Search | `TavilySearchResults` (community) | `TavilySearch` (langchain-tavily) |\n",
    "| State Schema | Pydantic, dataclass | TypedDict only |\n",
    "\n",
    "### Migration Checklist\n",
    "\n",
    "1. ✅ Replace `AgentExecutor` with `create_agent()`\n",
    "2. ✅ Use model string format: `\"openai:gpt-4o-mini\"`\n",
    "3. ✅ Install `langchain-tavily` for search tools\n",
    "4. ✅ Use `TypedDict` for state schemas\n",
    "5. ✅ Add middleware for cross-cutting concerns\n",
    "6. ✅ Use `response_format` for structured outputs\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [LangChain 1.0 Migration Guide](https://docs.langchain.com/oss/python/migrate/langchain-v1)\n",
    "- [LangChain 1.0 Blog Post](https://blog.langchain.com/langchain-langgraph-1dot0/)\n",
    "- [Built-in Middleware Docs](https://docs.langchain.com/oss/python/langchain/middleware/built-in)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
