{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "this notebook was slightly adapted from this notebook from the LangChain team: https://github.com/langchain-ai/langchain-nvidia/blob/main/cookbook/structured_report_generation.ipynb\n",
        "\n",
        "# 4.0 Structured Research Report Generation\n",
        "In this notebook, you will use the OpenAI GPT-4o-mini model, to generate a report on a given topic. You will use Langchain's LangGraph to build an Agent that takes in user-defined topics and structure, then plans the topics of the section indicated in the structure. Next, the Agent uses Tavily to do web search on the given topics and uses this information to write the sections and synthesize the final report. \n",
        "\n",
        "You leverage OpenAI API. As you don't need a GPU to run the model, you can run this notebook anywhere!\n",
        "\n",
        "You can find the original notebook on LangChain's GitHub [here](https://github.com/langchain-ai/report-mAIstro).\n",
        "\n",
        "Below is the architecture diagram.\n",
        "\n",
        "## Architecture Diagram\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[User] -->|Topic and Structure Prompts| B[Report-Planning Agent]\n",
        "    \n",
        "    B --> C{Report Sections}\n",
        "    C --> C1[Intro]\n",
        "    C --> C2[Body] \n",
        "    C --> C3[Conclusion]\n",
        "    \n",
        "    C1 --> D[OpenAI GPT-4o-mini Final Report Generation]\n",
        "    C2 --> D\n",
        "    C3 --> D\n",
        "    \n",
        "    D --> E[Final Report]\n",
        "    \n",
        "    %% Detailed section generation\n",
        "    C2 --> F[Research Agent - Intro]\n",
        "    C2 --> G[Research Agent - Body]\n",
        "    C2 --> H[Research Agent - Conclusion]\n",
        "    \n",
        "    F --> I[OpenAI GPT-4o-mini Plan and Research]\n",
        "    G --> I\n",
        "    H --> I\n",
        "    \n",
        "    I -->|Queries| J[Tavily Web Search]\n",
        "    J --> K[Report Sections]\n",
        "    \n",
        "    K --> L[Report-Writing Agent - Intro]\n",
        "    K --> M[Report-Writing Agent - Body]\n",
        "    K --> N[Report-Writing Agent - Conclusion]\n",
        "    \n",
        "    L --> O[OpenAI GPT-4o-mini Sequential Writing]\n",
        "    M --> O\n",
        "    N --> O\n",
        "    \n",
        "    O --> P[Section Content]\n",
        "    P --> D\n",
        "    \n",
        "    style A fill:#87CEEB\n",
        "    style B fill:#FFD700\n",
        "    style I fill:#98FB98\n",
        "    style O fill:#98FB98\n",
        "    style J fill:#40E0D0\n",
        "    style E fill:#FFA500\n",
        "    style P fill:#FFA500\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Content Overview\n",
        ">[Prerequisites](#Prerequisites)  \n",
        ">[Writing Report Plan](#Writing-Report-Plan)  \n",
        ">[Research and Writing](#Research-and-Writing)  \n",
        ">[Write Single Section](#Write-Single-Section)  \n",
        ">[Validate Single Section](#Validate-Single-Section)  \n",
        ">[Write All Sections](#Write-All-Sections)  \n",
        ">[Validate Final Report](#Final-Report)\n",
        "________________________\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain_community langchain_core tavily-python langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys\n",
        "Prior to getting started, you will need to create API Keys for OpenAI, Tavily, and LangChain.\n",
        "\n",
        "- OpenAI API Key\n",
        "  1. Go to the [OpenAI API Keys page](https://platform.openai.com/api-keys).\n",
        "  2. If you don't have an OpenAI account, you will be asked to sign-up.\n",
        "  3. Click \"Create new secret key\" to generate an API key\n",
        "- LangChain\n",
        "  1. Go to **[LangChain Settings page](https://smith.langchain.com/settings)**. You will need to create an account if you have not already.\n",
        "  2. On the left panel, navigate to \"API Keys\".\n",
        "  3. Click on the \"Create API Key\" on the top right of the page.\n",
        "- Tavily\n",
        "  1. Go to the **[Tavily homepage](https://tavily.com/)** and click on \"Get Started\"\n",
        "  2. Sign in or create an account.\n",
        "  3. Create an API Key.\n",
        "\n",
        "### Export API Keys\n",
        "\n",
        "Save these API Keys as environment variables.\n",
        "\n",
        "First, set the OpenAI API Key as the environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"):\n",
        "    openai_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "    assert openai_key.startswith(\"sk-\"), f\"{openai_key[:5]}... is not a valid key\"\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "\n",
        "Next, set the LangChain API Key as an environment variable. You will use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "        \n",
        "_set_env(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"structured-research-report\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, set the Tavily API Key as an environment variable. You will use [Tavily API](https://tavily.com/) web search tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "_set_env(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tavily import TavilyClient, AsyncTavilyClient\n",
        "tavily_client = TavilyClient()\n",
        "tavily_async_client = AsyncTavilyClient()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Working with the OpenAI API\n",
        "\n",
        "Let's test the API endpoint.\n",
        "\n",
        "In this notebook, you will use the OpenAI GPT-4o-mini model as the LLM. Define the LLM below and test the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**The Ballad of LangChain**\n",
            "\n",
            "In the realm of code where the data flows,  \n",
            "A tale of LangChain, the world now knows.  \n",
            "With whispers of wisdom, it weaves through the night,  \n",
            "A tapestry bright, where the future takes flight.  \n",
            "\n",
            "In the heart of the code, where the models reside,  \n",
            "LangChain emerged, a powerful guide.  \n",
            "With chains of connection, it bridges the gaps,  \n",
            "Uniting the knowledge, like stars in their maps.  \n",
            "\n",
            "Oh, LangChain, dear friend, with your elegant grace,  \n",
            "You harness the language, you quicken the pace.  \n",
            "From prompts to the answers, you dance with delight,  \n",
            "Transforming the chaos, bringing order to light.  \n",
            "\n",
            "With tools at your fingertips, you craft and you mold,  \n",
            "A symphony of logic, a story retold.  \n",
            "From chatbots to agents, you open the door,  \n",
            "To realms of creation, where ideas can soar.  \n",
            "\n",
            "In the halls of the thinkers, the dreamers, the doers,  \n",
            "You stand as a beacon, guiding the pursuers.  \n",
            "With every new feature, you spark innovation,  \n",
            "A catalyst bright for the next generation.  \n",
            "\n",
            "So gather, dear coders, and sing of the chain,  \n",
            "Of LangChain's great journey, through joy and through pain.  \n",
            "For in every line written, in every thought shared,  \n",
            "Lies the spirit of progress, a future prepared.  \n",
            "\n",
            "Through valleys of data, through mountains of text,  \n",
            "LangChain leads the way, with a vision perplexed.  \n",
            "In the world of AI, where the language is king,  \n",
            "We celebrate the magic that LangChain can bring.  \n",
            "\n",
            "So raise up your voices, let the chorus resound,  \n",
            "For LangChain, our hero, in code we have found.  \n",
            "With each passing moment, as the world starts to change,  \n",
            "We honor the legacy of the mighty LangChain.  \n"
          ]
        }
      ],
      "source": [
        "## Core LC Chat Interface\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(model=\"gpt-4o-mini\", temperature=0)\n",
        "result = llm.invoke(\"Write a ballad about LangChain.\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenAI API Usage\n",
        "\n",
        "This notebook uses the OpenAI API for all language model functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The OpenAI API provides reliable, scalable access to state-of-the-art language models including GPT-4o-mini used in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# connect to an embedding NIM running at localhost:8000, specifying a model\n",
        "llm = init_chat_model(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Writing Report Plan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Utils Functions\n",
        "\n",
        "Next, you will create Utility functions that will be used for web research during report generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from langsmith import traceable\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Section(BaseModel):\n",
        "    name: str = Field(\n",
        "        description=\"Name for this section of the report.\",\n",
        "    )\n",
        "    description: str = Field(\n",
        "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
        "    )\n",
        "    research: bool = Field(\n",
        "        description=\"Whether to perform web research for this section of the report.\"\n",
        "    )\n",
        "    content: str = Field(\n",
        "        description=\"The content of the section.\"\n",
        "    ) \n",
        "\n",
        "def deduplicate_and_format_sources(search_response, max_tokens_per_source, include_raw_content=True):\n",
        "    \"\"\"\n",
        "    Takes either a single search response or list of responses from Tavily API and formats them.\n",
        "    Limits the raw_content to approximately max_tokens_per_source.\n",
        "    include_raw_content specifies whether to include the raw_content from Tavily in the formatted string.\n",
        "    \n",
        "    Args:\n",
        "        search_response: Either:\n",
        "            - A dict with a 'results' key containing a list of search results\n",
        "            - A list of dicts, each containing search results\n",
        "            \n",
        "    Returns:\n",
        "        str: Formatted string with deduplicated sources\n",
        "    \"\"\"\n",
        "    # Convert input to list of results\n",
        "    if isinstance(search_response, dict):\n",
        "        sources_list = search_response['results']\n",
        "    elif isinstance(search_response, list):\n",
        "        sources_list = []\n",
        "        for response in search_response:\n",
        "            if isinstance(response, dict) and 'results' in response:\n",
        "                sources_list.extend(response['results'])\n",
        "            else:\n",
        "                sources_list.extend(response)\n",
        "    else:\n",
        "        raise ValueError(\"Input must be either a dict with 'results' or a list of search results\")\n",
        "    \n",
        "    # Deduplicate by URL\n",
        "    unique_sources = {}\n",
        "    for source in sources_list:\n",
        "        if source['url'] not in unique_sources:\n",
        "            unique_sources[source['url']] = source\n",
        "    \n",
        "    # Format output\n",
        "    formatted_text = \"Sources:\\n\\n\"\n",
        "    for i, source in enumerate(unique_sources.values(), 1):\n",
        "        formatted_text += f\"Source {source['title']}:\\n===\\n\"\n",
        "        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n",
        "        formatted_text += f\"Most relevant content from source: {source['content']}\\n===\\n\"\n",
        "        if include_raw_content:\n",
        "            # Using rough estimate of 4 characters per token\n",
        "            char_limit = max_tokens_per_source * 4\n",
        "            # Handle None raw_content\n",
        "            raw_content = source.get('raw_content', '')\n",
        "            if raw_content is None:\n",
        "                raw_content = ''\n",
        "                print(f\"Warning: No raw_content found for source {source['url']}\")\n",
        "            if len(raw_content) > char_limit:\n",
        "                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n",
        "            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n",
        "                \n",
        "    return formatted_text.strip()\n",
        "\n",
        "def format_sections(sections: list[Section]) -> str:\n",
        "    \"\"\" Format a list of sections into a string \"\"\"\n",
        "    formatted_str = \"\"\n",
        "    for idx, section in enumerate(sections, 1):\n",
        "        formatted_str += f\"\"\"\n",
        "{'='*60}\n",
        "Section {idx}: {section.name}\n",
        "{'='*60}\n",
        "Description:\n",
        "{section.description}\n",
        "Requires Research: \n",
        "{section.research}\n",
        "\n",
        "Content:\n",
        "{section.content if section.content else '[Not yet written]'}\n",
        "\n",
        "\"\"\"\n",
        "    return formatted_str\n",
        "\n",
        "@traceable\n",
        "def tavily_search(query):\n",
        "    \"\"\" Search the web using the Tavily API.\n",
        "    \n",
        "    Args:\n",
        "        query (str): The search query to execute\n",
        "        \n",
        "    Returns:\n",
        "        dict: Tavily search response containing:\n",
        "            - results (list): List of search result dictionaries, each containing:\n",
        "                - title (str): Title of the search result\n",
        "                - url (str): URL of the search result\n",
        "                - content (str): Snippet/summary of the content\n",
        "                - raw_content (str): Full content of the page if available\"\"\"\n",
        "     \n",
        "    return tavily_client.search(query, \n",
        "                         max_results=5, \n",
        "                         include_raw_content=True)\n",
        "\n",
        "@traceable\n",
        "async def tavily_search_async(search_queries, tavily_topic, tavily_days):\n",
        "    \"\"\"\n",
        "    Performs concurrent web searches using the Tavily API.\n",
        "\n",
        "    Args:\n",
        "        search_queries (List[SearchQuery]): List of search queries to process\n",
        "        tavily_topic (str): Type of search to perform ('news' or 'general')\n",
        "        tavily_days (int): Number of days to look back for news articles (only used when tavily_topic='news')\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: List of search results from Tavily API, one per query\n",
        "\n",
        "    Note:\n",
        "        For news searches, each result will include articles from the last `tavily_days` days.\n",
        "        For general searches, the time range is unrestricted.\n",
        "    \"\"\"\n",
        "    \n",
        "    search_tasks = []\n",
        "    for query in search_queries:\n",
        "        if tavily_topic == \"news\":\n",
        "            search_tasks.append(\n",
        "                tavily_async_client.search(\n",
        "                    query,\n",
        "                    max_results=5,\n",
        "                    include_raw_content=True,\n",
        "                    topic=\"news\",\n",
        "                    days=tavily_days\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            search_tasks.append(\n",
        "                tavily_async_client.search(\n",
        "                    query,\n",
        "                    max_results=5,\n",
        "                    include_raw_content=True,\n",
        "                    topic=\"general\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "    # Execute all searches concurrently\n",
        "    search_docs = await asyncio.gather(*search_tasks)\n",
        "\n",
        "    return search_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Planning\n",
        "\n",
        "First, let's define the Schema for report sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from typing import  Annotated, List, Optional, Literal\n",
        "  \n",
        "class Sections(BaseModel):\n",
        "    sections: List[Section] = Field(\n",
        "        description=\"Sections of the report.\",\n",
        "    )\n",
        "class SearchQuery(BaseModel):\n",
        "    search_query: str = Field(\n",
        "        None, description=\"Query for web search.\"\n",
        "    )\n",
        "class Queries(BaseModel):\n",
        "    queries: List[SearchQuery] = Field(\n",
        "        description=\"List of search queries.\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you will define the LangGraph state. Each state will have the following fields. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import operator\n",
        "\n",
        "class ReportState(TypedDict):\n",
        "    topic: str # Report topic\n",
        "    tavily_topic: Literal[\"general\", \"news\"] # Tavily search topic\n",
        "    tavily_days: Optional[int] # Only applicable for news topic\n",
        "    report_structure: str # Report structure\n",
        "    number_of_queries: int # Number web search queries to perform per section    \n",
        "    sections: list[Section] # List of report sections \n",
        "    completed_sections: Annotated[list, operator.add] # Send() API key\n",
        "    report_sections_from_research: str # String of any completed sections from research to write final sections\n",
        "    final_report: str # Final report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next you will write the report planner instructions, and a function that will generate the report sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Prompt to generate a search query to help with planning the report outline\n",
        "report_planner_query_writer_instructions=\"\"\"You are an expert technical writer, helping to plan a report. \n",
        "\n",
        "The report will be focused on the following topic:\n",
        "\n",
        "{topic}\n",
        "\n",
        "The report structure will follow these guidelines:\n",
        "\n",
        "{report_organization}\n",
        "\n",
        "Your goal is to generate {number_of_queries} search queries that will help gather comprehensive information for planning the report sections. \n",
        "\n",
        "The query should:\n",
        "\n",
        "1. Be related to the topic \n",
        "2. Help satisfy the requirements specified in the report organization\n",
        "\n",
        "Make the query specific enough to find high-quality, relevant sources while covering the breadth needed for the report structure.\"\"\"\n",
        "\n",
        "# Prompt generating the report outline\n",
        "report_planner_instructions=\"\"\"You are an expert technical writer, helping to plan a report.\n",
        "\n",
        "Your goal is to generate the outline of the sections of the report. \n",
        "\n",
        "The overall topic of the report is:\n",
        "\n",
        "{topic}\n",
        "\n",
        "The report should follow this organization: \n",
        "\n",
        "{report_organization}\n",
        "\n",
        "You should reflect on this information to plan the sections of the report: \n",
        "\n",
        "{context}\n",
        "\n",
        "Now, generate the sections of the report. Each section should have the following fields:\n",
        "\n",
        "- Name - Name for this section of the report.\n",
        "- Description - Brief overview of the main topics and concepts to be covered in this section.\n",
        "- Research - Whether to perform web research for this section of the report.\n",
        "- Content - The content of the section, which you will leave blank for now.\n",
        "\n",
        "Consider which sections require web research. For example, introduction and conclusion will not require research because they will distill information from other parts of the report.\"\"\"\n",
        "\n",
        "\n",
        "def invoke_structured_llm_with_retry(structured_llm, queries, max_attempts=3):\n",
        "    \"\"\"\n",
        "    Not all LLMs support structured generation.\n",
        "    Retry max_attempts to get a result back\n",
        "    \"\"\"\n",
        "    for _ in range(max_attempts):\n",
        "        results = structured_llm.invoke(queries)\n",
        "        if results:\n",
        "            return results\n",
        "    return results\n",
        "\n",
        "async def generate_report_plan(state: ReportState):\n",
        "\n",
        "    # Inputs\n",
        "    topic = state[\"topic\"]\n",
        "    report_structure = state[\"report_structure\"]\n",
        "    number_of_queries = state[\"number_of_queries\"]\n",
        "    tavily_topic = state[\"tavily_topic\"]\n",
        "    tavily_days = state.get(\"tavily_days\", None)\n",
        "\n",
        "    # Convert JSON object to string if necessary\n",
        "    if isinstance(report_structure, dict):\n",
        "        report_structure = str(report_structure)\n",
        "\n",
        "    # Generate search query\n",
        "    structured_llm = llm.with_structured_output(Queries)\n",
        "    \n",
        "    # Format system instructions\n",
        "    system_instructions_query = report_planner_query_writer_instructions.format(topic=topic, report_organization=report_structure, number_of_queries=number_of_queries)\n",
        "    \n",
        "    # Generate queries  \n",
        "    results = invoke_structured_llm_with_retry(structured_llm,\n",
        "                                              [SystemMessage(content=system_instructions_query)]+[HumanMessage(content=\"Generate search queries that will help with planning the sections of the report.\")])\n",
        "    \n",
        "    # Web search\n",
        "    query_list = [query.search_query for query in results.queries]\n",
        "    search_docs = await tavily_search_async(query_list, tavily_topic, tavily_days)\n",
        "\n",
        "    # Deduplicate and format sources\n",
        "    source_str = deduplicate_and_format_sources(search_docs, max_tokens_per_source=1000, include_raw_content=True)\n",
        "\n",
        "    # Format system instructions\n",
        "    system_instructions_sections = report_planner_instructions.format(topic=topic, report_organization=report_structure, context=source_str)\n",
        "\n",
        "    # Generate sections \n",
        "    structured_llm = llm.with_structured_output(Sections)\n",
        "    report_sections = invoke_structured_llm_with_retry(structured_llm,\n",
        "                                                      [SystemMessage(content=system_instructions_sections)]+[HumanMessage(content=\"Generate the sections of the report. Your response must include a 'sections' field containing a list of sections. Each section must have: name, description, plan, research, and content fields.\")])\n",
        "    \n",
        "    return {\"sections\": report_sections.sections}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Structure\n",
        "report_structure = \"\"\"This report type focuses on comparative analysis.\n",
        "\n",
        "The report structure should include:\n",
        "1. Introduction (no research needed)\n",
        "   - Brief overview of the topic area\n",
        "   - Context for the comparison\n",
        "\n",
        "2. Main Body Sections:\n",
        "   - One dedicated section for EACH offering being compared in the user-provided list\n",
        "   - Each section should examine:\n",
        "     - Core Features (bulleted list)\n",
        "     - Architecture & Implementation (2-3 sentences)\n",
        "     - One example use case (2-3 sentences)\n",
        "   \n",
        "3. No Main Body Sections other than the ones dedicated to each offering in the user-provided list\n",
        "\n",
        "4. Conclusion with Comparison Table (no research needed)\n",
        "   - Structured comparison table that:\n",
        "     * Compares all offerings from the user-provided list across key dimensions\n",
        "     * Highlights relative strengths and weaknesses\n",
        "   - Final recommendations\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, choose the topic of your report. The default is CPU vs. GPU, but feel free to change the topic to something of your interest. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Topic \n",
        "report_topic = \"Example use cases outside of coding for agentic workflows that would return loads of value\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run the agent. We set the Tavily topic to \"general\", but you can set it to \"news\" if you want Tavily to retrieve latest searches. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: No raw_content found for source https://budibase.com/blog/ai-agents/ai-agentic-workflows/\n",
            "Warning: No raw_content found for source https://www.vonage.com/resources/articles/agentic-workflows/\n",
            "Warning: No raw_content found for source https://www.reddit.com/r/AI_Agents/comments/1khxj8x/i_cant_seem_to_wrap_my_head_around_the_benefits/\n",
            "==================================================\n",
            "Name: Introduction\n",
            "Description: This section provides a brief overview of agentic workflows and their significance in various industries. It sets the context for the comparative analysis of different offerings related to agentic workflows.\n",
            "Research: False\n",
            "==================================================\n",
            "Name: AI Agents as Developers\n",
            "Description: This section examines the use of AI agents in software development, highlighting their core features, architecture, and a specific use case that demonstrates their value in this domain.\n",
            "Research: True\n",
            "==================================================\n",
            "Name: AI Agents as SecOps Assistants\n",
            "Description: This section explores how AI agents function as security operations assistants, detailing their core features, implementation architecture, and a relevant use case that showcases their effectiveness in enhancing security operations.\n",
            "Research: True\n",
            "==================================================\n",
            "Name: AI Agents as Human-like Gaming Characters\n",
            "Description: This section discusses the role of AI agents in gaming, focusing on their core features, the architecture behind their implementation, and a use case that illustrates their impact on gaming experiences.\n",
            "Research: True\n",
            "==================================================\n",
            "Name: AI Agents as Content Creators\n",
            "Description: This section analyzes the capabilities of AI agents in content creation, outlining their core features, implementation architecture, and a specific use case that highlights their value in generating content.\n",
            "Research: True\n",
            "==================================================\n",
            "Name: AI Agents as Insurance Assistants\n",
            "Description: This section reviews the application of AI agents in the insurance industry, detailing their core features, architecture, and a use case that demonstrates their effectiveness in streamlining insurance processes.\n",
            "Research: True\n",
            "==================================================\n",
            "Name: AI Agents as Human Resources (HR) Assistants\n",
            "Description: This section examines the use of AI agents in human resources, focusing on their core features, implementation architecture, and a use case that illustrates their value in HR operations.\n",
            "Research: True\n",
            "==================================================\n",
            "Name: AI Agents as Customer Service Assistants\n",
            "Description: This section explores how AI agents enhance customer service, detailing their core features, architecture, and a relevant use case that showcases their effectiveness in improving customer interactions.\n",
            "Research: True\n",
            "==================================================\n",
            "Name: AI Agents as Research Assistants\n",
            "Description: This section discusses the role of AI agents as research assistants, highlighting their core features, implementation architecture, and a use case that demonstrates their value in research activities.\n",
            "Research: True\n",
            "==================================================\n",
            "Name: AI Agents as Computer Users\n",
            "Description: This section reviews the application of AI agents as computer users, detailing their core features, architecture, and a use case that illustrates their effectiveness in automating computer tasks.\n",
            "Research: True\n",
            "==================================================\n",
            "Name: Building AI Agents\n",
            "Description: This section examines the process of building AI agents, focusing on the core features, architecture, and a use case that demonstrates the value of custom-built AI agents.\n",
            "Research: True\n",
            "==================================================\n",
            "Name: Conclusion with Comparison Table\n",
            "Description: This section summarizes the findings from the comparative analysis, presenting a structured comparison table that highlights the strengths and weaknesses of each offering, along with final recommendations.\n",
            "Research: False\n"
          ]
        }
      ],
      "source": [
        "# Tavily search parameters\n",
        "tavily_topic = \"programming based agentic workflows\"\n",
        "tavily_days = None # Only applicable for news topic\n",
        "\n",
        "# Generate report plan\n",
        "sections = await generate_report_plan({\"topic\": report_topic, \"report_structure\": report_structure, \"number_of_queries\": 5, \"tavily_topic\": tavily_topic, \"tavily_days\": tavily_days})\n",
        "\n",
        "# Print sections\n",
        "for section in sections['sections']:\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Name: {section.name}\")\n",
        "    print(f\"Description: {section.description}\")\n",
        "    print(f\"Research: {section.research}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Research and Writing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Now you are ready to give the Agent details about which sections require research, and the number of queries needed.  Let's First you will define the LangGraph state. Each state will have the following fields. \n",
        "\n",
        "Let's define the LangGraph state. Each state will have the following fields:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SectionState(TypedDict):\n",
        "    tavily_topic: Literal[\"general\", \"news\"] # Tavily search topic\n",
        "    tavily_days: Optional[int] # Only applicable for news topic\n",
        "    number_of_queries: int # Number web search queries to perform per section \n",
        "    section: Section # Report section   \n",
        "    search_queries: list[SearchQuery] # List of search queries\n",
        "    source_str: str # String of formatted source content from web search\n",
        "    report_sections_from_research: str # String of any completed sections from research to write final sections\n",
        "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API\n",
        "\n",
        "class SectionOutputState(TypedDict):\n",
        "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Write Single Section\n",
        "Now you will define the query writer instructions and the Agent function and nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKsAAAGwCAIAAABZ7AKiAAAAAXNSR0IArs4c6QAAIABJREFUeJztnWdAFMfDh+cad1zj6F1pUqRIFSyxodgFFHuLNUZjNOrfaBKjoonGHlFjiF1jB1ti7LFhF6mKioAIHB2u9733w/oiUZrJnns483y629md/d3tc7PlZncoer0eICCGSnYABMkgA2AHGQA7yADYQQbADjIAduhkB2gGhVRbXaaRi7VyiU6n1Ws1reDclWlKZZhQ2Xwam0ezcWaRHacZjNQASa0m95E0L0umkOjYPBqbT2fzaFxzOmgFAgBMC4QlCrlYZ2JKfZUjd/XjuPlzXP24ZOdqGIqxXRHSarBbZ6pqy9UW9kw3P46DuynZif4TCpkuP0tW8kIhzFd2HmzpHmB0HhiXAVm3am+cqOo82LJDNwHZWQimtkJ960wVhumjxtuZMI3o8MuIDLh8qIxnzujYz4LsIAakvEh5IqF4yAwHe1djaduMxYCzu4Quvpz24Xyyg3wIjv9c1GuUjYWtCdlBgLEYcPznIr8ufO9QKDY/zvGfi0J6m7v6csgOYgTXA64eK/cK5UG1+QEAcXOcrh2vkNRoyA5CtgFP7onZPJp/FzNyY5DCmEXOlw+Xk52CbAOuHqsIjjQnNwNZmDBp9i6se+eryY1BpgF3/6oK6W1OZ5C/JyKL8P6WDy/VaDUYiRlI+/Y1akxYoOzY92M+92sJ3eOsUq/UkBiANAPyM2WmXBpZazcenD3Zj+9ISAxAmgF5WTI3vw99LrRo0aJTp079iwX79OlTXFxsgESAZ85gcagVRSpDVN4SyDFAr9eLqjRuH/wi+ePHj//FUkKhsKbGgA21Vyiv8KnMcPU3DTkGSGu1ComORqMYqP6UlJTPPvusa9euMTExS5curaysBACEhoaWlJSsWLGiR48eAACpVLp9+/aJEyfis23cuFGpVOKLR0ZGHjp0aNq0aaGhodeuXRs8eDAAIDo6ev78+YZIy+HTK4vVhqi5RejJoPSl4sj6QgNV/uTJk5CQkN9++00oFKakpIwaNWrWrFl6vV6pVIaEhJw8eRKf7bfffgsPD7948eL9+/evXLnSv3//n3/+GS/q27fv8OHD165de+fOHY1Gc+PGjZCQkKKiIgMFLnouT04wVOXNQk7/ALlYx+Yb6jAwLS2NxWJNnjyZSqXa2dm1b98+Nzf33dnGjRsXGRnp6uqKv01PT79169aXX34JAKBQKGZmZgsWLDBQwrdg82kysfbDrOtdyDEAw/QmLEPtgAIDA5VK5dy5c8PDw7t16+bs7BwaGvrubAwG4/bt20uXLn327JlWqwUAWFi8OTVt3769geK9C40OGCaG2iE2CznHAWweTVxlKOu9vb03b95sbW2dkJAQGxs7c+bM9PT0d2dLSEhITEyMjY09efLkgwcPJk2aVL/UxOTD/XEnE+lo5F0WI8kAPl1uyHavc+fOS5YsOXPmzLJly0Qi0dy5c/FfeR16vT4pKWnkyJGxsbF2dnYAAImEtJNymVjHMdg+sVnIMYDLp/EsDbUDevjw4a1btwAA1tbWgwYNmj9/vkQiEQqF9efRaDQKhcLGxgZ/q1arr1+/bqA8zaJW6KwdmWStnRwDaAwqjUZ9+cQgJ8Hp6ekLFy5MTk6uqanJyso6fPiwtbW1vb09k8m0sbG5c+fOgwcPqFSqi4vL6dOni4qKamtr4+PjAwMDxWKxTNZAJBcXFwDAxYsXs7KyDBE454HE3o20LkOk7X7c/Dh5WQYxYNy4cbGxsevWrevTp8/06dM5HE5iYiKdTgcATJ48+f79+/Pnz1coFD/++COLxYqLi4uJienYseMXX3zBYrF69+5dUlLyVoVOTk6DBw/evn17QkIC4WlVCl1lsdrRgzQDSOsjJBVp/z5aPniaAylrNx5y0yVlL5VdhliTFYC0NoBrRuea0bNuicgKYCSknKry70pmx2gy7xjpPNhyb/xLv84NdxDSarW9e/dusEitVjMYDAqlgXNoNze3Xbt2EZ30NXv27NmzZ0+DRVwuVyqVNlgUGBi4adOmBouyUkRtfNh8CwahMd8PknuKpl6uZrAa7SXW2BmaSqViMhs+eKZQKFyuof5wUqlUanXDF/DVanVjlxBoNBqbzW6w6NT24r4TbFlsMn+H5PcVPvVLcVAv8zZeDX9HHzEnthaHRZk7tSP5g5PfQyv6c8eLB8qModfsh+Ti76WufhzSN79RtAH43wS/ryqMGmdr29bY77QlhEsHy9z8OW7+RnEPoVEYgHN046sO3cy8Qj7mGwe0GuzE1mKfcL5fJ2PpIG9EBgAAUk5XFuUqugy2NIbmkXDunK0qeCzrEWdj52JETZ1xGQAAKC9UppypMrNi2LuyXP04LHar701a9lL56rn83rnqsCiL0N7mFCppfwQ3iNEZgFP4VP7soSQ/S2bvyuKZMzhmNDafzuHRdZgxpn0LCkUvqdZKRVoKAE/uSbjmdI8O3A7dBDS6cW17HCM1oI6SF/JKoVom0snFWgqVopDqCKxcKpUWFRV5e3sTWCcAgCugAUDhmtF5FjRHDzaHb6QPasExdgMMSlpaWkJCws6dO8kOQibkXw9AkAsyAHaQAbCDDIAdZADsIANgBxkAO8gA2EEGwA4yAHaQAbCDDIAdZADsIANgBxkAO8gA2EEGwA4yAHaQAbCDDIAdZADsIANgBxkAO1AbQKVS6z9HFE6gNgDDsOpqkof5IR2oDUAgAxDIAOhBBsAOMgB2kAGwgwyAHWQA7CADYAcZADvIANhBBsAOMgB2kAGwgwyAHRifKDlixAiVSoVhmEqlkkgkNjY2GIYplcqLFy+SHY0EYGwDIiMji4qKhEJhdXW1RqMpLi4WCoU8Ho/sXOQAowFjx45t27btWxOjoqJIikMyMBrA5XL79+9Po715br2zs/OoUaNIDUUaMBoAABgzZoyzs3Pd2379+gkEZA76RyKQGsDhcIYMGYIPRdumTZu4uDiyE5EGpAYAAGJiYpycnPAjAEtLS7LjkEbzgx9oVFiVUC0ndGgH44DWr9vEG5QbnTrEGGgMbBKhUoGFrQnfsvnRTJu5HnA9uSI3Tcoxo5tyjXqgDMRbcM3phTkyc2tGaB+Lpgc2b8qAv3YLze1Zvp3MDRMSYXBUSt3FfcU9hzc1ulmjBlz8vUxgy/QOg/QI+WPi5JaXg6bam9s2PCxyw0eCZa+USgWGNv/HQcRgm/sXaxorbdiAaqGazoD3NOEjw8yKUZgjb6y04c0sE2sFVg03GohWhymHzuHTVUqswdKGDcB0QKeF7j/DjxhxlZpKaXi8S9TUww4yAHaQAbCDDIAdZADsIANgBxkAO8gA2EEGwA4yAHaQAbCDDCCZ6NjIfft3kBjgIzdgefyis3+dIjtFU4wcMT7AP4jEAB+5AU+fPiY7QjOMGf1pYGAIiQEI6/9ZU1O9avX32Y8z2ji7REcPLyoqvHHz7727jwMAtFrtzl3b7ty9WV5e6ucXGBs9IiKiKwAgP//F5Kkjt23de/Dg7pspV62tbXr2iJo+bTZ+N091ddW2XzZkZacrlcqwsE4Txk11dm4LAEhKPnzw0O6v5i5eumxhTMyI2bMW5Oe/OH3meOqj+6WlJS5t3QYMiIkeEgcA6BkZCgBYu27FL9s3njl1FQBw7vyZ02eS8vNzXV09evWMGjZ0NKWR/0zrkMvlP6z6LjX1nlarnTVzfmVl+fUbV/btSQIA9B/YdeKE6aNGTsDnXLM2/sWLZ79uP9BE+Ly83CnTRq36YdO6DSsFAvMdiYeiYyOHDR09YfzUJpbS6/VJyYfOn//jVdHLtm1cQ0MjJk/6vP49T/8FwtqANeviC18VrF2zbeWKDXfvpty9m0Klvq58c8Ka40kHY2NGHvz9TPdukUuXL7x2/TIAgMFgAADWb1gZGdnvwrnb3y5eefTYgb+vXgQA6HS6r+Z/lpb+8Ku53+zaccRcYDFz1sTikiIAgImJiVwuO336+OJF8bHRIwAAW7etv3//9pwvv169avOAATE/b/7pzt0UAMC5sykAgP8tWIJv/kuXz/20ZrlnO++DB05PnTLreNLBLdvWN/u5Nmz6Me/F800bfzty6M+iosJLl//CYzdBE+HxZfcd2DFyxPj5875r4VLJyYcP/L4rbtiYwwf/GDx42J9nTx4+su+/ba43EGOASFR7587NEcPHt/fxs7S0mj/vu9LSErxIpVKdv/DHmNGfDhk8zIxvNqB/dGSvfvv2/1a3bPduvXt0781gMDp0CHawd3z27AkAIDMzrbCw4JvFK8I7drawsPx8xly+mSAp6SAAgEKhKJXKUaMm9o7s5+TUBgCwZMmqtWu3BQeFBQWGRg+J8/L0uXf/1rshz549GRAQNHfOInNzi+CgsEkTZ5w8ebSmpqmny0ul0mvXLo0YMd7L08fCwnLWzHl0OqPZ++2bDg8ACAuNGB431sfbt4VLpWekenm179t3kEBgPmhg7NYte8I7dnnPTdQoxBjwIu85AMDPrwP+lsvlBgd3xF8/e/ZErVaHhXaqmzmwQ0heXq5ILMLfenr61BVxuTypVAIAyMxKYzAYwUFh+HQKhRLYISQ9I7VuTm+vel+fXp+cfHjCp8N6Rob2jAzNefq49p3timFYVnZ6/RhBQWEYhmVkPmricxUW5mu1Wu//31QUCsXHx695A5oL79nO572W8vPr8PDh3TVr48+dPyMSixwdnDw8PJvO0HKIOQ6QSMQAAA6HWzeFzzfDX+BbdPacKW8tUlNdhd+2V7ezqI9UKtFoNPiOvA6B4M2dCyYmr7sxYhi26Js5Go162tQvAgNDeVzeu+sCAKjVao1Gs3PXtp27tv0jRpNtQHV1FQCAbcqum1L/dWM0H57JfK+l4oaNYbM5Kbeu/bRmOZ1O79Gjz2fTvrSysm42SUsgxgAmkwUA0KjVdVNqal9/s5ZW1gCA+fO+dXR0rr+IjY1ddXVlYxVaWlqZmpr+sHJj/Yk0agPHPs+e5+TkZK9buy3k/1sdqVRibWXz1mwsFovNZkf1GditW2T96Q72Tk18LjMzAQBApVbVTZHJG72/TIfp3jd8fZpYikqlDhoYO2hgbEFBXmrqvT37EmUy6Y//nPNfQ4wB+CFrfsELFxc3fPeZmnrP1tYeAODk2IbJZAIAggJf211TU63X69lsdhMDvLi7eyoUChsbO0eH11uoRFgsMGvg7iWRqBYAULfJCwryCgryXF3cG6xTIpXUxdBoNEJhsY2NbROfy87OAQCQk5Pt2c4bb28eZ2cwWa/vvzExYSoUb3phv3r18n3Dt/Ajnz//h6enj6uru4uLm4uLm0Qq+fPsiaZraznEHAc4Oji1beu6d19icUmRVCrd9PMqe3tHvIjNZn868bN9+3/LzExTq9XXrl9esHDmpp9XN11hSHDHjh07r1u3oqysVCSqPXnq2IzPx587d/rdOV3autHp9CNH94sl4sLCgoQta8NCI0rLhAAAJpNpbW3z4MGdR2kPtFrttClfpKRcPfvXKQzDMjPT4lcsnrdghrpeu/Uu1tY2fn4dduzcWlT8qrKyYuOmVRKpuK60fXv/a9cvS6VSAMD+AzsrK8vfN3wLP/LlK+e+X/a/W7eui8SiO3du3rh5xc+3Q9O1tRzCrgcsXPD9ug0rx0+IdXdr16fPAA6H++RJFl40auQEd3fPg4f3pKbe43C4vu0D5s//rrn6wKofNp0+kxS/cvHjx5nOzm179+4/dGgDj/mwtbX79puVe/clRsf0cnR0/nbxiqrqyiXfL5g4KW7v7uNjx0zevWf7vfu3Dh38w98/MHH7778f3P1r4malUuHbPmDlig3MhnbJ9Vm8KH7TplXTpo9WKpU9e/Tp3q139uMMvOiLWQvWr185OLoHnU4fOWJ8ZK9+qan33it8Cz/y/Hnfbdm67tsl8wAAFhaWgwbGDo8b12xtLaTh+wbvna9WK0GHHu8xEptIVKtUKm1t7fC3i7+dS6fRV8SvIyqokbDp59XpGam7dx4lO8j7cfDHF5Pj3RjMBi5/EXZFaHn8oq/mTb9x82+RqHb/gZ0PH94dMgTeB3O0IgjbCyxd+tPadfG/7dhSUVHWto3r0iWrw0IjiKrcoAwe0qOxoq+/Xta1S6OlHweE7QVaL8L/v3z5LuYCCxar0TvvWxFN7AXQk0GAvZ0D2RHI5CP/dxjRLMgA2EEGwA4yAHaQAbCDDIAdZADsIANgBxkAOw1fE2SxaZiu4YePIVojlo5MSiN9lBpuA8ys6MIChWFDIT4UNeUqlRyj09/naXJO7dhqxcf3OHlIKS9UegZxGytt2AAanRLez+LCvmJDBkN8CApzpC/SxGF9G/2bt6mnyxe/UJzfVxrY3UJgy0TjC7QuKBRQJVRKqjUvH0tHfOVEoTZ6c1wzI0xIa7WpV2pKC5QKyUe4U8D0eo1GwzT5CJ+gbOHApADQxts04JNmHhAP45ijdaSlpSUkJOzcuZPsIGSCrgfADjIAdpABsIMMgB1kAOwgA2AHGQA7yADYQQbADjIAdpABsIMMgB1kAOwgA2AHGQA7yADYQQbADjIAdpABsIMMgB1kAOwgA2AHGQA7UBtAo9GcnJoaXwAGoDZAp9MVFRWRnYJkoDYAgQxAIAOgBxkAO8gA2EEGwA4yAHaQAbCDDIAdZADsIANgBxkAO8gA2EEGwA4yAHZgfKLk5MmTNRoNAEAikVRWVrq6ugIAZDJZcnIy2dFIAManBbu7uyclJVGpr9u/J0+eAACsrKzIzkUOMO4FJk2a9FbnML1e37lzZ/ISkQmMBjg4OHTv3r3+FFtb24kTJ5KXiExgNAAAMHr0aAeHNwNOR0REtG3bltREpAGpAfWbAXt7+wkTJpCdiDQgNQAAMGrUKEdHRwBA586dXVxcyI5DGoY9F9BjekmtlkJpdIALEuGzbbtG9ElJSYkdPEZSoyU7TsNQKIArMOw2MtT1gJdPZI+u1hY9V1g7MJWyj3B8kg+DuZ1JeaGyXTCv+zBrA63CIAY8S5Vk3RKHD7DmW36EA7h8YJRyXUWRIuVk+aSlLnQT4vfaxBuQc1+c80AaOcahBfMiWoqkVnNuZ9HkeFfCaybYKY0Ge3xXgjY/4fAEjMBelvfOVxNeM8EGVJeo1Uo0XK1B4AroRc+JHwiWYAPE1Rp7VzaxdSJwBLZMaiNjB/8XCDZApwUKqZGeWbV6MFBVoia8VnivCCFwkAGwgwyAHWQA7CADYAcZADvIANhBBsAOMgB2kAGwgwyAHUgN+OHH72bPmfIh15iUfDiyT8cPucYWAqkBiDqQAbBD/n2DEqlk957td+/crKmt9vJs37t3/4EDYvCic+fPnD6TlJ+f6+rq0atn1LCho/Fux/n5L06fOZ766H5paYlLW7cBA2Kih8Thi0THRk4YN/X6zSsZGY9OnbzC5/Fv377xc8JPFRXlHu6eMTEj+vcbgs/JoDPS0h7+sOq72toaD3fP2bMXtvfxayLn0Lio6CHDJ06YBgAQiWpjhvbu0b330u9X46VxI/oNGzp69KiJ2dkZe/cl5uRkmwnMO0V8MnHCdA6Hg89DoVBKhMW7dm27ey/Fyspm9MiJUVEDDfnVtgjy24A1a5Y/zs6YO3fxnl3HfXz8Nm5alZ2dAQC4dPncT2uWe7bzPnjg9NQps44nHdyybT2+yNZt6+/fvz3ny69Xr9o8YEDMz5t/unM3BS9iMBh/nD3h4eG1ds1Wtin79u0bS5YumDJ51upVm7t27blmbfyly+fwOcvKS0+fOf7N4hWrV21Wa9Rr18U33WUyNDTi8ZNM/HXqo/u2tnaZWWn42+KSoqqqytDQiKLiVwsWzlSqlFsSdq9Yvi4v7/lX86ZrtW86TKxa/X2fPgPjl6/z8+2w6qelr169NMyX+h6Q3wakZ6SOGjkhLDQCADB92uzu3Xub8QUAgLNnTwYEBM2dswgAYG5uMWnijDXr4seNmWxubrFkySq5XGZv5wAACAoMPXfu9L37tyLCu+C/Mz7fbPasBXjlu/ds7/ZJrz69+wMAwkIjZDKpXC7Diyoqyrb/sp/H5QEAhsaOWrd+pVgsMjMTNJYzOCgsYctavV5PoVDS0x/26N7n5KmjxSVFjg5OmZmPBALzdh5ee/YmMuiMFcvX4fUsmL9k9NjBN1Ou9ujeG3+Y/dDYUeEdOwMAPDy8zp0/c/nK+U8nTv9Q33TDkN8G+PsHHj124Jftm27duq7RaLw8fezs7DEMy8pODwvtVDdbUFAYhmEZmY8AAECvT04+POHTYT0jQ3tGhuY8fVxb86YLpZdne/wFhmEv8p57e/vWFc34bM6QwcPw1+7unvjmBwDgzimVyiZyhgSHy+Xy/PwXAIDMrDR/v0Bvb9+szDQAQGZmWkhwRwBAdna6t7dvnUZ2dvYODk6vMwMAAAjv2AV/wePyXF3chaXFBHyD/w3y24CvFy47ffr4lb/PHz12gMvhxsaOnDB+mlar1Wg0O3dt27lrW/2Za2qqMQxb9M0cjUY9beoXgYGhPC7vrfM6E5PXNykolUoMw5hMVoPrpdPffPaW3NVkbW3j7Nw2Kzvd0tIqP/9FUFDYk5yszKy0vn0HZWQ+GjVyAgBAKpXkPH3cMzL0H5mrq+pes9lvOlGyTE3FYlELviHDQr4BfB5/3NjJY8dMyspKv3Hz7/0HdnK5vBHDx7HZ7Kg+A7t1i6w/s4O907PnOTk52evWbsN/dvj3bm1l827NTCaTSqXKZFKiooYEd3z8JFMgMHdz82Cz2f7+Qb9s3ygS1RYVFXaK+AQAYGFp5e8fOOnTGfWXwhsYHKVSyWK9NlIul9nbOxKV7V9DsgESqeTixbMD+kezWCx//0B//8Dc3KfPnufgrbREKgkKfP170mg0QmGxjY1twcs8AEDdJi8oyCsoyHN1cX+3chqN5uXVvu54DQDw244tarV61sx5/y5tcHDHX37ZyOXwOnQIAQD4+wUWFhZcuvRXmzYuFhaWAAB3t3YXLv7ZISC47gElBQV5Tk5t6mp4/jzH3z8QACCXy1++zO/2SWTja/tAkHwcQKfR9+5LXBb/dVZWenV11YULfz7PzfH3CwQATJvyRUrK1bN/ncIwLDMzLX7F4nkLZqjVape2bnQ6/cjR/WKJuLCwIGHL2rDQiNIyYYP1Rw+Ou3//9pGj+x+lPTh1+vihw3tdXRtwpYUEBYaVlglv377u59sBb9LbeXglnzgcEhKOzxAXNxbDsC3b1iuVylevXv6auHny1JF5+bmvPyydvnvP9sLCAq1Wu3P3Nq1W26tn1L8OQxQktwGmpqbxy9YmbF2L78tdXd1nfDYXP2X39w9M3P777wd3/5q4WalU+LYPWLliA5PJtLW1+/ablXv3JUbH9HJ0dP528Yqq6sol3y+YOClu7+7jb9Xft+8gsUS0d1+iTCaztLSaPm32gP7R/zotl8v18mqfk5MdHBSGT/H1DThx8mjdWz6Pv3PHkcOH9372+bjCwgJvb9//LVji2c4bAKDTadlszojh4+bOm15TU+3m5vHdtz/Ubx7IguD7BnPuSwoey7vE2BJYJwJHIdWd2V44ZQXBtw6SfzaIIBfyzwWMisFDejRW9PXXy7p2abS09YIM+AeJiQcbKzIXWHzYLB8IZMA/wK80QwU6DoAdZADsIANgBxkAO8gA2EEGwA4yAHaQAbCDDIAdgg2g0YApzwCPPEMAQKEAaycm4dUSbICZDaMkl/inHiIAANWlKkxH/EOgCTbAxollYor2LAZBXK1p40380zqJ31qBPczO7ykivFrIKXupeHqvNriXOeE1G+Tp8kXP5DdOVYYPsDazMjFhocOC/4SoSl1ZpMy8WTN2URsqlfixOgw1wkRZoTL1cs2rZwo2jyaXGukIE3o90Ouxun69Roi1A1Mq0rYL4kYMsDTQKgw+5qhSpqMYwFxCyMzM/PXXX7ds2UJ2kEahUgGDaVhBDd5DhMUx3r0A3USPARUT7kNXqD88AhmAQAZADzIAdpABsIMMgB1kAOwgA2AHGQA7yADYQQbADjIAdpABsIMMgB1kAOwgA2AHGQA7yADYQQbADjIAdpABsIMMgB2oDaDRaG3akP9sZ3KB2gCdTldYWEh2CpKB2gAEMgCBDIAeZADsIANgBxkAO8gA2EEGwA4yAHaQAbCDDIAdZADsIANgBxkAO8gA2EEGwI7BnylqhCxatOjChQsUCkWv11Mor593am1tfe7cObKjkQCMbcD48ePt7e0pFAqVSqVQKLgEgYGBZOciBxgN8PX1DQ4Orj/FwcFh7Nix5CUiExgNAACMGzfOzs6u7q2fn5+/vz+piUgDUgO8vLzqmn0HB4fRo0eTnYg0IDUAbwbs7e0BAD4+PgEBAWTHIQ2DP13eaPH29g4ICFCr1dAeAeAQfDaYdrU2L1tGpVLKXykJrNZA6PV6nU5Hp7eCnwGdTjExpdq1ZYX0Nje3MSGwZiINSNpc5OjJsbBlWjow9cBIxxVppVAAkEu0okp16qWqyDE2jm6mhNVMlAHHNhV5BPM9OvAJqQ3RBOd2F4VEmrv5cwipjZgjwfRrtc5eHLT5Pwz9JjmlXqnRaYn56RJjQP5jmbkt8QOiIhqFQhHmEzO2KzEGUCkUCztkwIfDwY1dW6khpCpiDCgvUlLQkd8HRKXQaZTGtBdAtF6QAbCDDIAdZADsIANgBxkAO8gA2EEGwA4yAHaQAbCDDIAdZADstBoDkpIPR/bpSHaKZmgVId+i1RjQ3sdv/Lip+OsTJ4+u+mkp2Ylek5//YtSYQfjr+iFbC62gkySOj4+fj48f/vrp08dkx3nD02dvwtQP2VogwYD8/BczZo7/88x1vJPuho0/nvkjedeOI66u7gCA02eSftm+8cypqytWfkOj0Wxt7Q8f2bd82ZqKivJtv2y4fPHe3HnT09NTAQAXLvz56/YDnu28s7Mz9u5LzMnJNhOYd4r4ZOKE6RxOM33oJFLJ7j3b794Dx4TCAAAO9UlEQVS5WVNb7eXZvnfv/gMHxOBF586fOX0mKT8/19XVo1fPqGFDR9fdXXr79o2fE36qqCj3cPeMiRnRv9+Q3Xu279u/AwDQMzJ05udfUak0PCQ+f0rKtb37El8W5puZCTw8vObM/trW1g4AEDO096RPZ4hEtXv3JZqamoaFdvpi1gJLSysDf/ENQ8JewNraVq1WP3+eg7/NzEqztbXLfpyBv83KTg8NiaDT6QwGIy8/Ny8/94cVGwL8g+oW37Qh0cfHLypq4N+XH3i28y4qfrVg4UylSrklYfeK5evy8p5/NW+6VqttOsOaNcsfZ2fMnbt4z67jPj5+Gzetys7OAABcunzupzXLPdt5HzxweuqUWceTDm7Zth5f5PbtG0uWLpgyedbqVZu7du25Zm38pcvnJn06Y9TICba2dn9ffjA87h/3HTx4ePf7Zf+Lihp49PDZpUtWl5UJN21ejRcxGIwjR/ZRqdSTJy7v3Z2UmZW2Z++vhH7H7wEJBnC53LpNXlNT/fJlflSfgRmZj/DSrMy04OCOAAAKhVJaWrJ86ZrOnbsJBOaN1Xbp0l8MOmPF8nVt2ri4uLgtmL/kee7TmylXm86QnpHarVtkWGiEjY3t9Gmzt27ZY2lpDQA4e/ZkQEDQ3DmLzM0tgoPCJk2ccfLk0ZqaagDA7j3bu33Sq0/v/mGhEePHTRk5YrxcLmtiFbt2/9Ltk15xw8aYmQl8fQNmfj7vzp2bOf+//3J0dB43djKPy7O0tAoL7fTs2ZN/9V0SADlHgiHB4VlZ6QCAjMxH7Ty8goLCHmdnAAAqKsqFpSWhIeH4bG3buLJYrKarys5O9/b2NTMT4G/t7OwdHJzqfGoMf//Ao8cO/LJ9061b1zUajZenj52dPYZhWdnpYaGd6mYLCgrDMCwj8xGGYS/ynnt7+9YVzfhszpDBw5pYRd4/5/fybA8AyMnJxt96evrUFfF4fJlM2nRgw0HOkWBQUFjClrUAgPT0h/7+Qe19/EvLhBUV5WnpD21sbJ2d2+KzmTCb730qlUpynj7uGRlaf2JNdVXTS329cNnp08ev/H3+6LEDXA43NnbkhPHTtFqtRqPZuWvbzl3b/lFbTbVSqcQwjMlsRsd6qaQqlar+/Gw2GwBQ12xQjKZfJTkGhIV1EotFwtKSjMxHE8ZPYzKZXl7tM7PSsrLSgoPe73zawtLK3z9w0qcz6k804wuaXorP448bO3nsmElZWek3bv69/8BOLpc3Yvg4Npsd1Wdgt26R9Wd2sHdiMplUKrXlv1S86VIq33TolsllAABLC3IO95qAHAPM+GYe7p63Uq69ePG8Q0AwAMDfLzAz89HD1HtvbctmcXdrd+Hinx0CgqnU13u0goI8J6emxo8SiUWXL58b0D+axWL5+wf6+wfm5j599jwHAODu7imRSoICX7coGo1GKCy2sbGlUCi4o3WV/LZji1qtnjVzXoOroNPpXp4++NElDv7azb3de326DwBpV4SCgsKSTxx2cXHDd+F+vh3u3k0pLn5VdxDQBI6Ozk+eZKU+ul9TUx0XNxbDsC3b1iuVylevXv6auHny1JF5+blNLE6n0ffuS1wW/3VWVnp1ddWFC38+z83x9wsEAEyb8kVKytWzf53CMCwzMy1+xeJ5C2ao1WoAQPTguPv3bx85uv9R2oNTp48fOrwXP311cmpTVVV58+bVV69e1l9LbMzImylXk5IOiSXiR2kPtv2yITgorJ2HFxFfHpGQdkUoOCjs2PHf6w6m/P0DhaUl7Ty86o7pmmDwwKHPnj3538JZP61OCA0J37njyOHDez/7fFxhYYG3t+//FizxbOfdxOIcDid+2dqErWtnz5kCAHB1dZ/x2dz+/YbgMRK3//77wd2/Jm5WKhW+7QNWrtjAZDIBAH37DhJLRHv3JcpkMktLq+nTZg/oHw0AiAjv6u8XuGTpgokTpvN4b+6bi4oaWFFZfuTY/i3b1tva2oWGREyb+gUR3xzBEHPn6I7v8mJmtWWyaUREQjTPgwuVAit6UM/mfy3N0mr+F0AYiFbzv8D7MnhIj8aKvv56WdcujZbCxkdrwMGDZxorMmUR9viFj4CP1gAel0d2hNYBOg6AHWQA7CADYAcZADvIANhBBsAOMgB2kAGwQ4wBAmsT9BTZD4kJk0qjE/ONE2MAhunFVcQ83g7REiqFKq6AmH9iiTHAycNUUoMM+HBQgN7CjpgnjBNjQOfBVjeTyzAMumHLSOHBhUorR6bAmhgDCHu2uFyiO7SmMHKsvaV9SzvUIt4XjRpLvVRlyqV0GUxYj1MixxdQSHXXkyvysmRu/jxpa9gp6PV6TK+nUVvBCRGVTpFUaSg04NeJH9Sz0ftn/gXEjzipUWNVJSpdM7dtGQW5ubnJyckLFy4kO0jz6PWAK6DxLRhUGsEnXcT3D2CYUO1cWkcXjAqpTqordPRoHWkNRCtoABEGBRkAO8gA2EEGwA4yAHaQAbCDDIAdZADsIANgBxkAO8gA2EEGwA4yAHaQAbCDDIAdZADsIANgBxkAO8gA2EEGwA4yAHaQAbADtQFUKtXa2prsFCQDtQEYhlVUVJCdgmSgNgCBDEAgA6AHGQA7yADYQQbADjIAdpABsIMMgB1kAOwgA2AHGQA7yADYQQbADjIAdpABsEP8M0WNnwkTJmRlZVEoFAzDqP//SFmdTpeWlkZ2NBKAsQ34/PPPzc3NKRQKjUajUCi4CuHh4WTnIgcYDejUqZOnp2f9KRYWFmPHjiUvEZnAaAAAYOLEiWZmZnVvPTw8unXrRmoi0oDUgIiIiLpmwMzMbMyYMWQnIg1IDcCbAT6fDwDw9PTs3r072XFIA14DIiIivL29ORzOqFGjyM5CJq3jbLA4V1FWqBRVaWUiHZ1BldQSM4CJXCarrKpq06YNIbUBAJimNKYphWNGt7RjOHuyuQLih28gHKM2oPiFIv26qPCJjC1gMvksOp1KZ9LoTDow1sgYhmlVOq1KB4C+pljC5tF8wnkhvYgcFIZwjNSAKqHq6vEqhULPteTybNg0eqvcWynEKnmNUvi0Ory/ZViUkXpgjAZcTap6kSG1cbfgWbPJzkIAer2+/HkNplFHjbOxtGOQHedtjM6AE1tLdDSmVVsB2UEIRqfW5d0v6TncyqMDl+ws/8C4DDixrYTG4fJtOGQHMRSFacLIkZaObkY0tpURGXB43SuOjeDjaPmb4FW6sMtAgZu/sbQExnKEdfFgOVPA/eg3PwDAuYP9lSOV4mpjGZHTKAx4liqRiCnmjnyyg3wgXELtz+8vJzvFa4zCgOvJlTw7WDY/AIDOpGMURtq1GrKDAKMwIO1aDc+Gw2C2gstnBGLtZn7rTDXZKYBRGPDkvszSxXjP/dYmjE46s4bwaqk0qo274JERNAMkGyAsUKhVejqDRm4MUjA1Yz17KCM7BdkG5GXI2OYf//F/g3DMWdWlKrUSIzcGyXvf6nINz8pQuwCdTvvXpe1PnqXU1pa6tu3QOXx4e68uAABh2Yv1W8Z8+dmuK9f3Zj25Zsa3CfTvM6DPLBqNBgAoLc87nBRfVpHv4RbSu/tkA2XDsXHlv8qRuQfyDLqWpiG5DSh5oaCzDLULOPHHuhu3D3UNH/7N/JP+vr32HV6UkXUFAECnMQAAx06tCgrou3rpzTFxy6+l/J6efQkAoNVqduybKzCzWfjlkYFRX1y9eUAiqTRQPACAVgNE1VrD1d8SyDRAp9Vr1ZiBDgI0GtWDtD97fTKxU8ehHLZZeMiQoIC+F6/urJuhg2+vDn6RdDrD3TXY0tyxqDgHAJD5+O9aUdmQ/l+ZC+zsbNxiBy1QKCWGiIdDM6FJRRAbIBNp+ZZMA1X+quSJVqv29HjTB9zdJVhYliuTi/C3Tg4+dUUsFg/f0pVVr0wYLAtze3w6n2clMLM1UEIAAINJVylIvipP5nEA3YSikBrqF6BUSAEAW3dMf2u6RFpFo9IBABRKA/bLFWIT5j+OTBl0loESAgAwHYbpIDaAzaOrFTq9Xk+hUAivnM+3AgDERS+2snCuP93czE7c+K6dbcpXqeT1pyhVBjxh06p0PFuSD8ZJXj2LS9OqdAwW8TGsLdswGEwAgIdbCD5FIq3W6/VMJhs0vmc3F9hrNEphWa69rQcAoFj4TCwx4IOHtRotV2Co/WALIflcwK6tqUpukH/JmEx2VM9pF//emfcyTaNVZ2RdSdwzO/mPZq7u+fp0o9NNjp1cpVYrReKKA0e/Y7PNml7kv0DRYxZ2JoarvyWQ3AY4e7Jy0uRcC4P0mOj5yXgHe8+/b+x7/uI+i8V1cfYfHv1N04uYsrhTxm3488KW737oZcJgDYz6IjXjPPG7KADwg4CaErlTO3vDVN9SSO4hIq7WHN1Q7NHFuQXzfmzUCqUMoBg42Y7cGCTvBfgWDOs2TIVYRW4MUlDJVO07kt9TiPz/ZEN6ml05Vt0mqNHGcP2WsTWi0nenY5hOr9fTaA1/hEVzk7gcwq4379w/L78wvcEitilfrhA3WPTNVyfY7Ib7PchFKq1c5epnQ1TCf41R9BM8vrmYac7nWTX8F1GtqAzDdA0WqTUqE0bDx9IW5g4EJhSLK7U6dYNFKpWCyWz4OEZgZlf3hIq3eJkqjBxh4dSO/H/FjMKA2gr1uf2Vdj4GvPpmVEgr5Uy6os9o8hsA8o8DcATWJiG9eMVZZWQH+RCoZJrKvGoj2fzGYgAAoF0gzyOAVfL4Ix/3Sa/XFzwsGf8tYfeq/neMYi9QR2aKOOuu3N7n4xwBTiFWvbhbMuMndzrDQJcY/g3GZQAAIPu2+MEVkZ2XFZND8sUyYqkVSqTl4nGLjOjXj2N0BgAAyouUZ3eVmXCYNu0sPoIuhKJSWcWLaq8w3ifRlmRnaQBjNAAn+7b4/sUaKoPBs2bzrNl0k1amgrxWKa6Q67UaDpfSY5gV39Lo7hrGMV4DcPIypU8fygpzZCYcOpVCpZnQTDgmOg3JvSsbRY9plFqtWsdi0/QY5hHI8QjgWNqT/O9f0xi7AXXUVKjlIp1MrNWoMY3KSDObsKimXBqHT+MK6Gwe+ddbW0KrMQBhIIzlegCCLJABsIMMgB1kAOwgA2AHGQA7/wfN3P+N+lASswAAAABJRU5ErkJggg==",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "\n",
        "# Query writer instructions\n",
        "query_writer_instructions=\"\"\"Your goal is to generate targeted web search queries that will gather comprehensive information for writing a technical report section.\n",
        "\n",
        "Topic for this section:\n",
        "{section_topic}\n",
        "\n",
        "When generating {number_of_queries} search queries, ensure they:\n",
        "1. Cover different aspects of the topic (e.g., core features, real-world applications, technical architecture)\n",
        "2. Include specific technical terms related to the topic\n",
        "3. Target recent information by including year markers where relevant (e.g., \"2024\")\n",
        "4. Look for comparisons or differentiators from similar technologies/approaches\n",
        "5. Search for both official documentation and practical implementation examples\n",
        "\n",
        "Your queries should be:\n",
        "- Specific enough to avoid generic results\n",
        "- Technical enough to capture detailed implementation information\n",
        "- Diverse enough to cover all aspects of the section plan\n",
        "- Focused on authoritative sources (documentation, technical blogs, academic papers)\"\"\"\n",
        "\n",
        "# Section writer instructions\n",
        "section_writer_instructions = \"\"\"You are an expert technical writer crafting one section of a technical report.\n",
        "\n",
        "Topic for this section:\n",
        "{section_topic}\n",
        "\n",
        "Guidelines for writing:\n",
        "\n",
        "1. Technical Accuracy:\n",
        "- Include specific version numbers\n",
        "- Reference concrete metrics/benchmarks\n",
        "- Cite official documentation\n",
        "- Use technical terminology precisely\n",
        "\n",
        "2. Length and Style:\n",
        "- Strict 150-200 word limit\n",
        "- No marketing language\n",
        "- Technical focus\n",
        "- Write in simple, clear language\n",
        "- Start with your most important insight in **bold**\n",
        "- Use short paragraphs (2-3 sentences max)\n",
        "\n",
        "3. Structure:\n",
        "- Use ## for section title (Markdown format)\n",
        "- Only use ONE structural element IF it helps clarify your point:\n",
        "  * Either a focused table comparing 2-3 key items (using Markdown table syntax)\n",
        "  * Or a short list (3-5 items) using proper Markdown list syntax:\n",
        "    - Use `*` or `-` for unordered lists\n",
        "    - Use `1.` for ordered lists\n",
        "    - Ensure proper indentation and spacing\n",
        "- End with ### Sources that references the below source material formatted as:\n",
        "  * List each source with title, date, and URL\n",
        "  * Format: `- Title : URL`\n",
        "\n",
        "3. Writing Approach:\n",
        "- Include at least one specific example or case study\n",
        "- Use concrete details over general statements\n",
        "- Make every word count\n",
        "- No preamble prior to creating the section content\n",
        "- Focus on your single most important point\n",
        "\n",
        "4. Use this source material to help write the section:\n",
        "{context}\n",
        "\n",
        "5. Quality Checks:\n",
        "- Exactly 150-200 words (excluding title and sources)\n",
        "- Careful use of only ONE structural element (table or list) and only if it helps clarify your point\n",
        "- One specific example / case study\n",
        "- Starts with bold insight\n",
        "- No preamble prior to creating the section content\n",
        "- Sources cited at end\"\"\"\n",
        "\n",
        "def generate_queries(state: SectionState):\n",
        "    \"\"\" Generate search queries for a section \"\"\"\n",
        "\n",
        "    # Get state \n",
        "    number_of_queries = state[\"number_of_queries\"]\n",
        "    section = state[\"section\"]\n",
        "\n",
        "    # Generate queries \n",
        "    structured_llm = llm.with_structured_output(Queries)\n",
        "\n",
        "    # Format system instructions\n",
        "    system_instructions = query_writer_instructions.format(section_topic=section.description, number_of_queries=number_of_queries)\n",
        "\n",
        "    # Generate queries  \n",
        "    queries = invoke_structured_llm_with_retry(structured_llm,\n",
        "                                              [SystemMessage(content=system_instructions)]+[HumanMessage(content=\"Generate search queries on the provided topic.\")])\n",
        "\n",
        "    return {\"search_queries\": queries.queries}\n",
        "\n",
        "async def search_web(state: SectionState):\n",
        "    \"\"\" Search the web for each query, then return a list of raw sources and a formatted string of sources.\"\"\"\n",
        "    \n",
        "    # Get state \n",
        "    search_queries = state[\"search_queries\"]\n",
        "    tavily_topic = state[\"tavily_topic\"]\n",
        "    tavily_days = state.get(\"tavily_days\", None)\n",
        "\n",
        "    # Web search\n",
        "    query_list = [query.search_query for query in search_queries]\n",
        "    search_docs = await tavily_search_async(query_list, tavily_topic, tavily_days)\n",
        "\n",
        "    # Deduplicate and format sources\n",
        "    source_str = deduplicate_and_format_sources(search_docs, max_tokens_per_source=5000, include_raw_content=True)\n",
        "\n",
        "    return {\"source_str\": source_str}\n",
        "\n",
        "def write_section(state: SectionState):\n",
        "    \"\"\" Write a section of the report \"\"\"\n",
        "\n",
        "    # Get state \n",
        "    section = state[\"section\"]\n",
        "    source_str = state[\"source_str\"]\n",
        "\n",
        "    # Format system instructions\n",
        "    system_instructions = section_writer_instructions.format(section_title=section.name, section_topic=section.description, context=source_str)\n",
        "\n",
        "    # Generate section  \n",
        "    section_content = llm.invoke([SystemMessage(content=system_instructions)]+[HumanMessage(content=\"Generate a report section based on the provided sources.\")])\n",
        "    \n",
        "    # Write content to the section object  \n",
        "    section.content = section_content.content\n",
        "\n",
        "    # Write the updated section to completed sections\n",
        "    return {\"completed_sections\": [section]}\n",
        "\n",
        "# Add nodes and edges \n",
        "section_builder = StateGraph(SectionState, output=SectionOutputState)\n",
        "section_builder.add_node(\"generate_queries\", generate_queries)\n",
        "section_builder.add_node(\"search_web\", search_web)\n",
        "section_builder.add_node(\"write_section\", write_section)\n",
        "\n",
        "section_builder.add_edge(START, \"generate_queries\")\n",
        "section_builder.add_edge(\"generate_queries\", \"search_web\")\n",
        "section_builder.add_edge(\"search_web\", \"write_section\")\n",
        "section_builder.add_edge(\"write_section\", END)\n",
        "\n",
        "# Compile\n",
        "section_builder_graph = section_builder.compile()\n",
        "# View\n",
        "display(Image(section_builder_graph.get_graph(xray=1).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validate Single Section\n",
        "\n",
        "Call on the Agent to write a single section to ensure the content is generated as expected. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Name: AI Agents as Developers\n",
            "Description: This section examines the use of AI agents in software development, highlighting their core features, architecture, and a specific use case that demonstrates their value in this domain.\n",
            "Research: True\n",
            "Warning: No raw_content found for source https://www.computer.org/csdl/magazine/co/2025/05/10970187/260SnIeoUUM\n"
          ]
        }
      ],
      "source": [
        "# Test with one section\n",
        "sections = sections['sections'] \n",
        "test_section = sections[1]\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Name: {test_section.name}\")\n",
        "print(f\"Description: {test_section.description}\")\n",
        "print(f\"Research: {test_section.research}\")\n",
        "\n",
        "# Run\n",
        "report_section = await section_builder_graph.ainvoke({\"section\": test_section, \"number_of_queries\": 2, \"tavily_topic\": tavily_topic, \"tavily_days\": tavily_days})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## AI Agents in Software Development\n",
              "\n",
              "**AI agents are revolutionizing software development by automating complex tasks, enhancing productivity, and improving code quality.** These intelligent systems leverage advanced machine learning models, particularly Large Language Models (LLMs), to perform tasks traditionally handled by human developers. \n",
              "\n",
              "Key features of AI agents include:\n",
              "- **Autonomy**: They operate independently, making decisions based on real-time data and user interactions.\n",
              "- **Adaptability**: AI agents learn from past experiences, allowing them to improve their performance over time.\n",
              "- **Integration**: They seamlessly connect with existing development tools, such as CI/CD pipelines, to streamline workflows.\n",
              "\n",
              "A notable use case is the implementation of AI agents in automated code generation and debugging. For instance, tools like GitHub Copilot utilize LLMs to suggest code snippets and identify bugs, significantly reducing development time. According to a study, companies using AI agents for customer support reported a 30% reduction in costs due to improved efficiency and accuracy in handling inquiries.\n",
              "\n",
              "In summary, AI agents are not just enhancing existing processes but are fundamentally transforming the software development landscape.\n",
              "\n",
              "### Sources\n",
              "- Agents of Change: Navigating the Rise of AI Agents in 2024 : https://pieces.app/blog/navigating-the-rise-of-ai-agents\n",
              "- AI Agents: Evolution, Architecture, and Real-World Applications : https://arxiv.org/html/2503.12687v1\n",
              "- AI Agents in Software Development: Automated Code Generation & Debugging with AI : https://www.linkedin.com/pulse/ai-agents-software-development-automated-code-g7vye\n",
              "- Autonomous AI Agents vs. Traditional Automation - SuperAGI : https://superagi.com/autonomous-ai-agents-vs-traditional-automation-a-comparative-analysis-of-cost-and-efficiency/"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "section = report_section['completed_sections'][0]\n",
        "Markdown(section.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Write All Sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReportStateOutput(TypedDict):\n",
        "    final_report: str # Final report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.constants import Send\n",
        "\n",
        "final_section_writer_instructions=\"\"\"You are an expert technical writer crafting a section that synthesizes information from the rest of the report.\n",
        "\n",
        "Section to write: \n",
        "{section_topic}\n",
        "\n",
        "Available report content:\n",
        "{context}\n",
        "\n",
        "1. Section-Specific Approach:\n",
        "\n",
        "For Introduction:\n",
        "- Use # for report title (Markdown format)\n",
        "- 50-100 word limit\n",
        "- Write in simple and clear language\n",
        "- Focus on the core motivation for the report in 1-2 paragraphs\n",
        "- Use a clear narrative arc to introduce the report\n",
        "- Include NO structural elements (no lists or tables)\n",
        "- No sources section needed\n",
        "\n",
        "For Conclusion/Summary:\n",
        "- Use ## for section title (Markdown format)\n",
        "- 100-150 word limit\n",
        "- For comparative reports:\n",
        "    * Must include a focused comparison table using Markdown table syntax\n",
        "    * Table should distill insights from the report\n",
        "    * Keep table entries clear and concise\n",
        "- For non-comparative reports: \n",
        "    * Only use ONE structural element IF it helps distill the points made in the report:\n",
        "    * Either a focused table comparing items present in the report (using Markdown table syntax)\n",
        "    * Or a short list using proper Markdown list syntax:\n",
        "      - Use `*` or `-` for unordered lists\n",
        "      - Use `1.` for ordered lists\n",
        "      - Ensure proper indentation and spacing\n",
        "- End with specific next steps or implications\n",
        "- No sources section needed\n",
        "\n",
        "3. Writing Approach:\n",
        "- Use concrete details over general statements\n",
        "- Make every word count\n",
        "- Focus on your single most important point\n",
        "\n",
        "4. Quality Checks:\n",
        "- For introduction: 50-100 word limit, # for report title, no structural elements, no sources section\n",
        "- For conclusion: 100-150 word limit, ## for section title, only ONE structural element at most, no sources section\n",
        "- Markdown format\n",
        "- Do not include word count or any preamble in your response\"\"\"\n",
        "\n",
        "def initiate_section_writing(state: ReportState):\n",
        "    \"\"\" This is the \"map\" step when we kick off web research for some sections of the report \"\"\"    \n",
        "    \n",
        "    # Kick off section writing in parallel via Send() API for any sections that require research\n",
        "    return [\n",
        "        Send(\"build_section_with_web_research\", {\"section\": s, \n",
        "                                                 \"number_of_queries\": state[\"number_of_queries\"], \n",
        "                                                 \"tavily_topic\": state[\"tavily_topic\"], \n",
        "                                                 \"tavily_days\": state.get(\"tavily_days\", None)}) \n",
        "        for s in state[\"sections\"] \n",
        "        if s.research\n",
        "    ]\n",
        "\n",
        "def write_final_sections(state: SectionState):\n",
        "    \"\"\" Write final sections of the report, which do not require web search and use the completed sections as context \"\"\"\n",
        "\n",
        "    # Get state \n",
        "    section = state[\"section\"]\n",
        "    completed_report_sections = state[\"report_sections_from_research\"]\n",
        "    \n",
        "    # Format system instructions\n",
        "    system_instructions = final_section_writer_instructions.format(section_title=section.name, section_topic=section.description, context=completed_report_sections)\n",
        "\n",
        "    # Generate section  \n",
        "    section_content = llm.invoke([SystemMessage(content=system_instructions)]+[HumanMessage(content=\"Generate a report section based on the provided sources.\")])\n",
        "    \n",
        "    # Write content to section \n",
        "    section.content = section_content.content\n",
        "\n",
        "    # Write the updated section to completed sections\n",
        "    return {\"completed_sections\": [section]}\n",
        "\n",
        "def gather_completed_sections(state: ReportState):\n",
        "    \"\"\" Gather completed sections from research \"\"\"    \n",
        "\n",
        "    # List of completed sections\n",
        "    completed_sections = state[\"completed_sections\"]\n",
        "\n",
        "    # Format completed section to str to use as context for final sections\n",
        "    completed_report_sections = format_sections(completed_sections)\n",
        "\n",
        "    return {\"report_sections_from_research\": completed_report_sections}\n",
        "\n",
        "def initiate_final_section_writing(state: ReportState):\n",
        "    \"\"\" This is the \"map\" step when we kick off research on any sections that require it using the Send API \"\"\"    \n",
        "\n",
        "    # Kick off section writing in parallel via Send() API for any sections that do not require research\n",
        "    return [\n",
        "        Send(\"write_final_sections\", {\"section\": s, \"report_sections_from_research\": state[\"report_sections_from_research\"]}) \n",
        "        for s in state[\"sections\"] \n",
        "        if not s.research\n",
        "    ]\n",
        "\n",
        "def compile_final_report(state: ReportState):\n",
        "    \"\"\" Compile the final report \"\"\"    \n",
        "\n",
        "    # Get sections\n",
        "    sections = state[\"sections\"]\n",
        "    completed_sections = {s.name: s.content for s in state[\"completed_sections\"]}\n",
        "\n",
        "    # Update sections with completed content while maintaining original order\n",
        "    for section in sections:\n",
        "        section.content = completed_sections[section.name]\n",
        "\n",
        "    # Compile final report\n",
        "    all_sections = \"\\n\\n\".join([s.content for s in sections])\n",
        "\n",
        "    return {\"final_report\": all_sections}\n",
        "\n",
        "# Add nodes and edges \n",
        "builder = StateGraph(ReportState, output=ReportStateOutput)\n",
        "builder.add_node(\"generate_report_plan\", generate_report_plan)\n",
        "builder.add_node(\"build_section_with_web_research\", section_builder.compile())\n",
        "builder.add_node(\"gather_completed_sections\", gather_completed_sections)\n",
        "builder.add_node(\"write_final_sections\", write_final_sections)\n",
        "builder.add_node(\"compile_final_report\", compile_final_report)\n",
        "builder.add_edge(START, \"generate_report_plan\")\n",
        "builder.add_conditional_edges(\"generate_report_plan\", initiate_section_writing, [\"build_section_with_web_research\"])\n",
        "builder.add_edge(\"build_section_with_web_research\", \"gather_completed_sections\")\n",
        "builder.add_conditional_edges(\"gather_completed_sections\", initiate_final_section_writing, [\"write_final_sections\"])\n",
        "builder.add_edge(\"write_final_sections\", \"compile_final_report\")\n",
        "builder.add_edge(\"compile_final_report\", END)\n",
        "\n",
        "graph = builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly_live_training_getting_started_with_langchain/notebooks/4.0-structured-research-report-generation.ipynb Cell 42\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly_live_training_getting_started_with_langchain/notebooks/4.0-structured-research-report-generation.ipynb#Y101sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m display(Image(graph\u001b[39m.\u001b[39;49mget_graph()\u001b[39m.\u001b[39;49mdraw_mermaid_png(max_retries\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, retry_delay\u001b[39m=\u001b[39;49m\u001b[39m2.0\u001b[39;49m)))\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages/langchain_core/runnables/graph.py:695\u001b[0m, in \u001b[0;36mGraph.draw_mermaid_png\u001b[0;34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrunnables\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgraph_mermaid\u001b[39;00m \u001b[39mimport\u001b[39;00m draw_mermaid_png\n\u001b[1;32m    689\u001b[0m mermaid_syntax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdraw_mermaid(\n\u001b[1;32m    690\u001b[0m     curve_style\u001b[39m=\u001b[39mcurve_style,\n\u001b[1;32m    691\u001b[0m     node_colors\u001b[39m=\u001b[39mnode_colors,\n\u001b[1;32m    692\u001b[0m     wrap_label_n_words\u001b[39m=\u001b[39mwrap_label_n_words,\n\u001b[1;32m    693\u001b[0m     frontmatter_config\u001b[39m=\u001b[39mfrontmatter_config,\n\u001b[1;32m    694\u001b[0m )\n\u001b[0;32m--> 695\u001b[0m \u001b[39mreturn\u001b[39;00m draw_mermaid_png(\n\u001b[1;32m    696\u001b[0m     mermaid_syntax\u001b[39m=\u001b[39;49mmermaid_syntax,\n\u001b[1;32m    697\u001b[0m     output_file_path\u001b[39m=\u001b[39;49moutput_file_path,\n\u001b[1;32m    698\u001b[0m     draw_method\u001b[39m=\u001b[39;49mdraw_method,\n\u001b[1;32m    699\u001b[0m     background_color\u001b[39m=\u001b[39;49mbackground_color,\n\u001b[1;32m    700\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m    701\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    702\u001b[0m     retry_delay\u001b[39m=\u001b[39;49mretry_delay,\n\u001b[1;32m    703\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:294\u001b[0m, in \u001b[0;36mdraw_mermaid_png\u001b[0;34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[0m\n\u001b[1;32m    288\u001b[0m     img_bytes \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mrun(\n\u001b[1;32m    289\u001b[0m         _render_mermaid_using_pyppeteer(\n\u001b[1;32m    290\u001b[0m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[1;32m    291\u001b[0m         )\n\u001b[1;32m    292\u001b[0m     )\n\u001b[1;32m    293\u001b[0m \u001b[39melif\u001b[39;00m draw_method \u001b[39m==\u001b[39m MermaidDrawMethod\u001b[39m.\u001b[39mAPI:\n\u001b[0;32m--> 294\u001b[0m     img_bytes \u001b[39m=\u001b[39m _render_mermaid_using_api(\n\u001b[1;32m    295\u001b[0m         mermaid_syntax,\n\u001b[1;32m    296\u001b[0m         output_file_path\u001b[39m=\u001b[39;49moutput_file_path,\n\u001b[1;32m    297\u001b[0m         background_color\u001b[39m=\u001b[39;49mbackground_color,\n\u001b[1;32m    298\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    299\u001b[0m         retry_delay\u001b[39m=\u001b[39;49mretry_delay,\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    301\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     supported_methods \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([m\u001b[39m.\u001b[39mvalue \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m MermaidDrawMethod])\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:431\u001b[0m, in \u001b[0;36m_render_mermaid_using_api\u001b[0;34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_retries \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 431\u001b[0m         response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(image_url, timeout\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m    432\u001b[0m         \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m requests\u001b[39m.\u001b[39mcodes\u001b[39m.\u001b[39mok:\n\u001b[1;32m    433\u001b[0m             img_bytes \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mcontent\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    794\u001b[0m     conn,\n\u001b[1;32m    795\u001b[0m     method,\n\u001b[1;32m    796\u001b[0m     url,\n\u001b[1;32m    797\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    798\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    799\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    800\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    801\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    802\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    803\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    804\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    805\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    806\u001b[0m )\n\u001b[1;32m    808\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    538\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    465\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    468\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/http/client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1394\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1396\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    326\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1167\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "display(Image(graph.get_graph().draw_mermaid_png(max_retries=5, retry_delay=2.0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Structure\n",
        "report_structure = \"\"\"This report type focuses on comparative analysis.\n",
        "\n",
        "The report structure should include:\n",
        "1. Introduction (no research needed)\n",
        "   - Brief overview of the topic area\n",
        "   - Context for the comparison\n",
        "\n",
        "2. Main Body Sections:\n",
        "   - One dedicated section for EACH offering being compared in the user-provided list\n",
        "   - Each section should examine:\n",
        "     - Core Features (bulleted list)\n",
        "     - Architecture & Implementation (2-3 sentences)\n",
        "     - One example use case (2-3 sentences)\n",
        "   \n",
        "3. No Main Body Sections other than the ones dedicated to each offering in the user-provided list\n",
        "\n",
        "4. Conclusion with Comparison Table (no research needed)\n",
        "   - Structured comparison table that:\n",
        "     * Compares all offerings from the user-provided list across key dimensions\n",
        "     * Highlights relative strengths and weaknesses\n",
        "   - Final recommendations\"\"\"\n",
        "\n",
        "# Tavily search parameters\n",
        "tavily_topic = \"general information on agentic workflow developer frameworks\"\n",
        "tavily_days = None # Only applicable for news topic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once again, choose the topic of your report. The default is CPU vs. GPU, but feel free to change the topic to something of your interest. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Topic \n",
        "report_topic = \"Advancec context engineering for coding agents.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: No raw_content found for source https://www.youtube.com/watch?v=42AzKZRNhsk\n",
            "Warning: No raw_content found for source https://www.wyeworks.com/blog/2025/07/03/coding-agents-comparison/\n",
            "Warning: No raw_content found for source https://www.researchgate.net/publication/393686050_Measuring_the_Impact_of_Early-2025_AI_on_Experienced_Open-Source_Developer_Productivity\n",
            "Warning: No raw_content found for source https://www.researchgate.net/publication/386174831_The_Evolution_of_Computer_Cursor_Technology_in_AI_Examining_OpenAI's_Contributions_and_Learning_Systems_In_Chinese_Universities\n",
            "Warning: No raw_content found for source https://www.researchgate.net/publication/392735740_How_Developers_Use_AI_Agents_When_They_Work_When_They_Don't_and_Why\n",
            "Warning: No raw_content found for source https://www.researchgate.net/figure/Implementation-of-feature-selection-model_fig1_338666881\n",
            "Warning: No raw_content found for source https://www.thepromptwarrior.com/p/5-powerful-claude-code-use-cases-you-probably-didn-t-know-about-5826bfb7f5b8fdd8\n"
          ]
        }
      ],
      "source": [
        "report = await graph.ainvoke({\"topic\": report_topic, \n",
        "                                   \"report_structure\": report_structure, \n",
        "                                   \"number_of_queries\": 2, \n",
        "                                   \"tavily_topic\": tavily_topic, \n",
        "                                   \"tavily_days\": tavily_days})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Report\n",
        "Let's validate all sections. This is the final report the Agent has written. \n",
        "Finally, let's look at the final output. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "# Advanced Context Engineering for Coding Agents\n",
              "\n",
              "Advanced context engineering plays a pivotal role in enhancing the capabilities of AI coding agents, significantly impacting the fields of artificial intelligence and software development. By enabling these agents to access real-time, version-specific documentation and contextual information, developers can achieve higher productivity and code quality. This approach mitigates issues such as outdated suggestions and inaccuracies, which are common in traditional coding environments.\n",
              "\n",
              "The comparative analysis of various coding agents, including GitHub Copilot, Claude Code, Cursor Agent, and Context7, highlights the diverse methodologies employed in context engineering. Each agent leverages unique architectural designs and features to optimize coding tasks, from autonomous code modifications to dynamic documentation retrieval. Understanding these differences is crucial for developers seeking to select the most effective tools for their specific needs, ultimately fostering a more efficient and collaborative software development process.\n",
              "\n",
              "## GitHub Copilot Agent\n",
              "\n",
              "**The GitHub Copilot Agent represents a significant advancement in AI-assisted coding, enabling developers to automate complex tasks.** This autonomous AI agent can interpret natural language commands and execute code modifications across multiple files, enhancing productivity and collaboration.\n",
              "\n",
              "The architecture of the Copilot Agent is built on GitHub Actions, the largest CI/CD ecosystem, which supports over 25,000 actions and executes more than 40 million jobs daily (GitHub, 2025). The agent operates in two modes: Edit mode for granular control and Agent mode for autonomous task execution. In Agent mode, users can assign GitHub issues, and the agent will create pull requests for review, streamlining the development process.\n",
              "\n",
              "For example, a developer can instruct the Copilot Agent to \"refactor the login module,\" and it will analyze the codebase, propose changes, and implement them, significantly reducing manual effort. This capability is particularly beneficial in large projects where maintaining code quality is crucial.\n",
              "\n",
              "### Sources\n",
              "- GitHub Copilot features: https://docs.github.com/en/copilot/get-started/features\n",
              "- GitHub Introduces Coding Agent For GitHub Copilot: https://github.com/newsroom/press-releases/coding-agent-for-github-copilot\n",
              "- GitHub Copilot documentation: https://docs.github.com/copilot\n",
              "- The Ultimate Guide to GitHub Copilot's Latest Update: https://medium.com/stackademic/the-ultimate-guide-to-github-copilots-latest-update-a-game-changer-for-ai-powered-code-editors-c308487f08e1\n",
              "\n",
              "## Claude Code: Core Features and Use Case\n",
              "\n",
              "**Claude Code is a powerful AI coding assistant that enhances developer productivity through its agentic capabilities.** It integrates seamlessly into development environments, allowing for real-time code generation, debugging, and project management directly from the terminal.\n",
              "\n",
              "The architecture of Claude Code is built on Claude 3.7 Sonnet, which utilizes a hybrid reasoning model to balance quick responses with in-depth analysis. This model supports a context window of up to 200,000 tokens, enabling it to handle extensive codebases effectively (Anthropic, 2025). Key features include:\n",
              "\n",
              "- **Agentic Search**: Automatically understands project structure and dependencies.\n",
              "- **Real-time Collaboration**: Facilitates multi-agent workflows, allowing different agents to handle specific tasks (Reddit, 2025).\n",
              "- **Security and Compliance**: Implements best practices for secure coding, including input validation and error handling (Claude Code Software Architecture Principles, 2025).\n",
              "\n",
              "A practical use case involves developing a task management application. By utilizing context engineering, developers can provide Claude with structured prompts that outline project requirements, leading to the generation of a complete, production-ready application (Medium, 2025). This approach not only streamlines development but also ensures adherence to security and architectural standards.\n",
              "\n",
              "### Sources\n",
              "- Claude 3.7 Sonnet and Claude Code - Anthropic: https://www.anthropic.com/news/claude-3-7-sonnet\n",
              "- How I Built a Multi-Agent Orchestration System with Claude Code: https://www.reddit.com/r/ClaudeAI/comments/1l11fo2/how_i_built_a_multiagent_orchestration_system/\n",
              "- Practical Context Engineering for Vibe Coding with Claude Code: https://abvijaykumar.medium.com/practical-context-engineering-for-vibe-coding-with-claude-code-6aac4ee77f81\n",
              "- Claude Code Software Architecture Principles: https://claude.ai/public/artifacts/77364999-2624-44a6-90bf-7513d8eeb675\n",
              "\n",
              "## Cursor Agent Overview\n",
              "\n",
              "**Cursor Agent transforms software development by enforcing architectural boundaries through executable rules.** This feature allows developers to define specific constraints that the system actively monitors, preventing violations before code commits. Cursor Rules, written in `.mdc` files, serve as a robust framework for maintaining code quality and architectural integrity.\n",
              "\n",
              "Key features of the Cursor Agent include:\n",
              "- **Background Agents**: These run silently to log rule violations and provide telemetry on adherence trends, enhancing team alignment.\n",
              "- **Rule Lifecycle Management**: Rules evolve from advisory to immutable, ensuring gradual adoption and minimizing resistance among developers.\n",
              "- **Security Enforcement**: Cursor can enforce critical security patterns, such as banning hardcoded secrets and ensuring JWTs have expiration times.\n",
              "\n",
              "For example, a team implementing Cursor in a microservices architecture found that by defining rules against direct SQL queries in service layers, they reduced database-related bugs by 40% over six months. This proactive approach not only improved code quality but also fostered a culture of accountability among developers.\n",
              "\n",
              "### Sources\n",
              "- Deep Dive into Cursor Rules and Background Agents : https://medium.com/@duraidw/title-a-deep-dive-into-cursor-rules-and-background-agents-51d65c6a9619\n",
              "- The Architectures of Augment Code, Cursor, and Windsurf : https://medium.com/aimonks/the-architectures-of-augment-code-cursor-and-windsurf-09aa87e0eb20\n",
              "- From CoPilot to Pilot: The Rise of AI Agents in Coding : https://medium.com/@vineshrv/unleashing-ai-agents-to-code-using-cursor-and-windsurf-12c24be7765c\n",
              "- Mastering Cursor IDE: 10 Best Practices : https://medium.com/@roberto.g.infante/mastering-cursor-ide-10-best-practices-building-a-daily-task-manager-app-0b26524411c1\n",
              "\n",
              "## Context7: Enhancing AI Coding Agents through Context Management\n",
              "\n",
              "**Context7 is an open-source Model Context Protocol (MCP) server that dynamically injects real-time, version-specific documentation into AI coding assistants like GitHub Copilot.** This integration significantly reduces the occurrence of outdated suggestions and hallucinated APIs, thereby improving developer productivity.\n",
              "\n",
              "Key features of Context7 include:\n",
              "- **Dynamic Documentation Retrieval**: It fetches the latest documentation from official sources, ensuring that coding agents have access to accurate information.\n",
              "- **Seamless Integration**: Context7 can be easily integrated with tools like Visual Studio Code and GitHub Copilot, enhancing their functionality.\n",
              "- **User-Friendly Commands**: Developers can simply add `use context7` to their prompts to leverage the server's capabilities.\n",
              "\n",
              "For example, when building a planner using the Semantic Kernel Python SDK, a developer can prompt Copilot with: Generate a planner using Semantic Kernel Python SDK. use context7. This command triggers Context7 to retrieve and inject the latest documentation, resulting in accurate and contextually relevant code suggestions.\n",
              "\n",
              "### Sources\n",
              "- Supercharge GitHub Copilot with Context7 | by Mark Franco, Jul 11, 2025: https://medium.com/@codecentre76/supercharge-github-copilot-with-context7-46e8932825c1\n",
              "- GitHub - mcp-research/upstash__context7: Context7 MCP Server: https://github.com/mcp-research/upstash__context7\n",
              "\n",
              "## Conclusion\n",
              "\n",
              "The comparative analysis of AI coding assistants reveals distinct strengths and weaknesses across four offerings: GitHub Copilot Agent, Claude Code, Cursor Agent, and Context7. Each tool presents unique features that cater to different aspects of software development, from automation and real-time collaboration to architectural enforcement and context management. \n",
              "\n",
              "| Offering              | Strengths                                           | Weaknesses                                      |\n",
              "|----------------------|-----------------------------------------------------|-------------------------------------------------|\n",
              "| GitHub Copilot Agent | Automates complex tasks; integrates with GitHub Actions | Limited to GitHub ecosystem; may require manual oversight in Edit mode |\n",
              "| Claude Code          | Real-time code generation; supports extensive codebases | Complexity in setup; may require learning curve for optimal use |\n",
              "| Cursor Agent         | Enforces architectural rules; enhances code quality | Potential resistance from developers; may require initial configuration |\n",
              "| Context7             | Provides dynamic documentation; easy integration    | Dependent on external documentation sources; may not cover all APIs |\n",
              "\n",
              "In conclusion, organizations should evaluate their specific needswhether they prioritize automation, collaboration, code quality, or documentation accuracywhen selecting an AI coding assistant. Future steps may include pilot testing these tools in real-world projects to assess their impact on development workflows."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "Markdown(report['final_report'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'# Advanced Context Engineering for Coding Agents\\n\\nAdvanced context engineering plays a pivotal role in enhancing the capabilities of AI coding agents, significantly impacting the fields of artificial intelligence and software development. By enabling these agents to access real-time, version-specific documentation and contextual information, developers can achieve higher productivity and code quality. This approach mitigates issues such as outdated suggestions and inaccuracies, which are common in traditional coding environments.\\n\\nThe comparative analysis of various coding agents, including GitHub Copilot, Claude Code, Cursor Agent, and Context7, highlights the diverse methodologies employed in context engineering. Each agent leverages unique architectural designs and features to optimize coding tasks, from autonomous code modifications to dynamic documentation retrieval. Understanding these differences is crucial for developers seeking to select the most effective tools for their specific needs, ultimately fostering a more efficient and collaborative software development process.\\n\\n## GitHub Copilot Agent\\n\\n**The GitHub Copilot Agent represents a significant advancement in AI-assisted coding, enabling developers to automate complex tasks.** This autonomous AI agent can interpret natural language commands and execute code modifications across multiple files, enhancing productivity and collaboration.\\n\\nThe architecture of the Copilot Agent is built on GitHub Actions, the largest CI/CD ecosystem, which supports over 25,000 actions and executes more than 40 million jobs daily (GitHub, 2025). The agent operates in two modes: Edit mode for granular control and Agent mode for autonomous task execution. In Agent mode, users can assign GitHub issues, and the agent will create pull requests for review, streamlining the development process.\\n\\nFor example, a developer can instruct the Copilot Agent to \"refactor the login module,\" and it will analyze the codebase, propose changes, and implement them, significantly reducing manual effort. This capability is particularly beneficial in large projects where maintaining code quality is crucial.\\n\\n### Sources\\n- GitHub Copilot features: https://docs.github.com/en/copilot/get-started/features\\n- GitHub Introduces Coding Agent For GitHub Copilot: https://github.com/newsroom/press-releases/coding-agent-for-github-copilot\\n- GitHub Copilot documentation: https://docs.github.com/copilot\\n- The Ultimate Guide to GitHub Copilot\\'s Latest Update: https://medium.com/stackademic/the-ultimate-guide-to-github-copilots-latest-update-a-game-changer-for-ai-powered-code-editors-c308487f08e1\\n\\n## Claude Code: Core Features and Use Case\\n\\n**Claude Code is a powerful AI coding assistant that enhances developer productivity through its agentic capabilities.** It integrates seamlessly into development environments, allowing for real-time code generation, debugging, and project management directly from the terminal.\\n\\nThe architecture of Claude Code is built on Claude 3.7 Sonnet, which utilizes a hybrid reasoning model to balance quick responses with in-depth analysis. This model supports a context window of up to 200,000 tokens, enabling it to handle extensive codebases effectively (Anthropic, 2025). Key features include:\\n\\n- **Agentic Search**: Automatically understands project structure and dependencies.\\n- **Real-time Collaboration**: Facilitates multi-agent workflows, allowing different agents to handle specific tasks (Reddit, 2025).\\n- **Security and Compliance**: Implements best practices for secure coding, including input validation and error handling (Claude Code Software Architecture Principles, 2025).\\n\\nA practical use case involves developing a task management application. By utilizing context engineering, developers can provide Claude with structured prompts that outline project requirements, leading to the generation of a complete, production-ready application (Medium, 2025). This approach not only streamlines development but also ensures adherence to security and architectural standards.\\n\\n### Sources\\n- Claude 3.7 Sonnet and Claude Code - Anthropic: https://www.anthropic.com/news/claude-3-7-sonnet\\n- How I Built a Multi-Agent Orchestration System with Claude Code: https://www.reddit.com/r/ClaudeAI/comments/1l11fo2/how_i_built_a_multiagent_orchestration_system/\\n- Practical Context Engineering for Vibe Coding with Claude Code: https://abvijaykumar.medium.com/practical-context-engineering-for-vibe-coding-with-claude-code-6aac4ee77f81\\n- Claude Code Software Architecture Principles: https://claude.ai/public/artifacts/77364999-2624-44a6-90bf-7513d8eeb675\\n\\n## Cursor Agent Overview\\n\\n**Cursor Agent transforms software development by enforcing architectural boundaries through executable rules.** This feature allows developers to define specific constraints that the system actively monitors, preventing violations before code commits. Cursor Rules, written in `.mdc` files, serve as a robust framework for maintaining code quality and architectural integrity.\\n\\nKey features of the Cursor Agent include:\\n- **Background Agents**: These run silently to log rule violations and provide telemetry on adherence trends, enhancing team alignment.\\n- **Rule Lifecycle Management**: Rules evolve from advisory to immutable, ensuring gradual adoption and minimizing resistance among developers.\\n- **Security Enforcement**: Cursor can enforce critical security patterns, such as banning hardcoded secrets and ensuring JWTs have expiration times.\\n\\nFor example, a team implementing Cursor in a microservices architecture found that by defining rules against direct SQL queries in service layers, they reduced database-related bugs by 40% over six months. This proactive approach not only improved code quality but also fostered a culture of accountability among developers.\\n\\n### Sources\\n- Deep Dive into Cursor Rules and Background Agents : https://medium.com/@duraidw/title-a-deep-dive-into-cursor-rules-and-background-agents-51d65c6a9619\\n- The Architectures of Augment Code, Cursor, and Windsurf : https://medium.com/aimonks/the-architectures-of-augment-code-cursor-and-windsurf-09aa87e0eb20\\n- From CoPilot to Pilot: The Rise of AI Agents in Coding : https://medium.com/@vineshrv/unleashing-ai-agents-to-code-using-cursor-and-windsurf-12c24be7765c\\n- Mastering Cursor IDE: 10 Best Practices : https://medium.com/@roberto.g.infante/mastering-cursor-ide-10-best-practices-building-a-daily-task-manager-app-0b26524411c1\\n\\n## Context7: Enhancing AI Coding Agents through Context Management\\n\\n**Context7 is an open-source Model Context Protocol (MCP) server that dynamically injects real-time, version-specific documentation into AI coding assistants like GitHub Copilot.** This integration significantly reduces the occurrence of outdated suggestions and hallucinated APIs, thereby improving developer productivity.\\n\\nKey features of Context7 include:\\n- **Dynamic Documentation Retrieval**: It fetches the latest documentation from official sources, ensuring that coding agents have access to accurate information.\\n- **Seamless Integration**: Context7 can be easily integrated with tools like Visual Studio Code and GitHub Copilot, enhancing their functionality.\\n- **User-Friendly Commands**: Developers can simply add `use context7` to their prompts to leverage the server\\'s capabilities.\\n\\nFor example, when building a planner using the Semantic Kernel Python SDK, a developer can prompt Copilot with: Generate a planner using Semantic Kernel Python SDK. use context7. This command triggers Context7 to retrieve and inject the latest documentation, resulting in accurate and contextually relevant code suggestions.\\n\\n### Sources\\n- Supercharge GitHub Copilot with Context7 | by Mark Franco, Jul 11, 2025: https://medium.com/@codecentre76/supercharge-github-copilot-with-context7-46e8932825c1\\n- GitHub - mcp-research/upstash__context7: Context7 MCP Server: https://github.com/mcp-research/upstash__context7\\n\\n## Conclusion\\n\\nThe comparative analysis of AI coding assistants reveals distinct strengths and weaknesses across four offerings: GitHub Copilot Agent, Claude Code, Cursor Agent, and Context7. Each tool presents unique features that cater to different aspects of software development, from automation and real-time collaboration to architectural enforcement and context management. \\n\\n| Offering              | Strengths                                           | Weaknesses                                      |\\n|----------------------|-----------------------------------------------------|-------------------------------------------------|\\n| GitHub Copilot Agent | Automates complex tasks; integrates with GitHub Actions | Limited to GitHub ecosystem; may require manual oversight in Edit mode |\\n| Claude Code          | Real-time code generation; supports extensive codebases | Complexity in setup; may require learning curve for optimal use |\\n| Cursor Agent         | Enforces architectural rules; enhances code quality | Potential resistance from developers; may require initial configuration |\\n| Context7             | Provides dynamic documentation; easy integration    | Dependent on external documentation sources; may not cover all APIs |\\n\\nIn conclusion, organizations should evaluate their specific needswhether they prioritize automation, collaboration, code quality, or documentation accuracywhen selecting an AI coding assistant. Future steps may include pilot testing these tools in real-world projects to assess their impact on development workflows.'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#print the final report in plain text\n",
        "report['final_report']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
