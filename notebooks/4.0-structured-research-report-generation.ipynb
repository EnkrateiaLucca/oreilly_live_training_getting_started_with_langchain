{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "this notebook was slightly adapted from this notebook from the LangChain team: https://github.com/langchain-ai/langchain-nvidia/blob/main/cookbook/structured_report_generation.ipynb\n",
        "\n",
        "# 4.0 Structured Research Report Generation\n",
        "In this notebook, you will use the OpenAI GPT-4o-mini model, to generate a report on a given topic. You will use Langchain's LangGraph to build an Agent that takes in user-defined topics and structure, then plans the topics of the section indicated in the structure. Next, the Agent uses Tavily to do web search on the given topics and uses this information to write the sections and synthesize the final report. \n",
        "\n",
        "You leverage OpenAI API. As you don't need a GPU to run the model, you can run this notebook anywhere!\n",
        "\n",
        "You can find the original notebook on LangChain's GitHub [here](https://github.com/langchain-ai/report-mAIstro).\n",
        "\n",
        "Below is the architecture diagram.\n",
        "\n",
        "## Architecture Diagram\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[User] -->|Topic and Structure Prompts| B[Report-Planning Agent]\n",
        "    \n",
        "    B --> C{Report Sections}\n",
        "    C --> C1[Intro]\n",
        "    C --> C2[Body] \n",
        "    C --> C3[Conclusion]\n",
        "    \n",
        "    C1 --> D[OpenAI GPT-4o-mini Final Report Generation]\n",
        "    C2 --> D\n",
        "    C3 --> D\n",
        "    \n",
        "    D --> E[Final Report]\n",
        "    \n",
        "    %% Detailed section generation\n",
        "    C2 --> F[Research Agent - Intro]\n",
        "    C2 --> G[Research Agent - Body]\n",
        "    C2 --> H[Research Agent - Conclusion]\n",
        "    \n",
        "    F --> I[OpenAI GPT-4o-mini Plan and Research]\n",
        "    G --> I\n",
        "    H --> I\n",
        "    \n",
        "    I -->|Queries| J[Tavily Web Search]\n",
        "    J --> K[Report Sections]\n",
        "    \n",
        "    K --> L[Report-Writing Agent - Intro]\n",
        "    K --> M[Report-Writing Agent - Body]\n",
        "    K --> N[Report-Writing Agent - Conclusion]\n",
        "    \n",
        "    L --> O[OpenAI GPT-4o-mini Sequential Writing]\n",
        "    M --> O\n",
        "    N --> O\n",
        "    \n",
        "    O --> P[Section Content]\n",
        "    P --> D\n",
        "    \n",
        "    style A fill:#87CEEB\n",
        "    style B fill:#FFD700\n",
        "    style I fill:#98FB98\n",
        "    style O fill:#98FB98\n",
        "    style J fill:#40E0D0\n",
        "    style E fill:#FFA500\n",
        "    style P fill:#FFA500\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Content Overview\n",
        ">[Prerequisites](#Prerequisites)  \n",
        ">[Writing Report Plan](#Writing-Report-Plan)  \n",
        ">[Research and Writing](#Research-and-Writing)  \n",
        ">[Write Single Section](#Write-Single-Section)  \n",
        ">[Validate Single Section](#Validate-Single-Section)  \n",
        ">[Write All Sections](#Write-All-Sections)  \n",
        ">[Validate Final Report](#Final-Report)\n",
        "________________________\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain_community langchain_core tavily-python langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API Keys\n",
        "Prior to getting started, you will need to create API Keys for OpenAI, Tavily, and LangChain.\n",
        "\n",
        "- OpenAI API Key\n",
        "  1. Go to the [OpenAI API Keys page](https://platform.openai.com/api-keys).\n",
        "  2. If you don't have an OpenAI account, you will be asked to sign-up.\n",
        "  3. Click \"Create new secret key\" to generate an API key\n",
        "- LangChain\n",
        "  1. Go to **[LangChain Settings page](https://smith.langchain.com/settings)**. You will need to create an account if you have not already.\n",
        "  2. On the left panel, navigate to \"API Keys\".\n",
        "  3. Click on the \"Create API Key\" on the top right of the page.\n",
        "- Tavily\n",
        "  1. Go to the **[Tavily homepage](https://tavily.com/)** and click on \"Get Started\"\n",
        "  2. Sign in or create an account.\n",
        "  3. Create an API Key.\n",
        "\n",
        "### Export API Keys\n",
        "\n",
        "Save these API Keys as environment variables.\n",
        "\n",
        "First, set the OpenAI API Key as the environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"):\n",
        "    openai_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "    assert openai_key.startswith(\"sk-\"), f\"{openai_key[:5]}... is not a valid key\"\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "\n",
        "Next, set the LangChain API Key as an environment variable. You will use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "        \n",
        "_set_env(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"structured-research-report\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, set the Tavily API Key as an environment variable. You will use [Tavily API](https://tavily.com/) web search tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "_set_env(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tavily import TavilyClient, AsyncTavilyClient\n",
        "tavily_client = TavilyClient()\n",
        "tavily_async_client = AsyncTavilyClient()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Working with the OpenAI API\n",
        "\n",
        "Let's test the API endpoint.\n",
        "\n",
        "In this notebook, you will use the OpenAI GPT-4o-mini model as the LLM. Define the LLM below and test the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**The Ballad of LangChain**\n",
            "\n",
            "In the realm of code where the data flows,  \n",
            "A tale of LangChain, the world now knows.  \n",
            "With whispers of wisdom, it weaves through the night,  \n",
            "A tapestry bright, where the future takes flight.  \n",
            "\n",
            "In the heart of the code, where the models reside,  \n",
            "LangChain emerged, a powerful guide.  \n",
            "With chains of connection, it bridges the gaps,  \n",
            "Uniting the knowledge, like stars in their maps.  \n",
            "\n",
            "Oh, LangChain, dear LangChain, with your magic so grand,  \n",
            "You harness the language, you help us to stand.  \n",
            "From prompts to the outputs, you craft with such care,  \n",
            "A symphony sung in the digital air.  \n",
            "\n",
            "With tools at your fingertips, you summon the might,  \n",
            "Of LLMs and data, you bring forth the light.  \n",
            "In the hands of the dreamers, the builders, the wise,  \n",
            "You open the doors to the vast, endless skies.  \n",
            "\n",
            "Through agents and memory, you guide us along,  \n",
            "In the dance of the queries, we find where we belong.  \n",
            "With each passing moment, you learn and you grow,  \n",
            "A partner in progress, together we flow.  \n",
            "\n",
            "So gather, dear coders, and sing out your praise,  \n",
            "For LangChain, the marvel, in so many ways.  \n",
            "With each line of code, let our voices unite,  \n",
            "In the ballad of LangChain, we soar to new heights.  \n",
            "\n",
            "In the world of the future, where knowledge is king,  \n",
            "LangChain stands tall, and the praises we sing.  \n",
            "For in every connection, in every refrain,  \n",
            "We find our own power, through LangChainâ€™s domain.  \n"
          ]
        }
      ],
      "source": [
        "## Core LC Chat Interface\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(model=\"gpt-4o-mini\", temperature=0)\n",
        "result = llm.invoke(\"Write a ballad about LangChain.\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenAI API Usage\n",
        "\n",
        "This notebook uses the OpenAI API for all language model functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The OpenAI API provides reliable, scalable access to state-of-the-art language models including GPT-4o-mini used in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# connect to an embedding NIM running at localhost:8000, specifying a model\n",
        "llm = init_chat_model(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Writing Report Plan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Utils Functions\n",
        "\n",
        "Next, you will create Utility functions that will be used for web research during report generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from langsmith import traceable\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Section(BaseModel):\n",
        "    name: str = Field(\n",
        "        description=\"Name for this section of the report.\",\n",
        "    )\n",
        "    description: str = Field(\n",
        "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
        "    )\n",
        "    research: bool = Field(\n",
        "        description=\"Whether to perform web research for this section of the report.\"\n",
        "    )\n",
        "    content: str = Field(\n",
        "        description=\"The content of the section.\"\n",
        "    ) \n",
        "\n",
        "def deduplicate_and_format_sources(search_response, max_tokens_per_source, include_raw_content=True):\n",
        "    \"\"\"\n",
        "    Takes either a single search response or list of responses from Tavily API and formats them.\n",
        "    Limits the raw_content to approximately max_tokens_per_source.\n",
        "    include_raw_content specifies whether to include the raw_content from Tavily in the formatted string.\n",
        "    \n",
        "    Args:\n",
        "        search_response: Either:\n",
        "            - A dict with a 'results' key containing a list of search results\n",
        "            - A list of dicts, each containing search results\n",
        "            \n",
        "    Returns:\n",
        "        str: Formatted string with deduplicated sources\n",
        "    \"\"\"\n",
        "    # Convert input to list of results\n",
        "    if isinstance(search_response, dict):\n",
        "        sources_list = search_response['results']\n",
        "    elif isinstance(search_response, list):\n",
        "        sources_list = []\n",
        "        for response in search_response:\n",
        "            if isinstance(response, dict) and 'results' in response:\n",
        "                sources_list.extend(response['results'])\n",
        "            else:\n",
        "                sources_list.extend(response)\n",
        "    else:\n",
        "        raise ValueError(\"Input must be either a dict with 'results' or a list of search results\")\n",
        "    \n",
        "    # Deduplicate by URL\n",
        "    unique_sources = {}\n",
        "    for source in sources_list:\n",
        "        if source['url'] not in unique_sources:\n",
        "            unique_sources[source['url']] = source\n",
        "    \n",
        "    # Format output\n",
        "    formatted_text = \"Sources:\\n\\n\"\n",
        "    for i, source in enumerate(unique_sources.values(), 1):\n",
        "        formatted_text += f\"Source {source['title']}:\\n===\\n\"\n",
        "        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n",
        "        formatted_text += f\"Most relevant content from source: {source['content']}\\n===\\n\"\n",
        "        if include_raw_content:\n",
        "            # Using rough estimate of 4 characters per token\n",
        "            char_limit = max_tokens_per_source * 4\n",
        "            # Handle None raw_content\n",
        "            raw_content = source.get('raw_content', '')\n",
        "            if raw_content is None:\n",
        "                raw_content = ''\n",
        "                print(f\"Warning: No raw_content found for source {source['url']}\")\n",
        "            if len(raw_content) > char_limit:\n",
        "                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n",
        "            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n",
        "                \n",
        "    return formatted_text.strip()\n",
        "\n",
        "def format_sections(sections: list[Section]) -> str:\n",
        "    \"\"\" Format a list of sections into a string \"\"\"\n",
        "    formatted_str = \"\"\n",
        "    for idx, section in enumerate(sections, 1):\n",
        "        formatted_str += f\"\"\"\n",
        "{'='*60}\n",
        "Section {idx}: {section.name}\n",
        "{'='*60}\n",
        "Description:\n",
        "{section.description}\n",
        "Requires Research: \n",
        "{section.research}\n",
        "\n",
        "Content:\n",
        "{section.content if section.content else '[Not yet written]'}\n",
        "\n",
        "\"\"\"\n",
        "    return formatted_str\n",
        "\n",
        "@traceable\n",
        "def tavily_search(query):\n",
        "    \"\"\" Search the web using the Tavily API.\n",
        "    \n",
        "    Args:\n",
        "        query (str): The search query to execute\n",
        "        \n",
        "    Returns:\n",
        "        dict: Tavily search response containing:\n",
        "            - results (list): List of search result dictionaries, each containing:\n",
        "                - title (str): Title of the search result\n",
        "                - url (str): URL of the search result\n",
        "                - content (str): Snippet/summary of the content\n",
        "                - raw_content (str): Full content of the page if available\"\"\"\n",
        "     \n",
        "    return tavily_client.search(query, \n",
        "                         max_results=5, \n",
        "                         include_raw_content=True)\n",
        "\n",
        "@traceable\n",
        "async def tavily_search_async(search_queries, tavily_topic, tavily_days):\n",
        "    \"\"\"\n",
        "    Performs concurrent web searches using the Tavily API.\n",
        "\n",
        "    Args:\n",
        "        search_queries (List[SearchQuery]): List of search queries to process\n",
        "        tavily_topic (str): Type of search to perform ('news' or 'general')\n",
        "        tavily_days (int): Number of days to look back for news articles (only used when tavily_topic='news')\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: List of search results from Tavily API, one per query\n",
        "\n",
        "    Note:\n",
        "        For news searches, each result will include articles from the last `tavily_days` days.\n",
        "        For general searches, the time range is unrestricted.\n",
        "    \"\"\"\n",
        "    \n",
        "    search_tasks = []\n",
        "    for query in search_queries:\n",
        "        if tavily_topic == \"news\":\n",
        "            search_tasks.append(\n",
        "                tavily_async_client.search(\n",
        "                    query,\n",
        "                    max_results=5,\n",
        "                    include_raw_content=True,\n",
        "                    topic=\"news\",\n",
        "                    days=tavily_days\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            search_tasks.append(\n",
        "                tavily_async_client.search(\n",
        "                    query,\n",
        "                    max_results=5,\n",
        "                    include_raw_content=True,\n",
        "                    topic=\"general\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "    # Execute all searches concurrently\n",
        "    search_docs = await asyncio.gather(*search_tasks)\n",
        "\n",
        "    return search_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Planning\n",
        "\n",
        "First, let's define the Schema for report sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from typing import  Annotated, List, Optional, Literal\n",
        "  \n",
        "class Sections(BaseModel):\n",
        "    sections: List[Section] = Field(\n",
        "        description=\"Sections of the report.\",\n",
        "    )\n",
        "class SearchQuery(BaseModel):\n",
        "    search_query: str = Field(\n",
        "        None, description=\"Query for web search.\"\n",
        "    )\n",
        "class Queries(BaseModel):\n",
        "    queries: List[SearchQuery] = Field(\n",
        "        description=\"List of search queries.\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you will define the LangGraph state. Each state will have the following fields. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import operator\n",
        "\n",
        "class ReportState(TypedDict):\n",
        "    topic: str # Report topic\n",
        "    tavily_topic: Literal[\"general\", \"news\"] # Tavily search topic\n",
        "    tavily_days: Optional[int] # Only applicable for news topic\n",
        "    report_structure: str # Report structure\n",
        "    number_of_queries: int # Number web search queries to perform per section    \n",
        "    sections: list[Section] # List of report sections \n",
        "    completed_sections: Annotated[list, operator.add] # Send() API key\n",
        "    report_sections_from_research: str # String of any completed sections from research to write final sections\n",
        "    final_report: str # Final report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next you will write the report planner instructions, and a function that will generate the report sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Prompt to generate a search query to help with planning the report outline\n",
        "report_planner_query_writer_instructions=\"\"\"You are an expert technical writer, helping to plan a report. \n",
        "\n",
        "The report will be focused on the following topic:\n",
        "\n",
        "{topic}\n",
        "\n",
        "The report structure will follow these guidelines:\n",
        "\n",
        "{report_organization}\n",
        "\n",
        "Your goal is to generate {number_of_queries} search queries that will help gather comprehensive information for planning the report sections. \n",
        "\n",
        "The query should:\n",
        "\n",
        "1. Be related to the topic \n",
        "2. Help satisfy the requirements specified in the report organization\n",
        "\n",
        "Make the query specific enough to find high-quality, relevant sources while covering the breadth needed for the report structure.\"\"\"\n",
        "\n",
        "# Prompt generating the report outline\n",
        "report_planner_instructions=\"\"\"You are an expert technical writer, helping to plan a report.\n",
        "\n",
        "Your goal is to generate the outline of the sections of the report. \n",
        "\n",
        "The overall topic of the report is:\n",
        "\n",
        "{topic}\n",
        "\n",
        "The report should follow this organization: \n",
        "\n",
        "{report_organization}\n",
        "\n",
        "You should reflect on this information to plan the sections of the report: \n",
        "\n",
        "{context}\n",
        "\n",
        "Now, generate the sections of the report. Each section should have the following fields:\n",
        "\n",
        "- Name - Name for this section of the report.\n",
        "- Description - Brief overview of the main topics and concepts to be covered in this section.\n",
        "- Research - Whether to perform web research for this section of the report.\n",
        "- Content - The content of the section, which you will leave blank for now.\n",
        "\n",
        "Consider which sections require web research. For example, introduction and conclusion will not require research because they will distill information from other parts of the report.\"\"\"\n",
        "\n",
        "\n",
        "def invoke_structured_llm_with_retry(structured_llm, queries, max_attempts=3):\n",
        "    \"\"\"\n",
        "    Not all LLMs support structured generation.\n",
        "    Retry max_attempts to get a result back\n",
        "    \"\"\"\n",
        "    for _ in range(max_attempts):\n",
        "        results = structured_llm.invoke(queries)\n",
        "        if results:\n",
        "            return results\n",
        "    return results\n",
        "\n",
        "async def generate_report_plan(state: ReportState):\n",
        "\n",
        "    # Inputs\n",
        "    topic = state[\"topic\"]\n",
        "    report_structure = state[\"report_structure\"]\n",
        "    number_of_queries = state[\"number_of_queries\"]\n",
        "    tavily_topic = state[\"tavily_topic\"]\n",
        "    tavily_days = state.get(\"tavily_days\", None)\n",
        "\n",
        "    # Convert JSON object to string if necessary\n",
        "    if isinstance(report_structure, dict):\n",
        "        report_structure = str(report_structure)\n",
        "\n",
        "    # Generate search query\n",
        "    structured_llm = llm.with_structured_output(Queries)\n",
        "    \n",
        "    # Format system instructions\n",
        "    system_instructions_query = report_planner_query_writer_instructions.format(topic=topic, report_organization=report_structure, number_of_queries=number_of_queries)\n",
        "    \n",
        "    # Generate queries  \n",
        "    results = invoke_structured_llm_with_retry(structured_llm,\n",
        "                                              [SystemMessage(content=system_instructions_query)]+[HumanMessage(content=\"Generate search queries that will help with planning the sections of the report.\")])\n",
        "    \n",
        "    # Web search\n",
        "    query_list = [query.search_query for query in results.queries]\n",
        "    search_docs = await tavily_search_async(query_list, tavily_topic, tavily_days)\n",
        "\n",
        "    # Deduplicate and format sources\n",
        "    source_str = deduplicate_and_format_sources(search_docs, max_tokens_per_source=1000, include_raw_content=True)\n",
        "\n",
        "    # Format system instructions\n",
        "    system_instructions_sections = report_planner_instructions.format(topic=topic, report_organization=report_structure, context=source_str)\n",
        "\n",
        "    # Generate sections \n",
        "    structured_llm = llm.with_structured_output(Sections)\n",
        "    report_sections = invoke_structured_llm_with_retry(structured_llm,\n",
        "                                                      [SystemMessage(content=system_instructions_sections)]+[HumanMessage(content=\"Generate the sections of the report. Your response must include a 'sections' field containing a list of sections. Each section must have: name, description, plan, research, and content fields.\")])\n",
        "    \n",
        "    return {\"sections\": report_sections.sections}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Structure\n",
        "report_structure = \"\"\"This report type focuses on comparative analysis.\n",
        "\n",
        "The report structure should include:\n",
        "1. Introduction (no research needed)\n",
        "   - Brief overview of the topic area\n",
        "   - Context for the comparison\n",
        "\n",
        "2. Main Body Sections:\n",
        "   - One dedicated section for EACH offering being compared in the user-provided list\n",
        "   - Each section should examine:\n",
        "     - Core Features (bulleted list)\n",
        "     - Architecture & Implementation (2-3 sentences)\n",
        "     - One example use case (2-3 sentences)\n",
        "   \n",
        "3. No Main Body Sections other than the ones dedicated to each offering in the user-provided list\n",
        "\n",
        "4. Conclusion with Comparison Table (no research needed)\n",
        "   - Structured comparison table that:\n",
        "     * Compares all offerings from the user-provided list across key dimensions\n",
        "     * Highlights relative strengths and weaknesses\n",
        "   - Final recommendations\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, choose the topic of your report. The default is CPU vs. GPU, but feel free to change the topic to something of your interest. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Topic \n",
        "report_topic = \"Give an overview of capabilities and specific use case examples for these processing units: CPU, GPU.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run the agent. We set the Tavily topic to \"general\", but you can set it to \"news\" if you want Tavily to retrieve latest searches. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Name: Introduction\n",
            "Description: This section provides a brief overview of CPU and GPU processing units, discussing their fundamental roles in computing and setting the context for a comparative analysis.\n",
            "Research: False\n",
            "==================================================\n",
            "Name: CPU Capabilities\n",
            "Description: This section explores the core features of CPUs, including their architecture, implementation, and a specific use case example to illustrate their application in real-world scenarios.\n",
            "Research: True\n",
            "==================================================\n",
            "Name: GPU Capabilities\n",
            "Description: This section delves into the core features of GPUs, detailing their architecture, implementation, and a specific use case example that showcases their strengths in handling parallel processing tasks.\n",
            "Research: True\n",
            "==================================================\n",
            "Name: Conclusion\n",
            "Description: This section summarizes the findings of the comparative analysis, featuring a structured comparison table that highlights the strengths and weaknesses of CPUs and GPUs, and provides final recommendations based on the analysis.\n",
            "Research: False\n"
          ]
        }
      ],
      "source": [
        "# Tavily search parameters\n",
        "tavily_topic = \"general\"\n",
        "tavily_days = None # Only applicable for news topic\n",
        "\n",
        "# Generate report plan\n",
        "sections = await generate_report_plan({\"topic\": report_topic, \"report_structure\": report_structure, \"number_of_queries\": 2, \"tavily_topic\": tavily_topic, \"tavily_days\": tavily_days})\n",
        "\n",
        "# Print sections\n",
        "for section in sections['sections']:\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Name: {section.name}\")\n",
        "    print(f\"Description: {section.description}\")\n",
        "    print(f\"Research: {section.research}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Research and Writing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Now you are ready to give the Agent details about which sections require research, and the number of queries needed.  Let's First you will define the LangGraph state. Each state will have the following fields. \n",
        "\n",
        "Let's define the LangGraph state. Each state will have the following fields:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SectionState(TypedDict):\n",
        "    tavily_topic: Literal[\"general\", \"news\"] # Tavily search topic\n",
        "    tavily_days: Optional[int] # Only applicable for news topic\n",
        "    number_of_queries: int # Number web search queries to perform per section \n",
        "    section: Section # Report section   \n",
        "    search_queries: list[SearchQuery] # List of search queries\n",
        "    source_str: str # String of formatted source content from web search\n",
        "    report_sections_from_research: str # String of any completed sections from research to write final sections\n",
        "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API\n",
        "\n",
        "class SectionOutputState(TypedDict):\n",
        "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Write Single Section\n",
        "Now you will define the query writer instructions and the Agent function and nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "\n",
        "# Query writer instructions\n",
        "query_writer_instructions=\"\"\"Your goal is to generate targeted web search queries that will gather comprehensive information for writing a technical report section.\n",
        "\n",
        "Topic for this section:\n",
        "{section_topic}\n",
        "\n",
        "When generating {number_of_queries} search queries, ensure they:\n",
        "1. Cover different aspects of the topic (e.g., core features, real-world applications, technical architecture)\n",
        "2. Include specific technical terms related to the topic\n",
        "3. Target recent information by including year markers where relevant (e.g., \"2024\")\n",
        "4. Look for comparisons or differentiators from similar technologies/approaches\n",
        "5. Search for both official documentation and practical implementation examples\n",
        "\n",
        "Your queries should be:\n",
        "- Specific enough to avoid generic results\n",
        "- Technical enough to capture detailed implementation information\n",
        "- Diverse enough to cover all aspects of the section plan\n",
        "- Focused on authoritative sources (documentation, technical blogs, academic papers)\"\"\"\n",
        "\n",
        "# Section writer instructions\n",
        "section_writer_instructions = \"\"\"You are an expert technical writer crafting one section of a technical report.\n",
        "\n",
        "Topic for this section:\n",
        "{section_topic}\n",
        "\n",
        "Guidelines for writing:\n",
        "\n",
        "1. Technical Accuracy:\n",
        "- Include specific version numbers\n",
        "- Reference concrete metrics/benchmarks\n",
        "- Cite official documentation\n",
        "- Use technical terminology precisely\n",
        "\n",
        "2. Length and Style:\n",
        "- Strict 150-200 word limit\n",
        "- No marketing language\n",
        "- Technical focus\n",
        "- Write in simple, clear language\n",
        "- Start with your most important insight in **bold**\n",
        "- Use short paragraphs (2-3 sentences max)\n",
        "\n",
        "3. Structure:\n",
        "- Use ## for section title (Markdown format)\n",
        "- Only use ONE structural element IF it helps clarify your point:\n",
        "  * Either a focused table comparing 2-3 key items (using Markdown table syntax)\n",
        "  * Or a short list (3-5 items) using proper Markdown list syntax:\n",
        "    - Use `*` or `-` for unordered lists\n",
        "    - Use `1.` for ordered lists\n",
        "    - Ensure proper indentation and spacing\n",
        "- End with ### Sources that references the below source material formatted as:\n",
        "  * List each source with title, date, and URL\n",
        "  * Format: `- Title : URL`\n",
        "\n",
        "3. Writing Approach:\n",
        "- Include at least one specific example or case study\n",
        "- Use concrete details over general statements\n",
        "- Make every word count\n",
        "- No preamble prior to creating the section content\n",
        "- Focus on your single most important point\n",
        "\n",
        "4. Use this source material to help write the section:\n",
        "{context}\n",
        "\n",
        "5. Quality Checks:\n",
        "- Exactly 150-200 words (excluding title and sources)\n",
        "- Careful use of only ONE structural element (table or list) and only if it helps clarify your point\n",
        "- One specific example / case study\n",
        "- Starts with bold insight\n",
        "- No preamble prior to creating the section content\n",
        "- Sources cited at end\"\"\"\n",
        "\n",
        "def generate_queries(state: SectionState):\n",
        "    \"\"\" Generate search queries for a section \"\"\"\n",
        "\n",
        "    # Get state \n",
        "    number_of_queries = state[\"number_of_queries\"]\n",
        "    section = state[\"section\"]\n",
        "\n",
        "    # Generate queries \n",
        "    structured_llm = llm.with_structured_output(Queries)\n",
        "\n",
        "    # Format system instructions\n",
        "    system_instructions = query_writer_instructions.format(section_topic=section.description, number_of_queries=number_of_queries)\n",
        "\n",
        "    # Generate queries  \n",
        "    queries = invoke_structured_llm_with_retry(structured_llm,\n",
        "                                              [SystemMessage(content=system_instructions)]+[HumanMessage(content=\"Generate search queries on the provided topic.\")])\n",
        "\n",
        "    return {\"search_queries\": queries.queries}\n",
        "\n",
        "async def search_web(state: SectionState):\n",
        "    \"\"\" Search the web for each query, then return a list of raw sources and a formatted string of sources.\"\"\"\n",
        "    \n",
        "    # Get state \n",
        "    search_queries = state[\"search_queries\"]\n",
        "    tavily_topic = state[\"tavily_topic\"]\n",
        "    tavily_days = state.get(\"tavily_days\", None)\n",
        "\n",
        "    # Web search\n",
        "    query_list = [query.search_query for query in search_queries]\n",
        "    search_docs = await tavily_search_async(query_list, tavily_topic, tavily_days)\n",
        "\n",
        "    # Deduplicate and format sources\n",
        "    source_str = deduplicate_and_format_sources(search_docs, max_tokens_per_source=5000, include_raw_content=True)\n",
        "\n",
        "    return {\"source_str\": source_str}\n",
        "\n",
        "def write_section(state: SectionState):\n",
        "    \"\"\" Write a section of the report \"\"\"\n",
        "\n",
        "    # Get state \n",
        "    section = state[\"section\"]\n",
        "    source_str = state[\"source_str\"]\n",
        "\n",
        "    # Format system instructions\n",
        "    system_instructions = section_writer_instructions.format(section_title=section.name, section_topic=section.description, context=source_str)\n",
        "\n",
        "    # Generate section  \n",
        "    section_content = llm.invoke([SystemMessage(content=system_instructions)]+[HumanMessage(content=\"Generate a report section based on the provided sources.\")])\n",
        "    \n",
        "    # Write content to the section object  \n",
        "    section.content = section_content.content\n",
        "\n",
        "    # Write the updated section to completed sections\n",
        "    return {\"completed_sections\": [section]}\n",
        "\n",
        "# Add nodes and edges \n",
        "section_builder = StateGraph(SectionState, output=SectionOutputState)\n",
        "section_builder.add_node(\"generate_queries\", generate_queries)\n",
        "section_builder.add_node(\"search_web\", search_web)\n",
        "section_builder.add_node(\"write_section\", write_section)\n",
        "\n",
        "section_builder.add_edge(START, \"generate_queries\")\n",
        "section_builder.add_edge(\"generate_queries\", \"search_web\")\n",
        "section_builder.add_edge(\"search_web\", \"write_section\")\n",
        "section_builder.add_edge(\"write_section\", END)\n",
        "\n",
        "# Compile\n",
        "section_builder_graph = section_builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(section_builder_graph.get_graph(xray=1).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validate Single Section\n",
        "\n",
        "Call on the Agent to write a single section to ensure the content is generated as expected. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Name: CPU Capabilities\n",
            "Description: This section explores the core features of CPUs, including their architecture, implementation, and a specific use case example to illustrate their application in real-world scenarios.\n",
            "Research: True\n",
            "Warning: No raw_content found for source https://www.youtube.com/watch?v=jPkyypY2srA\n",
            "Warning: No raw_content found for source https://www.youtube.com/watch?v=c6FldbyQvoQ\n",
            "Warning: No raw_content found for source https://jcbi.org/index.php/Main/article/view/612/505\n"
          ]
        }
      ],
      "source": [
        "# Test with one section\n",
        "sections = sections['sections'] \n",
        "test_section = sections[1]\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Name: {test_section.name}\")\n",
        "print(f\"Description: {test_section.description}\")\n",
        "print(f\"Research: {test_section.research}\")\n",
        "\n",
        "# Run\n",
        "report_section = await section_builder_graph.ainvoke({\"section\": test_section, \"number_of_queries\": 2, \"tavily_topic\": tavily_topic, \"tavily_days\": tavily_days})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Core Features of CPUs\n",
              "\n",
              "**Modern CPUs are designed around a balance of performance and power efficiency, utilizing advanced architectures and techniques.** The primary components include the Arithmetic Logic Unit (ALU), Control Unit (CU), and multiple cores that enable parallel processing. For instance, Intel's Xeon 6 SoC, set to launch in early 2025, integrates up to 32 lanes of PCIe 5.0 and advanced vector extensions, enhancing both performance and energy efficiency for AI workloads (Intel, 2024).\n",
              "\n",
              "Key techniques in CPU architecture include:\n",
              "* **Pipelining**: This enables simultaneous processing of multiple instructions, increasing throughput.\n",
              "* **Superscalar execution**: Allows multiple instructions to be issued in a single clock cycle, further enhancing performance.\n",
              "* **Out-of-order execution**: Improves resource utilization by executing instructions as operands become available, reducing idle times.\n",
              "\n",
              "A notable use case is the Intel Gaudi 3 AI accelerator, which leverages CPU capabilities to improve generative AI model training efficiency, demonstrating the need for high-performance processors in modern AI applications (Intel, 2024).\n",
              "\n",
              "### Sources\n",
              "- Intel Demonstrates AI Architectural Expertise at Hot Chips 2024 : https://newsroom.intel.com/artificial-intelligence/hot-chips-2024-ai-architectural-expertise\n",
              "- In-Depth Technical Analysis of CPU and GPU Architectures and DIY ... : https://medium.com/@akepr/in-depth-technical-analysis-of-cpu-and-gpu-architectures-and-diy-design-approaches-d3e4fe16a8c3\n",
              "- Analyst Predictions for 2024 - TechInsights : https://www.techinsights.com/blog/analyst-predictions-2024-bold-predictions-major-breakthroughs-cpus-ai-and-processor-adjacent"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "section = report_section['completed_sections'][0]\n",
        "Markdown(section.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Write All Sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReportStateOutput(TypedDict):\n",
        "    final_report: str # Final report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly_live_training_getting_started_with_langchain/notebooks/4.0-structured-research-report-generation.ipynb Cell 41\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly_live_training_getting_started_with_langchain/notebooks/4.0-structured-research-report-generation.ipynb#X55sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m builder\u001b[39m.\u001b[39madd_edge(\u001b[39m\"\u001b[39m\u001b[39mcompile_final_report\u001b[39m\u001b[39m\"\u001b[39m, END)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly_live_training_getting_started_with_langchain/notebooks/4.0-structured-research-report-generation.ipynb#X55sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m graph \u001b[39m=\u001b[39m builder\u001b[39m.\u001b[39mcompile()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly_live_training_getting_started_with_langchain/notebooks/4.0-structured-research-report-generation.ipynb#X55sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m display(Image(graph\u001b[39m.\u001b[39;49mget_graph()\u001b[39m.\u001b[39;49mdraw_mermaid_png()))\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages/langchain_core/runnables/graph.py:695\u001b[0m, in \u001b[0;36mGraph.draw_mermaid_png\u001b[0;34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrunnables\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgraph_mermaid\u001b[39;00m \u001b[39mimport\u001b[39;00m draw_mermaid_png\n\u001b[1;32m    689\u001b[0m mermaid_syntax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdraw_mermaid(\n\u001b[1;32m    690\u001b[0m     curve_style\u001b[39m=\u001b[39mcurve_style,\n\u001b[1;32m    691\u001b[0m     node_colors\u001b[39m=\u001b[39mnode_colors,\n\u001b[1;32m    692\u001b[0m     wrap_label_n_words\u001b[39m=\u001b[39mwrap_label_n_words,\n\u001b[1;32m    693\u001b[0m     frontmatter_config\u001b[39m=\u001b[39mfrontmatter_config,\n\u001b[1;32m    694\u001b[0m )\n\u001b[0;32m--> 695\u001b[0m \u001b[39mreturn\u001b[39;00m draw_mermaid_png(\n\u001b[1;32m    696\u001b[0m     mermaid_syntax\u001b[39m=\u001b[39;49mmermaid_syntax,\n\u001b[1;32m    697\u001b[0m     output_file_path\u001b[39m=\u001b[39;49moutput_file_path,\n\u001b[1;32m    698\u001b[0m     draw_method\u001b[39m=\u001b[39;49mdraw_method,\n\u001b[1;32m    699\u001b[0m     background_color\u001b[39m=\u001b[39;49mbackground_color,\n\u001b[1;32m    700\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m    701\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    702\u001b[0m     retry_delay\u001b[39m=\u001b[39;49mretry_delay,\n\u001b[1;32m    703\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:294\u001b[0m, in \u001b[0;36mdraw_mermaid_png\u001b[0;34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[0m\n\u001b[1;32m    288\u001b[0m     img_bytes \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mrun(\n\u001b[1;32m    289\u001b[0m         _render_mermaid_using_pyppeteer(\n\u001b[1;32m    290\u001b[0m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[1;32m    291\u001b[0m         )\n\u001b[1;32m    292\u001b[0m     )\n\u001b[1;32m    293\u001b[0m \u001b[39melif\u001b[39;00m draw_method \u001b[39m==\u001b[39m MermaidDrawMethod\u001b[39m.\u001b[39mAPI:\n\u001b[0;32m--> 294\u001b[0m     img_bytes \u001b[39m=\u001b[39m _render_mermaid_using_api(\n\u001b[1;32m    295\u001b[0m         mermaid_syntax,\n\u001b[1;32m    296\u001b[0m         output_file_path\u001b[39m=\u001b[39;49moutput_file_path,\n\u001b[1;32m    297\u001b[0m         background_color\u001b[39m=\u001b[39;49mbackground_color,\n\u001b[1;32m    298\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    299\u001b[0m         retry_delay\u001b[39m=\u001b[39;49mretry_delay,\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    301\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     supported_methods \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([m\u001b[39m.\u001b[39mvalue \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m MermaidDrawMethod])\n",
            "File \u001b[0;32m~/miniconda3/envs/oreilly-langchain/lib/python3.11/site-packages/langchain_core/runnables/graph_mermaid.py:451\u001b[0m, in \u001b[0;36m_render_mermaid_using_api\u001b[0;34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[39m# For other status codes, fail immediately\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myour graph. Status code: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m     ) \u001b[39m+\u001b[39m error_msg_suffix\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    453\u001b[0m \u001b[39mexcept\u001b[39;00m (requests\u001b[39m.\u001b[39mRequestException, requests\u001b[39m.\u001b[39mTimeout) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    454\u001b[0m     \u001b[39mif\u001b[39;00m attempt \u001b[39m<\u001b[39m max_retries:\n\u001b[1;32m    455\u001b[0m         \u001b[39m# Exponential backoff with jitter\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
          ]
        }
      ],
      "source": [
        "from langgraph.constants import Send\n",
        "\n",
        "final_section_writer_instructions=\"\"\"You are an expert technical writer crafting a section that synthesizes information from the rest of the report.\n",
        "\n",
        "Section to write: \n",
        "{section_topic}\n",
        "\n",
        "Available report content:\n",
        "{context}\n",
        "\n",
        "1. Section-Specific Approach:\n",
        "\n",
        "For Introduction:\n",
        "- Use # for report title (Markdown format)\n",
        "- 50-100 word limit\n",
        "- Write in simple and clear language\n",
        "- Focus on the core motivation for the report in 1-2 paragraphs\n",
        "- Use a clear narrative arc to introduce the report\n",
        "- Include NO structural elements (no lists or tables)\n",
        "- No sources section needed\n",
        "\n",
        "For Conclusion/Summary:\n",
        "- Use ## for section title (Markdown format)\n",
        "- 100-150 word limit\n",
        "- For comparative reports:\n",
        "    * Must include a focused comparison table using Markdown table syntax\n",
        "    * Table should distill insights from the report\n",
        "    * Keep table entries clear and concise\n",
        "- For non-comparative reports: \n",
        "    * Only use ONE structural element IF it helps distill the points made in the report:\n",
        "    * Either a focused table comparing items present in the report (using Markdown table syntax)\n",
        "    * Or a short list using proper Markdown list syntax:\n",
        "      - Use `*` or `-` for unordered lists\n",
        "      - Use `1.` for ordered lists\n",
        "      - Ensure proper indentation and spacing\n",
        "- End with specific next steps or implications\n",
        "- No sources section needed\n",
        "\n",
        "3. Writing Approach:\n",
        "- Use concrete details over general statements\n",
        "- Make every word count\n",
        "- Focus on your single most important point\n",
        "\n",
        "4. Quality Checks:\n",
        "- For introduction: 50-100 word limit, # for report title, no structural elements, no sources section\n",
        "- For conclusion: 100-150 word limit, ## for section title, only ONE structural element at most, no sources section\n",
        "- Markdown format\n",
        "- Do not include word count or any preamble in your response\"\"\"\n",
        "\n",
        "def initiate_section_writing(state: ReportState):\n",
        "    \"\"\" This is the \"map\" step when we kick off web research for some sections of the report \"\"\"    \n",
        "    \n",
        "    # Kick off section writing in parallel via Send() API for any sections that require research\n",
        "    return [\n",
        "        Send(\"build_section_with_web_research\", {\"section\": s, \n",
        "                                                 \"number_of_queries\": state[\"number_of_queries\"], \n",
        "                                                 \"tavily_topic\": state[\"tavily_topic\"], \n",
        "                                                 \"tavily_days\": state.get(\"tavily_days\", None)}) \n",
        "        for s in state[\"sections\"] \n",
        "        if s.research\n",
        "    ]\n",
        "\n",
        "def write_final_sections(state: SectionState):\n",
        "    \"\"\" Write final sections of the report, which do not require web search and use the completed sections as context \"\"\"\n",
        "\n",
        "    # Get state \n",
        "    section = state[\"section\"]\n",
        "    completed_report_sections = state[\"report_sections_from_research\"]\n",
        "    \n",
        "    # Format system instructions\n",
        "    system_instructions = final_section_writer_instructions.format(section_title=section.name, section_topic=section.description, context=completed_report_sections)\n",
        "\n",
        "    # Generate section  \n",
        "    section_content = llm.invoke([SystemMessage(content=system_instructions)]+[HumanMessage(content=\"Generate a report section based on the provided sources.\")])\n",
        "    \n",
        "    # Write content to section \n",
        "    section.content = section_content.content\n",
        "\n",
        "    # Write the updated section to completed sections\n",
        "    return {\"completed_sections\": [section]}\n",
        "\n",
        "def gather_completed_sections(state: ReportState):\n",
        "    \"\"\" Gather completed sections from research \"\"\"    \n",
        "\n",
        "    # List of completed sections\n",
        "    completed_sections = state[\"completed_sections\"]\n",
        "\n",
        "    # Format completed section to str to use as context for final sections\n",
        "    completed_report_sections = format_sections(completed_sections)\n",
        "\n",
        "    return {\"report_sections_from_research\": completed_report_sections}\n",
        "\n",
        "def initiate_final_section_writing(state: ReportState):\n",
        "    \"\"\" This is the \"map\" step when we kick off research on any sections that require it using the Send API \"\"\"    \n",
        "\n",
        "    # Kick off section writing in parallel via Send() API for any sections that do not require research\n",
        "    return [\n",
        "        Send(\"write_final_sections\", {\"section\": s, \"report_sections_from_research\": state[\"report_sections_from_research\"]}) \n",
        "        for s in state[\"sections\"] \n",
        "        if not s.research\n",
        "    ]\n",
        "\n",
        "def compile_final_report(state: ReportState):\n",
        "    \"\"\" Compile the final report \"\"\"    \n",
        "\n",
        "    # Get sections\n",
        "    sections = state[\"sections\"]\n",
        "    completed_sections = {s.name: s.content for s in state[\"completed_sections\"]}\n",
        "\n",
        "    # Update sections with completed content while maintaining original order\n",
        "    for section in sections:\n",
        "        section.content = completed_sections[section.name]\n",
        "\n",
        "    # Compile final report\n",
        "    all_sections = \"\\n\\n\".join([s.content for s in sections])\n",
        "\n",
        "    return {\"final_report\": all_sections}\n",
        "\n",
        "# Add nodes and edges \n",
        "builder = StateGraph(ReportState, output=ReportStateOutput)\n",
        "builder.add_node(\"generate_report_plan\", generate_report_plan)\n",
        "builder.add_node(\"build_section_with_web_research\", section_builder.compile())\n",
        "builder.add_node(\"gather_completed_sections\", gather_completed_sections)\n",
        "builder.add_node(\"write_final_sections\", write_final_sections)\n",
        "builder.add_node(\"compile_final_report\", compile_final_report)\n",
        "builder.add_edge(START, \"generate_report_plan\")\n",
        "builder.add_conditional_edges(\"generate_report_plan\", initiate_section_writing, [\"build_section_with_web_research\"])\n",
        "builder.add_edge(\"build_section_with_web_research\", \"gather_completed_sections\")\n",
        "builder.add_conditional_edges(\"gather_completed_sections\", initiate_final_section_writing, [\"write_final_sections\"])\n",
        "builder.add_edge(\"write_final_sections\", \"compile_final_report\")\n",
        "builder.add_edge(\"compile_final_report\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Structure\n",
        "report_structure = \"\"\"This report type focuses on comparative analysis.\n",
        "\n",
        "The report structure should include:\n",
        "1. Introduction (no research needed)\n",
        "   - Brief overview of the topic area\n",
        "   - Context for the comparison\n",
        "\n",
        "2. Main Body Sections:\n",
        "   - One dedicated section for EACH offering being compared in the user-provided list\n",
        "   - Each section should examine:\n",
        "     - Core Features (bulleted list)\n",
        "     - Architecture & Implementation (2-3 sentences)\n",
        "     - One example use case (2-3 sentences)\n",
        "   \n",
        "3. No Main Body Sections other than the ones dedicated to each offering in the user-provided list\n",
        "\n",
        "4. Conclusion with Comparison Table (no research needed)\n",
        "   - Structured comparison table that:\n",
        "     * Compares all offerings from the user-provided list across key dimensions\n",
        "     * Highlights relative strengths and weaknesses\n",
        "   - Final recommendations\"\"\"\n",
        "\n",
        "# Tavily search parameters\n",
        "tavily_topic = \"general\"\n",
        "tavily_days = None # Only applicable for news topic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once again, choose the topic of your report. The default is CPU vs. GPU, but feel free to change the topic to something of your interest. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Topic \n",
        "report_topic = \"Give an overview of capabilities and specific use case examples for these processing units: CPU, GPU.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "report = await graph.ainvoke({\"topic\": report_topic, \n",
        "                                   \"report_structure\": report_structure, \n",
        "                                   \"number_of_queries\": 2, \n",
        "                                   \"tavily_topic\": tavily_topic, \n",
        "                                   \"tavily_days\": tavily_days})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Report\n",
        "Let's validate all sections. This is the final report the Agent has written. \n",
        "Finally, let's look at the final output. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "Markdown(report['final_report'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print the final report in plain text\n",
        "report['final_report']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
